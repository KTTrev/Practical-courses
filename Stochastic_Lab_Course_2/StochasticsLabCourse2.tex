\documentclass{report}
%chapter style Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, %Bjornstrup
\usepackage[Bjarne]{fncychap}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\numberwithin{equation}{section} %for equations numbering
\usepackage{amssymb}
\usepackage[top=5cm, bottom=5cm, left=3cm, right=3cm]{geometry}
\pagestyle{headings}
\usepackage{lmodern}
\usepackage{enumitem}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{float, graphicx, subcaption}
\usepackage[%  
colorlinks=true,
pdfborder={0 0 0},
linkcolor=red
]{hyperref}
\usepackage{fancyhdr}
\usepackage{caption} %to fix referencing to exact the label
\usepackage[dvipsnames]{xcolor}
\DeclareMathOperator{\argmax}{argmax}
\newtheorem{theorem}{Theorem}
%define my own color
\definecolor{royalblue}{rgb}{0.0, 0.14, 0.4}

\usepackage{tikz} %for .tex vector images

%pagestyle
\pagestyle{fancy}
\fancyhead[RE,LO]{{\color{royalblue}Stochastics Lab Course II}}
\fancyhead[LE,RO]{\leftmark}
\fancyfoot[RE,LO]{\rightmark}
{\color{royalblue}\fancyfoot[LE,RO]{\thepage}}
\cfoot{}

%decorative lines
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headrule}{\hbox to\headwidth{
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\footrule}{\hbox to\headwidth{%
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}

\title{\textsc{Stochastics Lab Course II}}
\author{Khwam Tabougua Trevor}
\date{March 2019}
\begin{document}
	
\maketitle

{\color{royalblue}\chapter*{Introduction}}

The "Stochastics Lab course II" is an Introductory Course for
statistics and stochastics applications with R programming language. The course lasted for two weeks in March 2019. The report written on \LaTeX, contains results, interpretations and figures from the ten exercises that had to be solved. Along with this report, there is also the R codes, which are recommended to understand the result.

%1111111111111111111111111111111111111111111111%
\tableofcontents
{\color{royalblue}\chapter{Tidyverse}}
{\color{royalblue}\section{Problem description}}
R base tools can accomplish "almost" every programming tasks. However, when using large datasets or when implementing complex tasks(like graphs, maps, tidying, etc), things get complicated. We want to enhance our algorithms fo batter results or productivity. To this aim, we will use the Tidyverse package.

{\color{royalblue}\section{Methods}}
Tidyverse is a collection of packages for data manipulation, exploration and visualization. The core packages are \textbf{ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats}, but we will only be using ggplot2, dplyr, tidyr, and tibble.
\begin{itemize}
	\item[--] \textbf{ggplot2} is a system for declaratively creating graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
	
	\item[--] \textbf{dplyr} is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges such as adding new variables (that are functions of existing variables), picking variables based on their names, selecting rows (based on their value), reducing multiple values down to a single summary, and changing the ordering of the rows.
	
	\item[--] \textbf{tidyr} package goal is to help you create tidy data. Tidy data is data where each variable is in a column, each observation is a row, and Each value is a cell.
	
	\item[--] \textbf{tibble} package goal is to use tibbles, which are modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors).
	
	
\end{itemize}

{\color{royalblue}\section{Results}}

\begin{enumerate}[label=(\alph*)]
	\item After loading and filtering the data childrenfinal.dta, we convert some variables (namely tetanusmother, breastfeeding, wantedchild, anetalvisits, and placedelivery) into double labeled \textit{<dbl>} (doubles, or real numbers).
	
	\item 
	\item[°]The \hyperlink{Figure 1.1}{figure 1.1} indicates that the effect of zstunt is negatively affecting te hypage.
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex1plot1.tex}}
		\caption{Scatter plot of zstunt against hypage with smooth line (in purple)}
	\end{figure}
	
	\item[°]gjdhgdhdg
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex1plot2.tex}}
		\caption{Some Meaningful Caption}
	\end{figure}
	
	\item[°]gjdhgdhdg
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex1plot3.tex}}
		\caption{Some Meaningful Caption}
	\end{figure} 
		
	\item 
	\begin{figure}[ht]
		\centering
		\includegraphics[scale=0.5]{Ex1plot4.png}
		\caption{Scatter plot of zstunt against hypage with smooth line (in purple)}
	\end{figure}	
		
		
\end{enumerate}	
%222222222222222222222222222222222222222222222%

{\color{royalblue}\chapter{Random number generation}}

{\color{royalblue}\section{Problem description}}
Generating a random variable from any distribution is very essential for stochastics studies. However, true random numbers are not always available, but we can use some algorithms that generate some pseudo-random numbers.\\

{\color{royalblue}\section{Methods}}
Pseudo-random numbers are a sequence of numbers that appear "random" or approximate properties of random numbers with the following properties:\\
\begin{itemize}
	\item Good approximation of the properties of random numbers.
	\item Number can be easily and efficiently generated.
	\item Reproducibility (truly random numbers never satisfy this).
\end{itemize}

With the help of some probability properties, it is typically enough to be able to use a uniform distributed random variable, in order to generate any pseudo-random numbers from a given distribution.\\
The simplest idea for generating uniformly distributed pseudo-random numbers is using a \textit{linear congruent generators} (LCG):\\
\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Choose positive integer parameters a, c and m.
				\item Choose an initial value $x_0 \in \{0, 1, \ldots, m - 1\}$ (this value is called the seed).
				\item For each $n \in \mathbb{N} $, compute\\
				\begin{equation}
				x_{n+1} := ax_{n} + c \hspace{0.2cm}
				\text{mod} \hspace{0.2cm} m, \hspace{1cm} \forall n \in \mathbb{N} 
				\end{equation}
				\item The psuedorandom numbers are the sequence $x_1; x_2; x_3; \ldots $.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We make some observations regarding the LCG algorithm.\\
\begin{itemize}
	\item The LCG can generates at most m distinct numbers which are contained in $\{0, 1, \ldots, m - 1\}$
	\item As soon as some number in the sequence, say ${x_n}_0$, is repeated (i.e.,$\exists p$ such that $x_{n_{0} + p} = {x_n}_0$),

	then the same is true for the entire sequence:
	\begin{equation}
	x_{n_{0} +p+j} = x_{n_{0} + j}, \hspace{0.5cm} \forall j \geqslant 1
	\end{equation}
	The number $p$ is called the period of the sequence. This musst be less than or equal to $m$.
	\item Suppose that (under the right conditions) the LCG approximates a uniform distribution with

	parameters $0$ and $m$. In this case, $x_n / m$ would approximate a uniform distribution with parameters
0 and 1.
\end{itemize}
To generate uniformly random numbers, the period has to be maximal
(i.e., $p = m$), so that we sample every value in the sequence before repeating any. One can show that LCG has a full period $m = 2^{b}$, $b \geqslant 2$ if and
only if $c \in (0; m)$ is odd and $a$ mod $4 = 1$.\\

After generating uniform pseudo-random numbers, we can easily obtain random variables from other distributions. The \textbf{inversion method} is one way of doing so, with the help of the following theorem:
\begin{theorem}
	Let F be a distribution function on $\mathbb{R}$. The quantile function $F^{-1}$ is defined by
	 \[ F^{-1}(u) = \text{inf} \{x: F(x)\ge u, 0 < u < 1 \} \]
	If $U \sim U_{[0; 1]}$, then $F^{-1}(u)$ has a distribution function
$F$.
\end{theorem}
Hence, for continuous distributions (exponential, Pareto, standard Cauchy, etc) where $F^{-1}$, we simply simulate $U_{i}$ (with LCG) and set $X_{i} = F^{-1}(U_i)$. If $F^{-1}$ cannot be inverted analytically, appropriate
numerical methods can be applied.\\

Let $X$ be a discrete random variable with ordered possible values $\{x_{1}, x{2},\ldots \}$, so that $F(x) = \displaystyle\sum_{i:x_{i} \leqslant x} P(X = x_{i})$ and
\[F^{-1}(r) =  \text{min} \{x_k \in \{x_{1}, x_{2},\ldots \}: \displaystyle\sum_{j=1}^{k} P(X = x_{j}) = \displaystyle\sum_{j=1}^{k}p_{j} \geqslant r\}\]
Then the inverse method becomes: set $X = x_{1}$ if and only if $U_{i} \in [0, p_{1})$ and $X = x_{k}$ if and only if
$U_{i} \in \Big[ \displaystyle\sum_{j=1}^{k-1}p_{j}, \displaystyle\sum_{j=1}^{k}p_{j}\Big)$, $k = 2, 3, \ldots$. Note that

\[P(X = x_{k}) = P\Big(\displaystyle\sum_{j=1}^{k-1}p_{j} \leqslant U_{i} < \displaystyle\sum_{j=1}^{k}p_{j} \Big) = \displaystyle\sum_{j=1}^{k}p_{j} - \displaystyle\sum_{j=1}^{k-1}p_{j} = p_{k} \]

For example, to simulate a Bernoulli random variable $Ber(p)$, generate $U \in U_{[0, 1]}$ and

set $X = 0$, if $U \leqslant 1 - p$ and $X = 1$ if $U > 1 - p$.\\

Another general approach to pseudo-random variables generation is the \textbf{acceptance-rejection method}.

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item [] \textbf{Data}: Two probability densitiy functions: $f$ for $X$ and $g$ for $Y$
				\item Find a constant $M > 0$ such that $sup_{x} \frac{f(x)}{g(x)} \leqslant c$ ;
				\item Obtain a sample $y$ from $Y$ ;
				\item Obtain a sample $u$ from the uniform distribution on $[0, 1]$;
				\item \textbf{if} $u < \dfrac{f(y)}{cg(y)}$ \textbf{then}
				\item $\bigg|$ Accept $y$ as a sample drawn from $f$;
				\item \textbf{else}
				\item $\bigg|$ Reject the value of $y$ and return to the sampling step (line 2);
				\item [] \textbf{Result:} $y$, a sample drawn from $f$ (using $g$)
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

\textbf{NB:} Computing $c$ could be difficult, but one can show that $c = sup_{x} {\frac{f(x)}{g(x)}}$\\

{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item 
	With the Wichmann-Hill pseudo-random number generator in R, we Simulate $N =
1000 $ binomial random variables $B(n = 10, p = 0.4)$ using three approaches: inversion

	method, by simulating corresponding Bernoulli random variables by inversion method
and using R built-in function rbinom. From the figure \ref{fig 2.1}, the histograms of the three samples present the same shape but are different. This proves that the "random" numbers generated by our methods are just approximations of true random numbers.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot1.tex}}
		\caption{Histogram of the empirical CDF of all
three samples}
		\label{fig 2.1}
	\end{figure}
	
	\item To use accept-reject method (and a generator for uniform random variables only), of $N = 10000$ standard normal distributed random variables
with density 
	$f(x) = (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}} $, the density of the
standard Cauchy distribution is used: 
	$ g(x)=\{{\pi(1+x^2)}^{-1}\} $.
	
	\item[°] The constant value $c$ for this method is given by $sup_{x} {\frac{f(x)}{g(x)}} = 1.520347 $
	\item[°] Then after computing the $N$ standard normal random variables, we notice that the estimated and theoretical acceptance probabilities are almost equal. This is well depicted with in the figure \ref{fig 2.2}, where the histogram of the obtain sample is symmetric and has the same shape as the standard normal density curve.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot2.tex}}
		\caption{Histogram of the obtained sample and the standard
		normal density (in blue)}
	\label{fig 2.2}
	\end{figure}
	
	\item[°] The QQ-plot in figure \ref{fig 2.3} shows points following the identity line. Hence the accept-reject method used to simulate a standard normal distributed sample (using the the standard Cauchy density) is well accurate.
	
	\newpage
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot3.tex}}
		\caption{QQ-plot}
		\label{fig 2.3}
	\end{figure}
	
	\item[°] However, it is not possible to simulate ample distributed from the standard Cauchy density using
the accept-reject method with a standard normal candidate density, simply because cannot find a $c$ such that $g(x) \leqslant cf(x)$ is verified ( because $ sup_{x} {\frac{g(x)}{f(x)}} = \infty $).
	
\end{enumerate}

%333333333333333333333333333333333333333333333%

{\color{royalblue}\chapter{Bootstrap}}
{\color{royalblue}\section{Problem description}}
Suppose that a sample $ \mathbf{X} = \{X_1,\ldots ,X_n \} $ is used to estimate a parameter $\theta$ of the distribution $P$ (which is unknown) and let $\hat{\theta} = S(\mathbf{X})$ be a statistic that estimates $\theta$. For the purpose of statistical inference on $\theta$,  we are interested in the sampling distribution of $\hat{\theta}$ (or certain aspects of it) so as to assess the accuracy of our estimator or to set confidence intervals for our estimate of $\theta$. If the true distribution $P$ were known, we could draw samples $\mathbf{X}_{l}, l = 1,\ldots, R \in \mathbb{N}$ from P and use Monte Carlo methods to estimate the sampling distribution of our estimate $\hat{\theta}$. The problem is that $P$ is unknown and we cannot sample from it.\\
The following section explains how to use bootstrap to make the interference on $\hat{\theta}$.

{\color{royalblue}\section{Methods}}
The bootstrap is a computerintensive resampling method, which  principle can be summarized by the following schematic diagram:
\begin{center}
	\begin{tabular}{|P{3.5cm} P{3.5cm} P{3.5cm} P{3.5cm}|}
		\hline
		\multicolumn{2}{|c}{\textbf{Real World}} & \multicolumn{2}{c|}{\textbf{Bootstrap World}} \\
		Unknown probability distribution & Observed random sample  & Empirical distribution & Bootstrap sample \\
		\multicolumn{4}{|c|}{$P \longrightarrow \mathbf{X} = \{X_1,\ldots ,X_n \}  \hspace{1cm} \Longrightarrow \hspace{1cm} \hat{P} \longrightarrow \mathbf{X}^{*} = \{{X_1}^{*},\ldots ,{X_n}^{*} \} $} \\
		\multicolumn{2}{|c}{$\Big\downarrow$} & \multicolumn{2}{c|}{$\Big\downarrow$} \\ 
		\multicolumn{2}{|c}{$\hat{\theta} = S(\mathbf{X})$} & \multicolumn{2}{c|}{$\hat{\theta}^{*} = S(\mathbf{X}^{*})$} \\  
		\multicolumn{2}{|c}{Statistic of interest} & \multicolumn{2}{c|}{Bootstrap replication} \\
		\hline  
	\end{tabular}
\end{center}

Then The idea is to sample from an empirical
distribution function. Recall that for random variables $Y = \{Y_1,\ldots ,Y_n \}$, the empirical distribution function is defined via $F_n(y) = n^{-1} \displaystyle\sum_{i=1}^{n}\mathbb{I}(Y_i \leqslant y)$ (we will use the notation $F_B$ for the bootstrap empirical distribution). If the sample of size $n$ is from a continuous distribution, then each
 observation has a probability $1/n$ and sampling from $F_n$ would be equivalent to draw with replacement from the sample. Hence the following algorithm:

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Draw $n$ times with replacement from $\mathbf{X}$ to get a bootstrap sample $\mathbf{X}^{*}_{1}$ of size $n$. Repeat $R$ times to get $R$ bootstrap samples $\mathbf{X}^{*}_{1},\ldots,\mathbf{X}^{*}_{R}$, each of size $n$.
				\item Compute bootstrap statistics $S(\mathbf{X}^{*}_{1}),\ldots, S(\mathbf{X}^{*}_{R})$.
				\item Make inference about $\theta$ based on $S(\mathbf{X}^{*}_{1}),\ldots, S(\mathbf{X}^{*}_{R})$.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We can also evaluate the goodness of the estimators (point or interval) based on the bootstrap sample. We construct confidence intervals for $\theta$ from the bootstrap replications (see step 2 in the above algorithm). \\

First recall the definition of a confidence interval. Let $\mathbf{X} = (X_1, \ldots, X_n)$ be a sample from a population with distribution $P \in \mathcal{P} = \{ P_\theta :\theta \in \Theta \subset \mathbb{R}^d \}$. Let $C(\mathbf{X})$ depend only on the sample $\mathbf{X}$ and $\theta \in \Theta$ be an unknown parameter
of interest. If
\[ \inf_{P \in \mathcal{P}} P(\theta \in C(\Theta)) \geqslant 1-\alpha \]
for a fixed $\alpha \in (0,1)$, then $C(\Theta)$ is a \textbf{confidence set} for $\theta$ with \textbf{level of
significance} $1-\alpha$. If the parameter $\theta$ is real-valued, then 
$C(\Theta) = [\underline{\theta}(\mathbf{X}), \bar{\theta}(\mathbf{X})]$, for a pair of real-valued
statistics $\underline{\theta}$ and $\bar{\theta}$ is called a confidence interval for $\theta$.\\
Therefore, a natural way to construct the bootstrap confidence interval is to use empirical quantiles of the bootstrap distribution of $S(\mathbf{X})$: compute $\hat{\theta}^{*}_i = S(\mathbf{X}^{*}_{i})$, $i = 1,\ldots,R$ bootstrap statistics and set the confidence interval for $\theta$ by $[\theta^{*}_{L}, \theta^{*}_{U}]$, where $\theta^{*}_{L}$ and $\theta^{*}_{U}$ are respectively $\lfloor R(\frac{1-\alpha}{2}) \rfloor$-th and $\lfloor R(1-\frac{1-\alpha}{2}) \rfloor$-th value in the ordered list of $\hat{\theta}^{*}_i$. Such confidence intervals are
called \textbf{bootstrap percentile}  confidence intervals. By defining $F_{B}(x) = P (\hat{\theta}^{*} \leqslant x)$, note that we have $P (\hat{\theta}^{*} \leqslant \hat{\theta}^{*}_L ) \approx \frac{1}{2} \alpha$ and $P (\hat{\theta}^{*} \geqslant \hat{\theta}^{*}_U ) \approx \frac{1}{2} \alpha$, which makes a coverage probability of $1-\alpha$.\\

The confidence interval should have equal probability to both sides of $\hat{\theta}^{*}$, that is $P(\hat{\theta}^{*} \leqslant \theta \leqslant \hat{\theta}^{*}_U) = P(\hat{\theta}^{*}_L \leqslant \theta \leqslant \hat{\theta}^{*}) $. If $\hat{\theta}^{*}$ is not the median of the bootstrap distribution, this condition is not fulfilled. An appropriate correction is given by $ \hat{\theta}^*_{LC} = F^{-1}_B(\Phi[z_\frac{\alpha}{2} + 2\hat{z}_0]) $ and $ \hat{\theta}^*_{UC} = F^{-1}_B(\Phi[z_{1-\frac{\alpha}{2}} + \hat{z}_0]) $ (respectively  the bias-corrected lower and upper confidence bound for $\theta$), where $\Phi(.)$ is the cdf of the standard normal distribution and $\hat{z}_0 = \Phi^{-1}{\{ F_B(\hat{\theta})\}} $. This interval is the \textbf{bias corrected percentile interval}. In practice, $\hat{\theta}^*_{LC}=\lfloor R\alpha_1 \rfloor $ and $\hat{\theta}^*_{UC}=\lfloor R\alpha_2 \rfloor $,
with $\alpha_1 = \Phi(z_\frac{\alpha}{2} + 2\hat{z}_0)$ and $\alpha_2 = \Phi(z_{1-\frac{\alpha}{2}} + 2\hat{z}_0)$. \\

An extension of the bias corrected percentile confidence interval, the $BC_a$ (\textbf{bias-corrected accelerated} bootstrap) confidence interval described as follow: 
\newpage

\[\alpha_1=\Phi\Big(\hat{z}_0 + \frac{\hat{z}_0 + z_{\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{\alpha/2})}\Big) \]
\[\alpha_2=\Phi\Big(\hat{z}_0 + \frac{\hat{z}_0 + z_{1-\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{1-\alpha/2})}\Big) ,\]
where
\[\hat{a}=\frac{\sum_{i=1}^{n}(\bar\theta_J - \hat{\theta}_i)^3}{6\Big\{\sum_{i=1}^{n}(\bar\theta_J - \hat{\theta}_i)^2 \Big\}^{3/2}} \]
With $\bar{\theta}_J=n^{-1}\sum_{i=1}^{n}\hat{\theta}_{(i)}$, for $\hat{\theta}_{(i)}$ as the estimator of $\theta$ obtained without observation $i$, i.e., $\hat{\theta}_{(i)} = S(X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n)$. It is easily performed in \textbf{R} with \textbf{bootstrap::bcanon}.

\newpage
{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item Let's consider a Weibull distribution with scale parameter $\lambda$, shape parameter $k$, variance $\sigma^2$ and median $x_{med}$. From a sample $(x_1,\ldots,x_n)$ simulated from the Weibull distribution with $\lambda=13$ and $k=1$, we aim to to build confidence intervals for $\sigma$ based on a statistics $\hat{s}^2=(n-1)^{-1}\sum_{i=1}^{n}(x_i-\bar{X})^2$, and $x_{med}$ based on the sample median.
	\begin{itemize}
		\item First, the sample size is set as $n = 100$, the number of bootstrap replications $R = 1000$
and the number of Monte Carlo samples $M = 1000$. We build two-sided bootstrap percentile confidence intervals for $\sigma$ and $x_{med}$ at the
significance level $\alpha=0.05$, and  Use $M$ Monte Carlo samples to estimate the coverage
probability(CP) and the average interval length(AIL) for both confidence intervals(CI). We get the following results:\\
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
				\hline
				& $x_{med}$ CI & $\sigma$ CI \\
				\hline
				CP & 0.944  & 0.856 \\
				\hline  
				AIL & 5.103662 & 6.006730 \\
				\hline  
			\end{tabular}
		\caption{\label{tableau31}Confidence intervals coverage probability and average interval length: $n = 100$, $R = 1000$}
		\end{table}
	The coverage probability for $x_{med}$ confidence interval is pretty close to $1-\alpha = 0.95$, the same for $\sigma$ confidence interval, but less than the CP $x_{med}$ CI. This might suggest that the bootstrap percentile confidence interval approximates $x_{med}$ CI more than $\sigma$ CI.
	
	\item Now, use the following settings: $n = R = 1000$ to get the results in table \ref{tableau32} and $n = 100$, $R = 5000$ and obtain the corresponding results in table \ref{tableau33}
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.947 & 0.935 \\
			\hline  
			AIL & 1.620852 & 2.211146 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau32}Confidence intervals coverage probability and average interval length: $n = R = 1000$}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.946 & 0.848 \\
			\hline  
			AIL & 5.101970 & 5.981314 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau33}Confidence intervals coverage probability and average interval length: $n = 100$, $R = 5000$}
	\end{table}
	We can notice that the CP value for both confidence intervals are again close to $0.95$, but huge differences with the AIL. Actually, we want the length of the confidence intervals to be narrow as possible, and the AIL in table \ref{tableau32} are the smallest AIL, and the CP are the largest. Hence, increasing the sample size and the number of bootstraps replications improves the accuracy of the bootstrap.
	
	\item With  $n = 100$, $R = 1000$ and $M=1000$, we build bootstrap accelerated
bias-corrected ($bc_a$) confidence intervals both for $\sigma$ and $x_{med}$, and Use $M$ Monte Carlo samples to assess the coverage probability and the average length of the confidence intervals to obtain the following table.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.956 & 0.912 \\
			\hline  
			AIL & 5.030710 & 6.661682 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau34}Confidence intervals coverage probability and average interval length: $bc_a$}
	\end{table}
	The CP values for the $bc_a$ confidence intervals are more closer to $0.95$ (especially for $\sigma CI$)than the ones of the bootstrap percentile confidence intervals(see table \ref{tableau31}). We also notice a slight difference in the AIL for both confidence intervals in both methods. The following table 
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{0.5cm}|P{2.5cm}|P{2cm}|}
			\hline
			& $x_{med}$ & $\sigma$ \\
			\hline
			$\hat{z}_0$ & $-0.04063$ & 0.1113 \\
			\hline  
			$\hat{a}$ & $-1.475\times10^{-15}$ & 0.09085 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau35}Average $\hat{z}_0$ and $\hat{a}$}
	\end{table}
	
	\end{itemize}
\newpage
    \item From the dataset \textit{shhs1.txt} has been obtained from \href{https://sleepdata.org/datasets/shhs}{Sleep Heart Health Study}, we are using the variable \textbf{rdi4p}: respiratory disturbance index. Figure \ref{fig 3.1}, we notice that the \textbf{rdi4p} is skewed on the left.
    
    \begin{figure}[ht]
    	\centering
    	\scalebox{1.2}{\input{Ex3plot.tex}}
    	\caption{Histogram of \textbf{rdi4p} with the empirical distribution}
    	\label{fig 3.1}
    \end{figure}
    
    By building bootstrap percentile and bootstrap accelerated bias-corrected confidence intervals for the standard deviation and
median, we get the following results (with $R = 1000$)
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|}
    		\hline
    		& $x_{med}$ & $\sigma$ \\
    		\hline
    		CI & [3.951, 4.419] & [11.785, 13.045] \\
    		\hline
    		CI$_{length}$ & 0.498 & 1.26 \\
    		\hline  
    	\end{tabular}
    	\caption{\label{tableau36}Results for bootstrap percentile confidence interval}
    \end{table}
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|}
    		\hline
    		& $x_{med}$ & $\sigma$ \\
    		\hline
    		CI & [3.944, 4.429] & [11.792, 13.103] \\
    		\hline
    		CI$_{length}$ & 0.485 & 1.311 \\
    		\hline
    		$\hat{z}_0$ & $0.00251$ & $-0.00251$ \\
    		\hline  
    		$\hat{a}$ & 0 & 0.0272 \\
    		\hline  
    	\end{tabular}
    	\caption{\label{tableau37}Results for $bc_a$}
    \end{table}
    The median of the variable \textbf{rdi4p} is 4.193012 and its standard deviation is 12.43283
    
\end{enumerate}

%44444444444444444444444444444444444444444444%

{\color{royalblue}\chapter{Generalised linear models}}
{\color{royalblue}\section{Problem description}}
In its simplest form, a linear model specifies the (linear) relationship between a dependent variable $Y$ (normally distributed), and a set of independent variables $X_i$, $i=1,\ldots,k\in \mathbb{N}$, so that $Y= b_0 + b_1X_1 +\ldots+ b_kX_k$, where $b_0$ is the regression coefficient for the intercept and the $b_i$ values are the regression coefficients (for variables 1 through k). However, there are many relationships that cannot adequately be summarized by a simple linear equation, for two major reasons:
\begin{itemize}
	\item  \textit{Distribution of the dependent variable.} The dependent variable of interest may have a non-continuous distribution, and thus, the predicted values should also follow the respective distribution; any other predicted values are not logically possible.
	\item \textit{Link function.} The second reason why the linear model might be inadequate to describe a particular relationship is that the effect of the predictors on the dependent variable may not be linear in nature.
\end{itemize}
Generalized linear models (GLMs) extend linear models to accommodate both non-normal response distributions and transformations to linearity.

{\color{royalblue}\section{Methods}}

Let $(Y_1, X_1),\ldots,(Y_n, X_n)$ be independent pairs of observations, where $Y_i$ is real-valued
and $X_i$ are $\mathbb{R}^k$-valued random variables. Generalised linear models (GLMs) have the following
three-part specification:

\begin{itemize}
	\item \textbf{The random component} (=response from an overdispersed exponential family). The data $Y_1,\ldots,Y_n$ are such that $Y_1|X_1,\ldots, Y_n|X_n$ are independent and $Y_i|X_i$ has
the p.d.f.
	\[ f_{\eta,\psi}(y_i|x_i) = \mathtt{exp}\bigg\{   \frac{\eta_iy_i - \kappa(\eta_i)}{\psi_i} \bigg\}h(y_i,\psi_i), \hspace{0.5cm} i=1,\ldots,n, \]
	where $\eta_i$ is called canonical parameter and i is an unknown scale or dispersion parameter. Functions $\kappa$ and $h$ are known and $\kappa^{''}(\eta) > 0$ is assumed. Note that 
	\[ \mu(\eta_i):=\mathtt{E}(Y_i|X_i) = \kappa_0(\eta_i) \hspace{0.25cm}\mathtt{and}\hspace{0.25cm} var(Y_i|X_i)=\psi_i\kappa^{''}(\eta), \hspace{0.5cm} i=1,\ldots,n \]
	\item \textbf{The systematic component} (=linear predictor) Canonical parameter $\eta_i$ is assumed to be related to $X_i$. The term $X^t_i\beta$ for unknown
$\beta \in \mathbb{R}^d$ is called the \textbf{linear predictor or systematic component}.
	\item The \textbf{link function} between random and systematic components. The relationship between $\eta_i$ and $X^t_i\beta$ is described through
	\[ g\{\mu(\eta_i)\} = X^t_i\beta, \hspace{0.5cm} i=1,\ldots,n \]
	where g is called a link function. The link function g is assumed to be a known, one-to-one, third-order continuously differentiable function. If $g=\mu^{-1}$ then $\eta_i=X^t_i\beta$, and $g$ is called the \textbf{canonical or natural link function}. If $g$ is not canonical, then

	it is assumed that $d(g\circ\mu)(\eta)/d\eta \ne 0$ for all $\eta$.	
\end{itemize}

In a GLM, the parameter of interest is $\beta$. Parameters $\psi_i$ are considered to be nuisance parameters. It is often assumed that $\psi_i = \psi/t_i$, $i = 1,\ldots,n$ with an unknown
$\psi$ and known $t_i$'s or, alternatively $\psi_i =a( )$ for some known function $a$. Note that $\psi_i$
enter $var(Y_i|X_i) = \psi_i\kappa{''}(\eta_i)$, making it more flexible, that is allowing for over- or underdispersion.\\

\textbf{Exemple:} Let Let $Y_i|X_i \backsim Poi(\lambda_i)$. We can write the density
\[ f_\eta(y_i)= \mathtt{exp}\{ y_ilog(\lambda_i) - \lambda_i \}\frac{1}{y_i!}\mathbb{I}_{\{ 1,2,\ldots \}}(y_i) \]
that is, the canonical parameter $\eta_i = \mathtt{log}(\lambda_i), \kappa(\eta_i) = \lambda_i = \mathtt{exp}(\eta_i)$, $\psi_i = 1$ and $h(y_i) = (y_i)^{-1}\mathbb{I}_{\{ 1,2,\ldots \}}(y_i)g(y_i)$. Since $E(Y_i|X_i) = \kappa_0(\eta_i) = \mathtt{exp}(\eta_i) =: \mu(\eta_i)$, the canonical link is

$g(x) = \mu^{-1}(x) = \mathtt{log}(x)$, which is called the \textbf{log-link} $(g(\mu(\eta_i)) = \eta_i)$. Hence, 
\[ \mathtt{log}\{\mathtt{E}(Y_i|X_i=x_i)\} = x^t_i\beta,\]
where $x_i \in \mathbb{R}^k,\hspace{0.25cm} i=1,\ldots,n$.\\

\begin{center}
	\textbf{\large Estimation}
\end{center}
Let $\theta = (\beta, \psi)$ and $(g\circ\mu)^{-1} = \zeta$ (for a canonical link $\zeta(x) \equiv x$). Then

\[ \ell(\theta)= \sum_{i=1}^{n} \bigg[ \frac{\zeta(X^t_i\beta)Y_i-\kappa\{\zeta(X^t_i\beta)\}} {a(\psi)} + \mathtt{log}h(Y_i,\psi)  \bigg] .\]
Further, consider the canonical link. Taking derivatives w.r.t. $\beta$ and we get the following
score equations
\[  \frac{\partial \ell(\theta)}{\partial\beta}= \frac{1}{a(\psi)} \sum_{i=1}^{n} \{ Y_i - \mu(X^t_i) \}X_i=0 \]

\[ \frac{\partial \ell(\theta)}{\partial\psi}= \sum_{i=1}^{n}\bigg[ \frac{\partial\mathtt{log}h(y_i,\psi)}{\partial\psi}+ \{a^{-1}(\psi)\}^{'}\{ X^t_i\beta Y_i+ \kappa(X^t_i\beta) \} \bigg]= 0 \]

Where $ \kappa(X^t_i\beta)= \mu(X^t_i\beta)$ was used. If MLE of $\beta$ exists, then it can be found from the
first equation without estimating. Estimation of $\psi$ from the second equation in many cases is a difficult task and depends on a particular distribution. To estimate $\beta$ and study its properties we also need
\[-\frac{\partial^2\ell(\theta)}{\partial\beta\partial\beta^t}= \frac{1}{a(\psi)} \sum_{i=1}^{n}\bigg[ \kappa(X^t_i\beta)^{''}X_i X^t_i \bigg]=: -\frac{F_n(\beta)}{a(\psi)} \]
With this, we can set up the Newton-Raphson algorithm as
\[ \hat\beta^{(j+1)}= \hat\beta^{(j)} +\big\{ F_n(\hat\beta^{(j)}) \big\}^{-1} S_n(\hat\beta^{(j)}), \hspace{0.25cm} j=0,1,2,\ldots, \]
where $S_n(\hat\beta^{(j)})= a(\psi)\partial\ell(\theta)/\partial\beta.$\\

\begin{center}
	\textbf{\large Goodness-of-fit and models' comparison}
\end{center}
Now, we want to to assess how good the model fits the data,i.e., to measure the discrepancy between the data $Y_i|X_i$ and estimated $\mathtt{E}(Y_i|X_i) = \mu_i$. First, some definitions. The \textbf{null model} is simplest model, and has only one parameter, representing a common mean $\mu$, say, for all $Y_i|X_i$. At the other extreme is the \textbf{full model}, which has $n$ parameters, one for each observation. The full model gives a baseline for measuring the discrepancy for an intermediate model with k parameters. Assume for the moment that $\psi$  is knownand denote $\ell(\hat{\mu},\psi)$ the log-likelihood with $\hat{\mu} = g^{?1}(X\hat{\beta})$. The maximum likelihood in thefull model is then $\ell(Y, \psi)$ ($=\mu_i$ are replaced by $Y_i$). Then the \textbf{deviance of the fitted	model} is defined as
\[ D(Y,\hat{\mu})= a(\psi)2\{ \ell(Y,\psi)- \ell(\hat{\mu}, \psi) \}  \]
Note that $D(Y,\hat{\mu})/a(\psi)$ is called the \textbf{scaled deviance}(or the deviance for $2\{ \ell(Y,\psi)- \ell(\hat{\mu}, \psi) \}$).
The \textbf{generalised Pearson statistic} is defined via
\[ \chi^2=  \frac{\sum_{i=1}^{n}(Y_i-\hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]. The following methods are used to measure the goodness-of-fit, and compare models:

\begin{itemize}
	\item  \textbf{Analysis of deviance:} Scaled deviance can be used to compare two nested models, i.e. the parameter spaceunder one model is a subspace of that under the second model. let $M_k$ and $M_q$, with $q<k$ (k and q are the number of parameters in $M_k$ and $M_q$ respectively) two nested models. Let us denote $D_{M_k}$ and $D_{M_q}$ respectively as the scaled deviance of $M_k$ and $M_q$. Since we have assume that $\psi$ is known, we have the following formula:
	\[\frac{D_{M_q}- D_{M_k}}{\psi} \overset{approx}{\sim} \chi^2_{k-q}\] 
	A widely usedrule of thumb(to measure goodness-of-fit) is that a good fit has the scaled deviance about $n ? k$, which is the expectation of a $\chi^2_{n-k}$ distributed random variable. Large values of the scaled deviance areconsidered to indicate a bad fit. However, this has to be treated with care. For Poissondata with large $\lambda_i$ and Binomial data with large $m_i$, the approximation to $\chi^2_{n-k}$ worksreasonable, but not in many other cases. Therefore we can use other methods.
	
	\item \textbf{Residual analysis:} Here, the residuals used are expected to behave approximately as zero-mean normally distributed variables. \textbf{Pearson residuals} defined via
	\[ r^p_i= \frac{Y_i-\hat{\mu}_i}{d\sqrt{V(\hat{\mu}_i)}}, \hspace{0.25cm} i=1,\dots,n \]. Pearson residuals have the disadvantage of being skewed for non-normal responses. As a remedy, we have the \textbf{ Anscombe residulas}, which in the special case of the Poisson distribution is given by \[ r^a_i= \frac{3(Y^{2/3}_i-\hat{\mu}^{2/3}_i)}{2\hat{\mu}^{1/6}_i}, \hspace{0.25cm} i=1,\dots,n \].
	\item \textbf{Deviance residuals:} based on the deviance, they are defined by \[ r^d_i= \mathtt{sign}(Y_i-\hat{\mu}_i)\sqrt{2\{  \ell_i(Y_i,\psi)- \ell_i(\hat{\mu}_i,\psi)\}}, \hspace{0.25cm} i=1,\dots,n \]
	where $\ell_i$ is the log-likelihood corresponding to the $i$-th observation, so that $\sum_{i=1}^{n}(r^d_i)^2= D(Y, \hat{\mu})$). A standardised version of the deviance (as well as Pearson)residuals are used: \[ \frac{r^d_i}{\sqrt{a(\hat{\psi})(1-h_i)}}, \hspace{0.25cm}, i=1,\dots,n\] 
	where $h_i = H_{i,i}$ with the hat matrix $H$ taking now the form $H = W^{1/2}X(X^t W X)^{-1}X^t W^{1/2}$, where W is the weight matrix from the Fisher scoring. In an adequate model the plotof standardised residuals against $\hat{\eta}= X\hat{\beta}$ should show \textit{no patterns}. The \textbf{null pattern} is a distribution of residuals with mean zero and constant variance.
	
	\item \textbf{Akaikeinformation criterion (AIC) and Bayes information criterion (BIC):} These criterions can be used to compare models with different subset of parameters or even to compare two differentmodels (e.g., with different link functions or a non-linear and with a linear model). Thesetwo criteria are most popular examples of penalised goodness-of-fit criteria
	\[AIC(M)= -2\ell(M)+ 2|M|\]
	\[BIC(M)= -2\ell(M)+ \mathtt{log}(n)|M|,\] where $\ell(M)$ denotes the log-likelihood corresponding to a model $M$ and $|M|$ is the number of parameters in that model $M$. The models, selected with these criteria are then
	\[ \hat{M}_{AIC}= \mathtt{arg}\min_{M\in \mathcal{M}}AIC(M)  \]
	\[ \hat{M}_{BIC}= \mathtt{arg}\min_{M\in \mathcal{M}}BIC(M)  \]	
\end{itemize}
\newpage

{\color{royalblue}\section{Results}}
For the exercise, the dataset \textit{student-mat.csv} can be found on \href{https://www.kaggle.com/uciml/student-alcohol-consumption}{Kaggle}. Variables G1, G2, G3 are first, second and finalgrades in mathematics. The remaining variables are explanatory variables. We would liketo identify variables that explain grades in mathematics.

\begin{enumerate}[label=(\alph*)]
	\item 
	
	
	\begin{figure}
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot1.tex}}
			\caption{Emperical densities}
			\label{fig 4.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot2.tex}}
			\caption{Q-Q plot with normal theoretical distribution}
			\label{fig 4.2}
		\end{subfigure}
		\caption{Checking normality assumption}
		\label{fig:roc_curve}
	\end{figure}
	
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex4plot3.tex}}
		\caption{Q-Q plot with Poisson as theoretical distribution}
		\label{fig 4.3}
	\end{figure}
	
	
	
	
\end{enumerate}





%55555555555555555555555555555555555555555555%

{\color{royalblue}\chapter{Survival analysis}}
{\color{royalblue}\section{Problem description}}
We want to analyze data where the outcome variable is the time until the occurrence of an event of interest. The event can be death, occurrence of a disease, marriage, divorce, etc. The time to event or survival time can be measured in days, weeks, years, etc. For example, if the event of interest is heart attack, then the survival time can be the time in years until a person develops a heart attack. subjects are usually followed over a specified time period and the focus is on the time at which the event of interest occurs. Why not use linear regression to model the survival time as a function of a set of predictor variables? First, survival times are typically positive numbers; ordinary linear regression may not be the best choice unless these times are first transformed in a way that
removes this restriction. Second, and more importantly, ordinary linear regression cannot effectively handle the censoring of observations. Why not compare proportion of events in your groups using risk/odds ratios or logistic regression? Simply because it ignores time. \\
To tackle these issues, we'll use some survival analysis methods.

{\color{royalblue}\section{Methods}}

{\color{royalblue}\section{Results}}

%66666666666666666666666666666666666666666666%

{\color{royalblue}\chapter{Kernel density estimation}}
{\color{royalblue}\section{Problem description}}

Consider observations which are realizations of univariate random variables,
$X_1, \ldots, X_n \sim F$ where $F$ denotes an unknown cumulative distribution function. The goal is to estimate the distribution $F$. In particular, we are interested in estimating the density $f = F^{'}$, assuming that it exists.\\
Instead of assuming a parametric model for the distribution (e.g. Normal distribution
with unknown expectation and variance), we rather want to be "as general as possible": that is, we only assume that the density exists and is suitably smooth (e.g. differentiable). It is then possible to estimate the unknown density function $f(\cdot)$.

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%77777777777777777777777777777777777777777777%

{\color{royalblue}\chapter{Nonparametric regression: local polynomials}}
{\color{royalblue}\section{Problem description}}

To study the relation between a dependent variable $Y$ and an independent variable $X$, the common method used is linear regression. When appropriate, this method is very useful as its suppose a simple model of the form 
\begin{equation}
Y = \beta_{0} + \beta_{i}x_{i} + \epsilon_{i}
\end{equation}
This is advantageous since it is easy to interpret and to calculate. Moreover, when the assumptions on the residues $\epsilon_{i}$  are verified, we can run some tests on the parameters. \\
However, the restricted assumption of linearity is frequently not fulfilled, eventually when the data set is very large. In that case, we would like to find a complex model that will better highlight the relation between $Y$ and $X$. A first approach for this aim would be to specify another parametric form for this relation, for example a transformation of the observations or a polynomial regression. Nonetheless it remains difficult to find the suitable relation since the form of the data does not really change after these transformations. That is why in this section, we opt for a non-parametric regression technique (local polynomials) in which data choose their own form of relation (the predictor does not take a predetermined form but is constructed according to information derived from the data) making things more flexible.

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%88888888888888888888888888888888888888888888%

{\color{royalblue}\chapter{Nonparametric regression: splines}}
{\color{royalblue}\section{Problem description}}

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%99999999999999999999999999999999999999999999%

{\color{royalblue}\chapter{Mixed models}}
{\color{royalblue}\section{Problem description}}
To illustrate the targeted problem ib this section, we use the following example. Let us consider the following linear model,\\
\begin{equation}
Y_{i,t} = \beta_{0} + \beta_{i}t + \epsilon_{i,t}
\end{equation}
Here, $\beta_{0}$ and $\beta_{i}$
 

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%10101010101010101010101010101010101010101010%

{\color{royalblue}\chapter{Partial least squares}}
{\color{royalblue}\section{Problem description}}

In a standard linear model, we have at our disposal $(X_i, Y_i)$ supposed to be linked with,
\begin{equation}
Y_i = X_{i}^{t}\beta+ \epsilon_{i}, \hspace{1cm}  1 \leqslant i \leqslant n
\end{equation}
In particular, each observation $X_i$ is described by p variables $(X_1, \ldots, X_n)$ so that the former relation should be understood as
\begin{equation}
Y_i = \sum_{j = 1}^{p} \beta_j X_{i}^{j}+ \epsilon_{i}, \hspace{1cm}  1 \leqslant i \leqslant n
\end{equation}

From a matricial point of view, the linear model can we written as follows :
\begin{equation}
Y_i = X \beta_0 + \epsilon_{i},\hspace{1cm}  Y \in \mathbb{R}^n, X \in \mathcal{M}_{n,p},  \beta_{0} \in \mathbb{R}^p
\end{equation}

A classical "optimal" estimator is the MLE :
\begin{equation}
\hat{\beta}_{MLE} := (X^{t}X)^{-1} X^{t}Y
\end{equation}

This can be obtained while remarking that J is a convex function, that possesses a unique minimizer if and only if $X^{t}X$ has a full rank, meaning that $J$ is indeed strongly convex :
\begin{equation}
 D^2 J= X^{t}X
\end{equation}
Which is a squared $p × p$ symmetric and positive matrix. It is non degenerate if $X^{t}X$ has full rank, meaning that necessarily $p \leqslant n$.\\

In large dimensional case, we often have $p > n$, hence a problem when applying linear regression in this case:\\
$X^{t}X$ is an $p × p$ matrix, but its rank is lower than $n$. If $n << p$, then
\begin{equation}
rk(X^{t}X) \leqslant n << p
\end{equation}
Consequently, the the Gram matrix $X^{t}X$ is not invertible and even very ill-conditionned (most of the eigenvalues are 0 !). The linear model $\hat{\beta}_{MLE}$ completely fails.\\
As a remedy to this problem that occurs most of the time in big data analysis, we will make use of the partial least squares (PLS) method.

{\color{royalblue}\section{Methods}}

{\color{royalblue}\section{Results}}


\end{document}