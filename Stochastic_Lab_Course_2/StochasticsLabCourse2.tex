\documentclass{report}
%chapter style Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, %Bjornstrup
\usepackage[Bjarne]{fncychap}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\numberwithin{equation}{section} %for equations numbering
\usepackage{amssymb}
\usepackage[top=5cm, bottom=5cm, left=3cm, right=3cm]{geometry}
\pagestyle{headings}
\usepackage{lmodern}
\usepackage{enumitem}
\usepackage{float, graphicx}
\usepackage[%  
colorlinks=true,
pdfborder={0 0 0},
linkcolor=red
]{hyperref}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\DeclareMathOperator{\argmax}{argmax}
\newtheorem{theorem}{Theorem}
%define my own color
\definecolor{royalblue}{rgb}{0.0, 0.14, 0.4}

\usepackage{tikz} %for .tex vector images

%pagestyle
\pagestyle{fancy}
\fancyhead[RE,LO]{{\color{royalblue}Stochastics Lab Course II}}
\fancyhead[LE,RO]{\leftmark}
\fancyfoot[RE,LO]{\rightmark}
{\color{royalblue}\fancyfoot[LE,RO]{\thepage}}
\cfoot{}

%decorative lines
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headrule}{\hbox to\headwidth{
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\footrule}{\hbox to\headwidth{%
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}

\title{\textsc{Stochastics Lab Course II}}
\author{Khwam Tabougua Trevor}
\date{March 2019}
\begin{document}
	
\maketitle

{\color{royalblue}\chapter*{Introduction}}

The "Stochastics Lab course II" is an Introductory Course for
statistics and stochastics applications with R programming language. The course lasted for two weeks in March 2019. The report written on \LaTeX, contains results, interpretations and figures from the ten exercises that had to be solved. Along with this report, there is also the R codes, which are recommended to understand the result.

%1111111111111111111111111111111111111111111111%
\tableofcontents
{\color{royalblue}\chapter{Tidyverse}}
{\color{royalblue}\section{Problem description}}
R base tools can accomplish "almost" every programming tasks. However, when using large datasets or when implementing complex tasks(like graphs, maps, tidying, etc), things get complicated. We want to enhance our algorithms fo batter results or productivity. To this aim, we will use the Tidyverse package.

{\color{royalblue}\section{Methods}}
Tidyverse is a collection of packages for data manipulation, exploration and visualization. The core packages are \textbf{ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats}, but we will only be using ggplot2, dplyr, tidyr, and tibble.
\begin{itemize}
	\item[--] \textbf{ggplot2} is a system for declaratively creating graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
	
	\item[--] \textbf{dplyr} is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges such as adding new variables (that are functions of existing variables), picking variables based on their names, selecting rows (based on their value), reducing multiple values down to a single summary, and changing the ordering of the rows.
	
	\item[--] \textbf{tidyr} package goal is to help you create tidy data. Tidy data is data where each variable is in a column, each observation is a row, and Each value is a cell.
	
	\item[--] \textbf{tibble} package goal is to use tibbles, which are modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors).
	
	
\end{itemize}

{\color{royalblue}\section{Results}}

\begin{enumerate}[label=(\alph*)]
	\item After loading and filtering the data childrenfinal.dta, we convert some variables (namely tetanusmother, breastfeeding, wantedchild, anetalvisits, and placedelivery) into double labeled \textit{<dbl>} (doubles, or real numbers).
	
	\item 
	\item[°]The \hyperlink{Figure 1.1}{figure 1.1} indicates that the effect of zstunt is negatively affecting te hypage.
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex1plot1.tex}}
		\caption{Scatter plot of zstunt against hypage with smooth line (in purple)}
	\end{figure}
	
	\item[°]gjdhgdhdg
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex1plot2.tex}}
		\caption{Some Meaningful Caption}
	\end{figure}
	
	\item[°]gjdhgdhdg
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex1plot3.tex}}
		\caption{Some Meaningful Caption}
	\end{figure} 
		
	\item 
	\begin{figure}[ht]
		\centering
		\includegraphics[scale=0.5]{Ex1plot4.png}
		\caption{Scatter plot of zstunt against hypage with smooth line (in purple)}
	\end{figure}	
		
		
\end{enumerate}	
%222222222222222222222222222222222222222222222%

{\color{royalblue}\chapter{Random number generation}}

{\color{royalblue}\section{Problem description}}
Generating a random variable from any distribution is very essential for stochastics studies. However, true random numbers are not always available, but we can use some algorithms that generate some pseudo-random numbers.\\

{\color{royalblue}\section{Methods}}
Pseudo-random numbers are a sequence of numbers that appear "random" or approximate properties of random numbers with the following properties:\\
\begin{itemize}
	\item Good approximation of the properties of random numbers.
	\item Number can be easily and efficiently generated.
	\item Reproducibility (truly random numbers never satisfy this).
\end{itemize}

With the help of some probability properties, it is typically enough to be able to use a uniform distributed random variable, in order to generate any pseudo-random numbers from a given distribution.\\
The simplest idea for generating uniformly distributed pseudo-random numbers is using a \textit{linear congruent generators} (LCG):\\
\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Choose positive integer parameters a, c and m.
				\item Choose an initial value $x_0 \in \{0, 1, \ldots, m - 1\}$ (this value is called the seed).
				\item For each $n \in \mathbb{N} $, compute\\
				\begin{equation}
				x_{n+1} := ax_{n} + c \hspace{0.2cm}
				\text{mod} \hspace{0.2cm} m, \hspace{1cm} \forall n \in \mathbb{N} 
				\end{equation}
				\item The psuedorandom numbers are the sequence $x_1; x_2; x_3; \ldots $.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We make some observations regarding the LCG algorithm.\\
\begin{itemize}
	\item The LCG can generates at most m distinct numbers which are contained in $\{0, 1, \ldots, m - 1\}$
	\item As soon as some number in the sequence, say ${x_n}_0$, is repeated (i.e.,$\exists p$ such that $x_{n_{0} + p} = {x_n}_0$),

	then the same is true for the entire sequence:
	\begin{equation}
	x_{n_{0} +p+j} = x_{n_{0} + j}, \hspace{0.5cm} \forall j \geqslant 1
	\end{equation}
	The number $p$ is called the period of the sequence. This musst be less than or equal to $m$.
	\item Suppose that (under the right conditions) the LCG approximates a uniform distribution with

	parameters $0$ and $m$. In this case, $x_n / m$ would approximate a uniform distribution with parameters
0 and 1.
\end{itemize}
To generate uniformly random numbers, the period has to be maximal
(i.e., $p = m$), so that we sample every value in the sequence before repeating any. One can show that LCG has a full period $m = 2^{b}$, $b \geqslant 2$ if and
only if $c \in (0; m)$ is odd and $a$ mod $4 = 1$.\\

After generating uniform pseudo-random numbers, we can easily obtain random variables from other distributions. The \textbf{inversion method} is one way of doing so, with the help of the following theorem:
\begin{theorem}
	Let F be a distribution function on $\mathbb{R}$. The quantile function $F^{-1}$ is defined by
	 \[ F^{-1}(u) = \text{inf} \{x: F(x)\ge u, 0 < u < 1 \} \]
	If $U \sim U_{[0; 1]}$, then $F^{-1}(u)$ has a distribution function
$F$.
\end{theorem}
Hence, for continuous distributions (exponential, Pareto, standard Cauchy, etc) where $F^{-1}$, we simply simulate $U_{i}$ (with LCG) and set $X_{i} = F^{-1}(U_i)$. If $F^{-1}$ cannot be inverted analytically, appropriate
numerical methods can be applied.\\

Let $X$ be a discrete random variable with ordered possible values $\{x_{1}, x{2},\ldots \}$, so that $F(x) = \displaystyle\sum_{i:x_{i} \leqslant x} P(X = x_{i})$ and
\[F^{-1}(r) =  \text{min} \{x_k \in \{x_{1}, x_{2},\ldots \}: \displaystyle\sum_{j=1}^{k} P(X = x_{j}) = \displaystyle\sum_{j=1}^{k}p_{j} \geqslant r\}\]
Then the inverse method becomes: set $X = x_{1}$ if and only if $U_{i} \in [0, p_{1})$ and $X = x_{k}$ if and only if
$U_{i} \in \Big[ \displaystyle\sum_{j=1}^{k-1}p_{j}, \displaystyle\sum_{j=1}^{k}p_{j}\Big)$, $k = 2, 3, \ldots$. Note that

\[P(X = x_{k}) = P\Big(\displaystyle\sum_{j=1}^{k-1}p_{j} \leqslant U_{i} < \displaystyle\sum_{j=1}^{k}p_{j} \Big) = \displaystyle\sum_{j=1}^{k}p_{j} - \displaystyle\sum_{j=1}^{k-1}p_{j} = p_{k} \]

For example, to simulate a Bernoulli random variable $Ber(p)$, generate $U \in U_{[0, 1]}$ and

set $X = 0$, if $U \leqslant 1 - p$ and $X = 1$ if $U > 1 - p$.\\

Another general approach to pseudo-random variables generation is the \textbf{acceptance-rejection method}.

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item [] \textbf{Data}: Two probability densitiy functions: $f$ for $X$ and $g$ for $Y$
				\item Find a constant $M > 0$ such that $sup_{x} \frac{f(x)}{g(x)} \leqslant c$ ;
				\item Obtain a sample $y$ from $Y$ ;
				\item Obtain a sample $u$ from the uniform distribution on $[0, 1]$;
				\item \textbf{if} $u < \dfrac{f(y)}{cg(y)}$ \textbf{then}
				\item $\bigg|$ Accept $y$ as a sample drawn from $f$;
				\item \textbf{else}
				\item $\bigg|$ Reject the value of $y$ and return to the sampling step (line 2);
				\item [] \textbf{Result:} $y$, a sample drawn from $f$ (using $g$)
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

\textbf{NB:} Computing $c$ could be difficult, but one can show that $c = sup_{x} {\frac{f(x)}{g(x)}}$\\

{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item 
	With the Wichmann-Hill pseudo-random number generator in R, we Simulate $N =
1000 $ binomial random variables $B(n = 10, p = 0.4)$ using three approaches: inversion

	method, by simulating corresponding Bernoulli random variables by inversion method
and using R built-in function rbinom. From the figure \ref{fig 2.1}, the histograms of the three samples present the same shape but are different. This proves that the "random" numbers generated by our methods are just approximations of true random numbers.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot1.tex}}
		\caption{Histogram of the empirical CDF of all
three samples}
		\label{fig 2.1}
	\end{figure}
	
	\item To use accept-reject method (and a generator for uniform random variables only), of $N = 10000$ standard normal distributed random variables
with density 
	$f(x) = (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}} $, the density of the
standard Cauchy distribution is used: 
	$ g(x)=\{{\pi(1+x^2)}^{-1}\} $.
	
	\item[°] The constant value $c$ for this method is given by $sup_{x} {\frac{f(x)}{g(x)}} = 1.520347 $
	\item[°] Then after computing the $N$ standard normal random variables, we notice that the estimated and theoretical acceptance probabilities are almost equal. This is well depicted with in the figure \ref{fig 2.2}, where the histogram of the obtain sample is symmetric and has the same shape as the standard normal density curve.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot2.tex}}
		\caption{Histogram of the obtained sample and the standard
		normal density (in blue)}
	\label{fig 2.2}
	\end{figure}
	
	\item[°] The QQ-plot in figure \ref{fig 2.3} shows points following the identity line. Hence the accept-reject method used to simulate a standard normal distributed sample (using the the standard Cauchy density) is well accurate.
	
	\newpage
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot3.tex}}
		\caption{QQ-plot}
		\label{fig 2.3}
	\end{figure}
	
	\item[°] However, it is not possible to simulate ample distributed from the standard Cauchy density using
the accept-reject method with a standard normal candidate density, simply because cannot find a $c$ such that $g(x) \leqslant cf(x)$ is verified ( because $ sup_{x} {\frac{g(x)}{f(x)}} = \infty $).
	
\end{enumerate}

%333333333333333333333333333333333333333333333%

{\color{royalblue}\chapter{Bootstrap}}
{\color{royalblue}\section{Problem description}}
Suppose that a sample $ X = \{X_1,\ldots ,X_n \} $ is used to estimate a parameter $\theta$ of the distribution $P$ (which is unknown) and let $\hat{\theta} = S(X)$ be a statistic that estimates $\theta$. For the purpose of statistical inference on $\theta$,  we are interested in the sampling distribution of $\hat{\theta}$ (or certain aspects of it) so as to assess the accuracy of our estimator or to set confidence intervals for our estimate of $\theta$. If the true distribution $P$ were known, we could draw samples $X^{(b)}, b = 1,\ldots, B$ from P and use Monte Carlo methods to estimate the sampling distribution of our estimate $\hat{\theta}$. The problem is that $P$ is unknown and we cannot sample from it.\\
The following section explains how to use bootstrap to make the interference on $\hat{\theta}$.

{\color{royalblue}\section{Methods}}
Bootstrap:
algorithm:
Bootstrap confidence intervals:


{\color{royalblue}\section{Results}}

%44444444444444444444444444444444444444444444%

{\color{royalblue}\chapter{Generalised linear models}}
{\color{royalblue}\section{Problem description}}

{\color{royalblue}\section{Methods}}

{\color{royalblue}\section{Results}}

%55555555555555555555555555555555555555555555%

{\color{royalblue}\chapter{Survival analysis}}
{\color{royalblue}\section{Problem description}}
We want to analyze data where the outcome variable is the time until the occurrence of an event of interest. The event can be death, occurrence of a disease, marriage, divorce, etc. The time to event or survival time can be measured in days, weeks, years, etc. For example, if the event of interest is heart attack, then the survival time can be the time in years until a person develops a heart attack. subjects are usually followed over a specified time period and the focus is on the time at which the event of interest occurs. Why not use linear regression to model the survival time as a function of a set of predictor variables? First, survival times are typically positive numbers; ordinary linear regression may not be the best choice unless these times are first transformed in a way that
removes this restriction. Second, and more importantly, ordinary linear regression cannot effectively handle the censoring of observations. Why not compare proportion of events in your groups using risk/odds ratios or logistic regression? Simply because it ignores time. \\
To tackle these issues, we'll use some survival analysis methods.

{\color{royalblue}\section{Methods}}

{\color{royalblue}\section{Results}}

%66666666666666666666666666666666666666666666%

{\color{royalblue}\chapter{Kernel density estimation}}
{\color{royalblue}\section{Problem description}}

Consider observations which are realizations of univariate random variables,
$X_1, \ldots, X_n \sim F$ where $F$ denotes an unknown cumulative distribution function. The goal is to estimate the distribution $F$. In particular, we are interested in estimating the density $f = F^{'}$, assuming that it exists.\\
Instead of assuming a parametric model for the distribution (e.g. Normal distribution
with unknown expectation and variance), we rather want to be "as general as possible": that is, we only assume that the density exists and is suitably smooth (e.g. differentiable). It is then possible to estimate the unknown density function $f(\cdot)$.

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%77777777777777777777777777777777777777777777%

{\color{royalblue}\chapter{Nonparametric regression: local polynomials}}
{\color{royalblue}\section{Problem description}}

To study the relation between a dependent variable $Y$ and an independent variable $X$, the common method used is linear regression. When appropriate, this method is very useful as its suppose a simple model of the form 
\begin{equation}
Y = \beta_{0} + \beta_{i}x_{i} + \epsilon_{i}
\end{equation}
This is advantageous since it is easy to interpret and to calculate. Moreover, when the assumptions on the residues $\epsilon_{i}$  are verified, we can run some tests on the parameters. \\
However, the restricted assumption of linearity is frequently not fulfilled, eventually when the data set is very large. In that case, we would like to find a complex model that will better highlight the relation between $Y$ and $X$. A first approach for this aim would be to specify another parametric form for this relation, for example a transformation of the observations or a polynomial regression. Nonetheless it remains difficult to find the suitable relation since the form of the data does not really change after these transformations. That is why in this section, we opt for a non-parametric regression technique (local polynomials) in which data choose their own form of relation (the predictor does not take a predetermined form but is constructed according to information derived from the data) making things more flexible.

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%88888888888888888888888888888888888888888888%

{\color{royalblue}\chapter{Nonparametric regression: splines}}
{\color{royalblue}\section{Problem description}}

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%99999999999999999999999999999999999999999999%

{\color{royalblue}\chapter{Mixed models}}
{\color{royalblue}\section{Problem description}}
To illustrate the targeted problem ib this section, we use the following example. Let us consider the following linear model,\\
\begin{equation}
Y_{i,t} = \beta_{0} + \beta_{i}t + \epsilon_{i,t}
\end{equation}
Here, $\beta_{0}$ and $\beta_{i}$
 

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%10101010101010101010101010101010101010101010%

{\color{royalblue}\chapter{Partial least squares}}
{\color{royalblue}\section{Problem description}}

In a standard linear model, we have at our disposal $(X_i, Y_i)$ supposed to be linked with,
\begin{equation}
Y_i = X_{i}^{t}\beta+ \epsilon_{i}, \hspace{1cm}  1 \leqslant i \leqslant n
\end{equation}
In particular, each observation $X_i$ is described by p variables $(X_1, \ldots, X_n)$ so that the former relation should be understood as
\begin{equation}
Y_i = \sum_{j = 1}^{p} \beta_j X_{i}^{j}+ \epsilon_{i}, \hspace{1cm}  1 \leqslant i \leqslant n
\end{equation}

From a matricial point of view, the linear model can we written as follows :
\begin{equation}
Y_i = X \beta_0 + \epsilon_{i},\hspace{1cm}  Y \in \mathbb{R}^n, X \in \mathcal{M}_{n,p},  \beta_{0} \in \mathbb{R}^p
\end{equation}

A classical "optimal" estimator is the MLE :
\begin{equation}
\hat{\beta}_{MLE} := (X^{t}X)^{-1} X^{t}Y
\end{equation}

This can be obtained while remarking that J is a convex function, that possesses a unique minimizer if and only if $X^{t}X$ has a full rank, meaning that $J$ is indeed strongly convex :
\begin{equation}
 D^2 J= X^{t}X
\end{equation}
Which is a squared $p × p$ symmetric and positive matrix. It is non degenerate if $X^{t}X$ has full rank, meaning that necessarily $p \leqslant n$.\\

In large dimensional case, we often have $p > n$, hence a problem when applying linear regression in this case:\\
$X^{t}X$ is an $p × p$ matrix, but its rank is lower than $n$. If $n << p$, then
\begin{equation}
rk(X^{t}X) \leqslant n << p
\end{equation}
Consequently, the the Gram matrix $X^{t}X$ is not invertible and even very ill-conditionned (most of the eigenvalues are 0 !). The linear model $\hat{\beta}_{MLE}$ completely fails.\\
As a remedy to this problem that occurs most of the time in big data analysis, we will make use of the partial least squares (PLS) method.

{\color{royalblue}\section{Methods}}

{\color{royalblue}\section{Results}}


\end{document}