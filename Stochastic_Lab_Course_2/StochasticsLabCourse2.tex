\documentclass{report}
%chapter style Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, %Bjornstrup
\usepackage[Bjarne]{fncychap}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\numberwithin{equation}{section} %for equations numbering
\usepackage{amssymb}
\usepackage[top=5cm, bottom=5cm, left=3cm, right=3cm]{geometry}
\pagestyle{headings}
\usepackage{lmodern}
\usepackage{enumitem}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{float, graphicx, subcaption}
\usepackage{flafter}
\usepackage[%  
colorlinks=true,
pdfborder={0 0 0},
linkcolor=red
]{hyperref}
\usepackage{fancyhdr}
\usepackage{caption} %to fix referencing to exact the label
\usepackage[dvipsnames]{xcolor}
\DeclareMathOperator{\argmax}{argmax}
\newtheorem{theorem}{Theorem}
%define my own color
\definecolor{royalblue}{rgb}{0.0, 0.14, 0.4}

\usepackage{tikz} %for .tex vector images

%pagestyle
\pagestyle{fancy}
\fancyhead[RE,LO]{{\color{royalblue}Stochastics Lab Course II}}
\fancyhead[LE,RO]{\leftmark}
\fancyfoot[RE,LO]{\rightmark}
{\color{royalblue}\fancyfoot[LE,RO]{\thepage}}
\cfoot{}

%decorative lines
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headrule}{\hbox to\headwidth{
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\footrule}{\hbox to\headwidth{%
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}

\title{\textsc{Stochastics Lab Course II}}
\author{Khwam Tabougua Trevor}
\date{March 2019}
\begin{document}
	
\maketitle

{\color{royalblue}\chapter*{Introduction}}

The "Stochastics Lab course II" is a practical Course for statistics and stochastics applications with R programming language. The course lasted for two weeks in March 2019. The report written on \LaTeX, contains a description of the problem, a description of the methods used
to solve the problem and a detailed discussion of the results.

%1111111111111111111111111111111111111111111111%
\tableofcontents
{\color{royalblue}\chapter{Tidyverse}}
{\color{royalblue}\section{Problem description}}
R base tools can accomplish "almost" every programming tasks. However, when using large datasets or when implementing complex tasks(like graphs, maps, tidying, etc), things get complicated. We want to enhance our algorithms fo batter results or productivity. To this aim, we will use the Tidyverse package.

{\color{royalblue}\section{Methods}}
Tidyverse is a collection of packages for data manipulation, exploration and visualization. The core packages are \textbf{ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats}, but we will only be using ggplot2, dplyr, tidyr, and tibble.
\begin{itemize}
	\item[--] \textbf{ggplot2} is a system for declaratively creating graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
	
	\item[--] \textbf{dplyr} is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges such as adding new variables (that are functions of existing variables), picking variables based on their names, selecting rows (based on their value), reducing multiple values down to a single summary, and changing the ordering of the rows.
	
	\item[--] \textbf{tidyr} package goal is to help you create tidy data. Tidy data is data where each variable is in a column, each observation is a row, and Each value is a cell.
	
	\item[--] \textbf{tibble} package goal is to use tibbles, which are modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors).
	
	
\end{itemize}

{\color{royalblue}\section{Results}}

\begin{enumerate}[label=(\alph*)]
	\item After loading and filtering the data childrenfinal.dta, we convert some variables (namely $\mathtt{tetanusmother}$, $\mathtt{breastfeeding}$, $\mathtt{wantedchild}$, $\mathtt{anetalvisits}$, and $\mathtt{placedelivery}$) into double labeled \textit{<dbl>} (doubles, or real numbers).
	
	\item The smooth line in figure \ref{f} indicates that the $\mathtt{zstunt}$ is deteriorating between the ages of 0 and 20, before stabilizing.
	\begin{figure}[H]
		\centering
		\scalebox{0.75}{\input{Ex1plot1.tex}}
		\caption{Scatter plot of $\mathtt{zstunt}$ against $\mathtt{hypage}$ with a smooth line (in purple)}
		\label{f}
	\end{figure}
	
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex1plot2.tex}}
			\caption{For each gender}
			\label{fig 1.2}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex1plot3.tex}}
			\caption{For each area}
			\label{fig 1.3}
		\end{subfigure}
		\caption{Smooth plots
of $\mathtt{zstunt}$ against $\mathtt{hypage}$ }
		\label{fi}
	\end{figure}
	
	\newpage
	The smmooth line in the graphics of figure \ref{fi} indicates that $\mathtt{zstunt}$ is more detoriated in rural places, and for female genders.
		
	\item	
	Figure \ref{fig 1.4} shows the map of Kenya, with colors indicating the amount of $\mathtt{zstunt}$ in counties. The color in Isolo' county is grey because we had no data available for that county. The county where children are stunted is West Pokot since in the county $\mathtt{zstunt} < -2$.
	 
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{Ex1plot4.png}
		\caption{Plot the map of Kenya with the mean of $\mathtt{zstunt}$ in the each county}
		\label{fig 1.4}
	\end{figure}
		
\end{enumerate}	
%222222222222222222222222222222222222222222222%

{\color{royalblue}\chapter{Random number generation}}

{\color{royalblue}\section{Problem description}}
Generating a random variable from any distribution is very essential for stochastics studies. However, true random numbers are not always available, but we can use some algorithms that generate some pseudo-random numbers.\\

{\color{royalblue}\section{Methods}}
Pseudo-random numbers are a sequence of numbers that appear "random" or approximate properties of random numbers with the following properties:\\
\begin{itemize}
	\item Good approximation of the properties of random numbers.
	\item Number can be easily and efficiently generated.
	\item Reproducibility (truly random numbers never satisfy this).
\end{itemize}

With the help of some probability properties, it is typically enough to be able to use a uniform distributed random variable, in order to generate any pseudo-random numbers from a given distribution.\\
The simplest idea for generating uniformly distributed pseudo-random numbers is using a \textit{linear congruent generators} (LCG):\\
\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Choose positive integer parameters a, c and m.
				\item Choose an initial value $x_0 \in \{0, 1, \ldots, m - 1\}$ (this value is called the seed).
				\item For each $n \in \mathbb{N} $, compute\\
				\begin{equation}
				x_{n+1} := ax_{n} + c \hspace{0.2cm}
				\text{mod} \hspace{0.2cm} m, \hspace{1cm} \forall n \in \mathbb{N} 
				\end{equation}
				\item The psuedorandom numbers are the sequence $x_1; x_2; x_3; \ldots $.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We make some observations regarding the LCG algorithm.\\
\begin{itemize}
	\item The LCG can generates at most m distinct numbers which are contained in $\{0, 1, \ldots, m - 1\}$
	\item As soon as some number in the sequence, say ${x_n}_0$, is repeated (i.e.,$\exists p$ such that $x_{n_{0} + p} = {x_n}_0$),

	then the same is true for the entire sequence:
	\begin{equation}
	x_{n_{0} +p+j} = x_{n_{0} + j}, \hspace{0.5cm} \forall j \geqslant 1
	\end{equation}
	The number $p$ is called the period of the sequence. This musst be less than or equal to $m$.
	\item Suppose that (under the right conditions) the LCG approximates a uniform distribution with

	parameters $0$ and $m$. In this case, $x_n / m$ would approximate a uniform distribution with parameters
0 and 1.
\end{itemize}
To generate uniformly random numbers, the period has to be maximal
(i.e., $p = m$), so that we sample every value in the sequence before repeating any. One can show that LCG has a full period $m = 2^{b}$, $b \geqslant 2$ if and
only if $c \in (0; m)$ is odd and $a$ mod $4 = 1$.\\

After generating uniform pseudo-random numbers, we can easily obtain random variables from other distributions. The \textbf{inversion method} is one way of doing so, with the help of the following theorem:
\begin{theorem}
	Let F be a distribution function on $\mathbb{R}$. The quantile function $F^{-1}$ is defined by
	 \[ F^{-1}(u) = \text{inf} \{x: F(x)\ge u, 0 < u < 1 \} \]
	If $U \sim U_{[0; 1]}$, then $F^{-1}(u)$ has a distribution function
$F$.
\end{theorem}
Hence, for continuous distributions (exponential, Pareto, standard Cauchy, etc) where $F^{-1}$, we simply simulate $U_{i}$ (with LCG) and set $X_{i} = F^{-1}(U_i)$. If $F^{-1}$ cannot be inverted analytically, appropriate
numerical methods can be applied.\\

Let $X$ be a discrete random variable with ordered possible values $\{x_{1}, x{2},\ldots \}$, so that $F(x) = \displaystyle\sum_{i:x_{i} \leqslant x} P(X = x_{i})$ and
\[F^{-1}(r) =  \text{min} \{x_k \in \{x_{1}, x_{2},\ldots \}: \displaystyle\sum_{j=1}^{k} P(X = x_{j}) = \displaystyle\sum_{j=1}^{k}p_{j} \geqslant r\}\]
Then the inverse method becomes: set $X = x_{1}$ if and only if $U_{i} \in [0, p_{1})$ and $X = x_{k}$ if and only if
$U_{i} \in \Big[ \displaystyle\sum_{j=1}^{k-1}p_{j}, \displaystyle\sum_{j=1}^{k}p_{j}\Big)$, $k = 2, 3, \ldots$. Note that

\[P(X = x_{k}) = P\Big(\displaystyle\sum_{j=1}^{k-1}p_{j} \leqslant U_{i} < \displaystyle\sum_{j=1}^{k}p_{j} \Big) = \displaystyle\sum_{j=1}^{k}p_{j} - \displaystyle\sum_{j=1}^{k-1}p_{j} = p_{k} \]

For example, to simulate a Bernoulli random variable $Ber(p)$, generate $U \in U_{[0, 1]}$ and

set $X = 0$, if $U \leqslant 1 - p$ and $X = 1$ if $U > 1 - p$.\\

Another general approach to pseudo-random variables generation is the \textbf{acceptance-rejection method}.

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item [] \textbf{Data}: Two probability densitiy functions: $f$ for $X$ and $g$ for $Y$
				\item Find a constant $M > 0$ such that $sup_{x} \frac{f(x)}{g(x)} \leqslant c$ ;
				\item Obtain a sample $y$ from $Y$ ;
				\item Obtain a sample $u$ from the uniform distribution on $[0, 1]$;
				\item \textbf{if} $u < \dfrac{f(y)}{cg(y)}$ \textbf{then}
				\item $\bigg|$ Accept $y$ as a sample drawn from $f$;
				\item \textbf{else}
				\item $\bigg|$ Reject the value of $y$ and return to the sampling step (line 2);
				\item [] \textbf{Result:} $y$, a sample drawn from $f$ (using $g$)
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

\textbf{NB:} Computing $c$ could be difficult, but one can show that $c = sup_{x} {\frac{f(x)}{g(x)}}$\\

\newpage
{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item 
	With the Wichmann-Hill pseudo-random number generator in R, we Simulate $N =
1000 $ binomial random variables $B(n = 10, p = 0.4)$ using three approaches: inversion

	method, by simulating corresponding Bernoulli random variables by inversion method
and using R built-in function rbinom. From the figure \ref{fig 2.1}, the histograms of the three samples present the same shape but are different. This proves that the "random" numbers generated by our methods are just approximations of true random numbers.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot1.tex}}
		\caption{Histogram of the empirical CDF of all
three samples}
		\label{fig 2.1}
	\end{figure}
	
	\item To use accept-reject method (and a generator for uniform random variables only), of $N = 10000$ standard normal distributed random variables
with density 
	$f(x) = (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}} $, the density of the
standard Cauchy distribution is used: 
	$ g(x)=\{{\pi(1+x^2)}^{-1}\} $.
	
	\item[°] The constant value $c$ for this method is given by $sup_{x} {\frac{f(x)}{g(x)}} = 1.520347 $
	\item[°] Then after computing the $N$ standard normal random variables, we notice that the estimated and theoretical acceptance probabilities are almost equal. This is well depicted with in the figure \ref{fig 2.2}, where the histogram of the obtain sample is symmetric and has the same shape as the standard normal density curve.
	
	\begin{figure}[H]
		\centering
		\scalebox{0.75}{\input{Ex2plot2.tex}}
		\caption{Histogram of the obtained sample and the standard
		normal density (in blue)}
	\label{fig 2.2}
	\end{figure}
	
	\item[°] The QQ-plot in figure \ref{fig 2.3} shows points following the identity line. Hence the accept-reject method used to simulate a standard normal distributed sample (using the the standard Cauchy density) is well accurate.
	
	\newpage
	
	\begin{figure}[H]
		\centering
		\scalebox{0.75}{\input{Ex2plot3.tex}}
		\caption{QQ-plot}
		\label{fig 2.3}
	\end{figure}
	
	\item[°] However, it is not possible to simulate ample distributed from the standard Cauchy density using
the accept-reject method with a standard normal candidate density, simply because cannot find a $c$ such that $g(x) \leqslant cf(x)$ is verified ( because $ sup_{x} {\frac{g(x)}{f(x)}} = \infty $).
	
\end{enumerate}

%333333333333333333333333333333333333333333333%

{\color{royalblue}\chapter{Bootstrap}}
{\color{royalblue}\section{Problem description}}
Suppose that a sample $ \mathbf{X} = \{X_1,\ldots ,X_n \} $ is used to estimate a parameter $\theta$ of the distribution $P$ (which is unknown) and let $\hat{\theta} = S(\mathbf{X})$ be a statistic that estimates $\theta$. For the purpose of statistical inference on $\theta$,  we are interested in the sampling distribution of $\hat{\theta}$ (or certain aspects of it) so as to assess the accuracy of our estimator or to set confidence intervals for our estimate of $\theta$. If the true distribution $P$ were known, we could draw samples $\mathbf{X}_{l}, l = 1,\ldots, R \in \mathbb{N}$ from P and use Monte Carlo methods to estimate the sampling distribution of our estimate $\hat{\theta}$. The problem is that $P$ is unknown and we cannot sample from it.\\
The following section explains how to use bootstrap to make the interference on $\hat{\theta}$.

{\color{royalblue}\section{Methods}}
The bootstrap is a computerintensive resampling method, which  principle can be summarized by the following schematic diagram:
\begin{center}
	\begin{tabular}{|P{3.5cm} P{3.5cm} P{3.5cm} P{3.5cm}|}
		\hline
		\multicolumn{2}{|c}{\textbf{Real World}} & \multicolumn{2}{c|}{\textbf{Bootstrap World}} \\
		Unknown probability distribution & Observed random sample  & Empirical distribution & Bootstrap sample \\
		\multicolumn{4}{|c|}{$P \longrightarrow \mathbf{X} = \{X_1,\ldots ,X_n \}  \hspace{1cm} \Longrightarrow \hspace{1cm} \hat{P} \longrightarrow \mathbf{X}^{*} = \{{X_1}^{*},\ldots ,{X_n}^{*} \} $} \\
		\multicolumn{2}{|c}{$\Big\downarrow$} & \multicolumn{2}{c|}{$\Big\downarrow$} \\ 
		\multicolumn{2}{|c}{$\hat{\theta} = S(\mathbf{X})$} & \multicolumn{2}{c|}{$\hat{\theta}^{*} = S(\mathbf{X}^{*})$} \\  
		\multicolumn{2}{|c}{Statistic of interest} & \multicolumn{2}{c|}{Bootstrap replication} \\
		\hline  
	\end{tabular}
\end{center}

Then The idea is to sample from an empirical
distribution function. Recall that for random variables $Y = \{Y_1,\ldots ,Y_n \}$, the empirical distribution function is defined via $F_n(y) = n^{-1} \displaystyle\sum_{i=1}^{n}\mathbb{I}(Y_i \leqslant y)$ (we will use the notation $F_B$ for the bootstrap empirical distribution). If the sample of size $n$ is from a continuous distribution, then each
 observation has a probability $1/n$ and sampling from $F_n$ would be equivalent to draw with replacement from the sample. Hence the following algorithm:

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Draw $n$ times with replacement from $\mathbf{X}$ to get a bootstrap sample $\mathbf{X}^{*}_{1}$ of size $n$. Repeat $R$ times to get $R$ bootstrap samples $\mathbf{X}^{*}_{1},\ldots,\mathbf{X}^{*}_{R}$, each of size $n$.
				\item Compute bootstrap statistics $S(\mathbf{X}^{*}_{1}),\ldots, S(\mathbf{X}^{*}_{R})$.
				\item Make inference about $\theta$ based on $S(\mathbf{X}^{*}_{1}),\ldots, S(\mathbf{X}^{*}_{R})$.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We can also evaluate the goodness of the estimators (point or interval) based on the bootstrap sample. We construct confidence intervals for $\theta$ from the bootstrap replications (see step 2 in the above algorithm). \\

First recall the definition of a confidence interval. Let $\mathbf{X} = (X_1, \ldots, X_n)$ be a sample from a population with distribution $P \in \mathcal{P} = \{ P_\theta :\theta \in \Theta \subset \mathbb{R}^d \}$. Let $C(\mathbf{X})$ depend only on the sample $\mathbf{X}$ and $\theta \in \Theta$ be an unknown parameter
of interest. If
\[ \inf_{P \in \mathcal{P}} P(\theta \in C(\Theta)) \geqslant 1-\alpha \]
for a fixed $\alpha \in (0,1)$, then $C(\Theta)$ is a \textbf{confidence set} for $\theta$ with \textbf{level of
significance} $1-\alpha$. If the parameter $\theta$ is real-valued, then 
$C(\Theta) = [\underline{\theta}(\mathbf{X}), \bar{\theta}(\mathbf{X})]$, for a pair of real-valued
statistics $\underline{\theta}$ and $\bar{\theta}$ is called a confidence interval for $\theta$.\\
Therefore, a natural way to construct the bootstrap confidence interval is to use empirical quantiles of the bootstrap distribution of $S(\mathbf{X})$: compute $\hat{\theta}^{*}_i = S(\mathbf{X}^{*}_{i})$, $i = 1,\ldots,R$ bootstrap statistics and set the confidence interval for $\theta$ by $[\theta^{*}_{L}, \theta^{*}_{U}]$, where $\theta^{*}_{L}$ and $\theta^{*}_{U}$ are respectively $\lfloor R(\frac{1-\alpha}{2}) \rfloor$-th and $\lfloor R(1-\frac{1-\alpha}{2}) \rfloor$-th value in the ordered list of $\hat{\theta}^{*}_i$. Such confidence intervals are
called \textbf{bootstrap percentile}  confidence intervals. By defining $F_{B}(x) = P (\hat{\theta}^{*} \leqslant x)$, note that we have $P (\hat{\theta}^{*} \leqslant \hat{\theta}^{*}_L ) \approx \frac{1}{2} \alpha$ and $P (\hat{\theta}^{*} \geqslant \hat{\theta}^{*}_U ) \approx \frac{1}{2} \alpha$, which makes a coverage probability of $1-\alpha$.\\

The confidence interval should have equal probability to both sides of $\hat{\theta}^{*}$, that is $P(\hat{\theta}^{*} \leqslant \theta \leqslant \hat{\theta}^{*}_U) = P(\hat{\theta}^{*}_L \leqslant \theta \leqslant \hat{\theta}^{*}) $. If $\hat{\theta}^{*}$ is not the median of the bootstrap distribution, this condition is not fulfilled. An appropriate correction is given by $ \hat{\theta}^*_{LC} = F^{-1}_B(\Phi[z_\frac{\alpha}{2} + 2\hat{z}_0]) $ and $ \hat{\theta}^*_{UC} = F^{-1}_B(\Phi[z_{1-\frac{\alpha}{2}} + \hat{z}_0]) $ (respectively  the bias-corrected lower and upper confidence bound for $\theta$), where $\Phi(.)$ is the cdf of the standard normal distribution and $\hat{z}_0 = \Phi^{-1}{\{ F_B(\hat{\theta})\}} $. This interval is the \textbf{bias corrected percentile interval}. In practice, $\hat{\theta}^*_{LC}=\lfloor R\alpha_1 \rfloor $ and $\hat{\theta}^*_{UC}=\lfloor R\alpha_2 \rfloor $,
with $\alpha_1 = \Phi(z_\frac{\alpha}{2} + 2\hat{z}_0)$ and $\alpha_2 = \Phi(z_{1-\frac{\alpha}{2}} + 2\hat{z}_0)$. \\

An extension of the bias corrected percentile confidence interval, the $BC_a$ (\textbf{bias-corrected accelerated} bootstrap) confidence interval described as follow: 
\newpage

\[\alpha_1=\Phi\Big(\hat{z}_0 + \frac{\hat{z}_0 + z_{\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{\alpha/2})}\Big) \]
\[\alpha_2=\Phi\Big(\hat{z}_0 + \frac{\hat{z}_0 + z_{1-\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{1-\alpha/2})}\Big) ,\]
where
\[\hat{a}=\frac{\sum_{i=1}^{n}(\bar\theta_J - \hat{\theta}_i)^3}{6\Big\{\sum_{i=1}^{n}(\bar\theta_J - \hat{\theta}_i)^2 \Big\}^{3/2}} \]
With $\bar{\theta}_J=n^{-1}\sum_{i=1}^{n}\hat{\theta}_{(i)}$, for $\hat{\theta}_{(i)}$ as the estimator of $\theta$ obtained without observation $i$, i.e., $\hat{\theta}_{(i)} = S(X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n)$. It is easily performed in \textbf{R} with \textbf{bootstrap::bcanon}.

\newpage
{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item Let's consider a Weibull distribution with scale parameter $\lambda$, shape parameter $k$, variance $\sigma^2$ and median $x_{med}$. From a sample $(x_1,\ldots,x_n)$ simulated from the Weibull distribution with $\lambda=13$ and $k=1$, we aim to to build confidence intervals for $\sigma$ based on a statistics $\hat{s}^2=(n-1)^{-1}\sum_{i=1}^{n}(x_i-\bar{X})^2$, and $x_{med}$ based on the sample median.
	\begin{itemize}
		\item First, the sample size is set as $n = 100$, the number of bootstrap replications $R = 1000$
and the number of Monte Carlo samples $M = 1000$. We build two-sided bootstrap percentile confidence intervals for $\sigma$ and $x_{med}$ at the
significance level $\alpha=0.05$, and  Use $M$ Monte Carlo samples to estimate the coverage
probability(CP) and the average interval length(AIL) for both confidence intervals(CI). We get the following results:\\
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
				\hline
				& $x_{med}$ CI & $\sigma$ CI \\
				\hline
				CP & 0.944  & 0.856 \\
				\hline  
				AIL & 5.103662 & 6.006730 \\
				\hline  
			\end{tabular}
		\caption{\label{tableau31}Confidence intervals coverage probability and average interval length: $n = 100$, $R = 1000$}
		\end{table}
	The coverage probability for $x_{med}$ confidence interval is pretty close to $1-\alpha = 0.95$, the same for $\sigma$ confidence interval, but less than the CP $x_{med}$ CI. This might suggest that the bootstrap percentile confidence interval approximates $x_{med}$ CI more than $\sigma$ CI.
	
	\item Now, use the following settings: $n = R = 1000$ to get the results in table \ref{tableau32} and $n = 100$, $R = 5000$ and obtain the corresponding results in table \ref{tableau33}
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.947 & 0.935 \\
			\hline  
			AIL & 1.620852 & 2.211146 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau32}Confidence intervals coverage probability and average interval length: $n = R = 1000$}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.946 & 0.848 \\
			\hline  
			AIL & 5.101970 & 5.981314 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau33}Confidence intervals coverage probability and average interval length: $n = 100$, $R = 5000$}
	\end{table}
	We can notice that the CP value for both confidence intervals are again close to $0.95$, but huge differences with the AIL. Actually, we want the length of the confidence intervals to be narrow as possible, and the AIL in table \ref{tableau32} are the smallest AIL, and the CP are the largest. Hence, increasing the sample size and the number of bootstraps replications improves the accuracy of the bootstrap.
	
	\item With  $n = 100$, $R = 1000$ and $M=1000$, we build bootstrap accelerated
bias-corrected ($bc_a$) confidence intervals both for $\sigma$ and $x_{med}$, and Use $M$ Monte Carlo samples to assess the coverage probability and the average length of the confidence intervals to obtain the following table.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.956 & 0.912 \\
			\hline  
			AIL & 5.030710 & 6.661682 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau34}Confidence intervals coverage probability and average interval length: $bc_a$}
	\end{table}
	The CP values for the $bc_a$ confidence intervals are more closer to $0.95$ (especially for $\sigma CI$)than the ones of the bootstrap percentile confidence intervals(see table \ref{tableau31}). We also notice a slight difference in the AIL for both confidence intervals in both methods.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{0.5cm}|P{2.5cm}|P{2cm}|}
			\hline
			& $x_{med}$ & $\sigma$ \\
			\hline
			$\hat{z}_0$ & $-0.04063$ & 0.1113 \\
			\hline  
			$\hat{a}$ & $-1.475\times10^{-15}$ & 0.09085 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau35}Average $\hat{z}_0$ and $\hat{a}$}
	\end{table}
	The results in table \ref{tableau35} show that $\hat{z}_0$ is negative for $x_{med}$ confidence intervals and positive for $\sigma$ confidence intervals, which are respectively signs of overestimation and underestimation. The estimated accelerated term $\hat{a}$ is slightly to zero for both confidence intervals, indicating a slight asymmetry in the data. Since all these values are close to zero, we can that the confidence bands have a good performance.
	
	\end{itemize}
\newpage
    \item From the dataset \textit{shhs1.txt} has been obtained from \href{https://sleepdata.org/datasets/shhs}{Sleep Heart Health Study}, we are using the variable \textbf{rdi4p}: respiratory disturbance index. Figure \ref{fig 3.1}, we notice that the \textbf{rdi4p} is skewed on the right(positiv skewness).
    
    \begin{figure}[ht]
    	\centering
    	\scalebox{1.2}{\input{Ex3plot.tex}}
    	\caption{Histogram of \textbf{rdi4p} with the empirical distribution}
    	\label{fig 3.1}
    \end{figure}
    
    By building bootstrap percentile and bootstrap accelerated bias-corrected confidence intervals for the standard deviation and
median, we get the following results (with $R = 1000$)
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|}
    		\hline
    		& $x_{med}$ & $\sigma$ \\
    		\hline
    		CI & [3.951, 4.419] & [11.785, 13.045] \\
    		\hline
    		CI$_{length}$ & 0.498 & 1.26 \\
    		\hline  
    	\end{tabular}
    	\caption{\label{tableau36}Results for bootstrap percentile confidence interval}
    \end{table}
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|}
    		\hline
    		& $x_{med}$ & $\sigma$ \\
    		\hline
    		CI & [3.944, 4.429] & [11.792, 13.103] \\
    		\hline
    		CI$_{length}$ & 0.485 & 1.311 \\
    		\hline
    		$\hat{z}_0$ & $0.00251$ & $-0.00251$ \\
    		\hline  
    		$\hat{a}$ & 0 & 0.0272 \\
    		\hline  
    	\end{tabular}
    	\caption{\label{tableau37}Results for $bc_a$}
    \end{table}
    The results from tables \ref{tableau36} and \ref{tableau37} show that the $x_{med}$ CI are almost the same, while  we notice a slight difference $\sigma$ CI. Actually notice shift to the right in the $\sigma$ CI with $bc_a$, but with slight shift to the right in the CI for $bc_a$. This can be explained by the fact that the empirical distribution is right-skewed, and that $\hat{a}$ is null for $x_{med}$ CI, and close to zero for $\sigma$ CI. We can notice that $\hat{z}_0$ is close to zero, indicating good minimization of overestimation (for $\sigma$ CI) and underestimation (for $x_{med}$ CI). 
    
\end{enumerate}

%44444444444444444444444444444444444444444444%

{\color{royalblue}\chapter{Generalised linear models}}
{\color{royalblue}\section{Problem description}}
In its simplest form, a linear model specifies the (linear) relationship between a dependent variable $Y$ (normally distributed), and a set of independent variables $X_i$, $i=1,\ldots,k\in \mathbb{N}$, so that $Y= b_0 + b_1X_1 +\ldots+ b_kX_k$, where $b_0$ is the regression coefficient for the intercept and the $b_i$ values are the regression coefficients (for variables 1 through k). However, there are many relationships that cannot adequately be summarized by a simple linear equation, for two major reasons:
\begin{itemize}
	\item  \textit{Distribution of the dependent variable.} The dependent variable of interest may have a non-continuous distribution, and thus, the predicted values should also follow the respective distribution; any other predicted values are not logically possible.
	\item \textit{Link function.} The second reason why the linear model might be inadequate to describe a particular relationship is that the effect of the predictors on the dependent variable may not be linear in nature.
\end{itemize}
Generalized linear models (GLMs) extend linear models to accommodate both non-normal response distributions and transformations to linearity.

{\color{royalblue}\section{Methods}}

Let $(Y_1, X_1),\ldots,(Y_n, X_n)$ be independent pairs of observations, where $Y_i$ is real-valued
and $X_i$ are $\mathbb{R}^k$-valued random variables. Generalised linear models (GLMs) have the following
three-part specification:

\begin{itemize}
	\item \textbf{The random component} (=response from an overdispersed exponential family). The data $Y_1,\ldots,Y_n$ are such that $Y_1|X_1,\ldots, Y_n|X_n$ are independent and $Y_i|X_i$ has
the p.d.f.
	\[ f_{\eta,\psi}(y_i|x_i) = \mathtt{exp}\bigg\{   \frac{\eta_iy_i - \kappa(\eta_i)}{\psi_i} \bigg\}h(y_i,\psi_i), \hspace{0.5cm} i=1,\ldots,n, \]
	where $\eta_i$ is called canonical parameter and i is an unknown scale or dispersion parameter. Functions $\kappa$ and $h$ are known and $\kappa^{''}(\eta) > 0$ is assumed. Note that 
	\[ \mu(\eta_i):=\mathtt{E}(Y_i|X_i) = \kappa_0(\eta_i) \hspace{0.25cm}\mathtt{and}\hspace{0.25cm} var(Y_i|X_i)=\psi_i\kappa^{''}(\eta), \hspace{0.5cm} i=1,\ldots,n \]
	\item \textbf{The systematic component} (=linear predictor) Canonical parameter $\eta_i$ is assumed to be related to $X_i$. The term $X^t_i\beta$ for unknown
$\beta \in \mathbb{R}^d$ is called the \textbf{linear predictor or systematic component}.
	\item The \textbf{link function} between random and systematic components. The relationship between $\eta_i$ and $X^t_i\beta$ is described through
	\[ g\{\mu(\eta_i)\} = X^t_i\beta, \hspace{0.5cm} i=1,\ldots,n \]
	where g is called a link function. The link function g is assumed to be a known, one-to-one, third-order continuously differentiable function. If $g=\mu^{-1}$ then $\eta_i=X^t_i\beta$, and $g$ is called the \textbf{canonical or natural link function}. If $g$ is not canonical, then

	it is assumed that $d(g\circ\mu)(\eta)/d\eta \ne 0$ for all $\eta$.	
\end{itemize}

In a GLM, the parameter of interest is $\beta$. Parameters $\psi_i$ are considered to be nuisance parameters. It is often assumed that $\psi_i = \psi/t_i$, $i = 1,\ldots,n$ with an unknown
$\psi$ and known $t_i$'s or, alternatively $\psi_i =a( )$ for some known function $a$. Note that $\psi_i$
enter $var(Y_i|X_i) = \psi_i\kappa{''}(\eta_i)$, making it more flexible, that is allowing for over- or underdispersion.\\

\textbf{Exemple:} Let Let $Y_i|X_i \backsim Poi(\lambda_i)$. We can write the density
\[ f_\eta(y_i)= \mathtt{exp}\{ y_ilog(\lambda_i) - \lambda_i \}\frac{1}{y_i!}\mathbb{I}_{\{ 1,2,\ldots \}}(y_i) \]
that is, the canonical parameter $\eta_i = \mathtt{log}(\lambda_i), \kappa(\eta_i) = \lambda_i = \mathtt{exp}(\eta_i)$, $\psi_i = 1$ and $h(y_i) = (y_i)^{-1}\mathbb{I}_{\{ 1,2,\ldots \}}(y_i)g(y_i)$. Since $E(Y_i|X_i) = \kappa_0(\eta_i) = \mathtt{exp}(\eta_i) =: \mu(\eta_i)$, the canonical link is

$g(x) = \mu^{-1}(x) = \mathtt{log}(x)$, which is called the \textbf{log-link} $(g(\mu(\eta_i)) = \eta_i)$. Hence, 
\[ \mathtt{log}\{\mathtt{E}(Y_i|X_i=x_i)\} = x^t_i\beta,\]
where $x_i \in \mathbb{R}^k,\hspace{0.25cm} i=1,\ldots,n$.\\

\begin{center}
	\textbf{\large Estimation}
\end{center}
Let $\theta = (\beta, \psi)$ and $(g\circ\mu)^{-1} = \zeta$ (for a canonical link $\zeta(x) \equiv x$). Then

\[ \ell(\theta)= \sum_{i=1}^{n} \bigg[ \frac{\zeta(X^t_i\beta)Y_i-\kappa\{\zeta(X^t_i\beta)\}} {a(\psi)} + \mathtt{log}h(Y_i,\psi)  \bigg] .\]
Further, consider the canonical link. Taking derivatives w.r.t. $\beta$ and we get the following
score equations
\[  \frac{\partial \ell(\theta)}{\partial\beta}= \frac{1}{a(\psi)} \sum_{i=1}^{n} \{ Y_i - \mu(X^t_i) \}X_i=0 \]

\[ \frac{\partial \ell(\theta)}{\partial\psi}= \sum_{i=1}^{n}\bigg[ \frac{\partial\mathtt{log}h(y_i,\psi)}{\partial\psi}+ \{a^{-1}(\psi)\}^{'}\{ X^t_i\beta Y_i+ \kappa(X^t_i\beta) \} \bigg]= 0 \]

Where $ \kappa(X^t_i\beta)= \mu(X^t_i\beta)$ was used. If MLE of $\beta$ exists, then it can be found from the
first equation without estimating. Estimation of $\psi$ from the second equation in many cases is a difficult task and depends on a particular distribution. To estimate $\beta$ and study its properties we also need
\[-\frac{\partial^2\ell(\theta)}{\partial\beta\partial\beta^t}= \frac{1}{a(\psi)} \sum_{i=1}^{n}\bigg[ \kappa(X^t_i\beta)^{''}X_i X^t_i \bigg]=: -\frac{F_n(\beta)}{a(\psi)} \]
With this, we can set up the Newton-Raphson algorithm as
\[ \hat\beta^{(j+1)}= \hat\beta^{(j)} +\big\{ F_n(\hat\beta^{(j)}) \big\}^{-1} S_n(\hat\beta^{(j)}), \hspace{0.25cm} j=0,1,2,\ldots, \]
where $S_n(\hat\beta^{(j)})= a(\psi)\partial\ell(\theta)/\partial\beta.$\\

\begin{center}
	\textbf{\large Goodness-of-fit and models' comparison}
\end{center}
Now, we want to to assess how good the model fits the data,
i.e., to measure the discrepancy between the data $Y_i|X_i$ and estimated $\mathtt{E}(Y_i|X_i) = \mu_i$. First, some definitions. The \textbf{null model} is simplest model, and has only one parameter, representing a common mean $\mu$, say, for all $Y_i|X_i$. At the other extreme is the \textbf{full model}, which has $n$ parameters, one for each observation. The full model gives a baseline for measuring the discrepancy for an intermediate model with k parameters. Assume for the moment that $\psi$  is known
and denote $\ell(\hat{\mu},\psi)$ the log-likelihood with $\hat{\mu} = g^{?1}(X\hat{\beta})$. The maximum likelihood in the
full model is then $\ell(Y, \psi)$ ($=\mu_i$ are replaced by $Y_i$). Then the \textbf{deviance of the fitted	model} is defined as
\[ D(Y,\hat{\mu})= a(\psi)2\{ \ell(Y,\psi)- \ell(\hat{\mu}, \psi) \}  \]
Note that $D(Y,\hat{\mu})/a(\psi)$ is called the \textbf{scaled deviance}(or the deviance for $2\{ \ell(Y,\psi)- \ell(\hat{\mu}, \psi) \}$).
The \textbf{generalised Pearson statistic} is defined via
\[ \chi^2=  \frac{\sum_{i=1}^{n}(Y_i-\hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]. The following methods are used to measure the goodness-of-fit, and compare models:

\begin{itemize}
	\item  \textbf{Analysis of deviance:} Scaled deviance can be used to compare two nested models, i.e. the parameter space
under one model is a subspace of that under the second model. let $M_k$ and $M_q$, with $q<k$ (k and q are the number of parameters in $M_k$ and $M_q$ respectively) two nested models. Let us denote $D_{M_k}$ and $D_{M_q}$ respectively as the scaled deviance of $M_k$ and $M_q$. Since we have assume that $\psi$ is known, we have the following formula:
	\[\frac{D_{M_q}- D_{M_k}}{\psi} \overset{approx}{\sim} \chi^2_{k-q}\] 
	A widely used
rule of thumb(to measure goodness-of-fit) is that a good fit has the scaled deviance about $n ? k$, which is the expectation of a $\chi^2_{n-k}$ distributed random variable. Large values of the scaled deviance are
considered to indicate a bad fit. However, this has to be treated with care. For Poisson
data with large $\lambda_i$ and Binomial data with large $m_i$, the approximation to $\chi^2_{n-k}$ works
reasonable, but not in many other cases. Therefore we can use other methods.
	
	\item \textbf{Residual analysis:} Here, the residuals used are expected to behave approximately as zero-mean normally distributed variables. \textbf{Pearson residuals} defined via
	\[ r^p_i= \frac{Y_i-\hat{\mu}_i}{d\sqrt{V(\hat{\mu}_i)}}, \hspace{0.25cm} i=1,\dots,n \]. Pearson residuals have the disadvantage of being skewed for non-normal responses. As a remedy, we have the \textbf{ Anscombe residulas}, which in the special case of the Poisson distribution is given by \[ r^a_i= \frac{3(Y^{2/3}_i-\hat{\mu}^{2/3}_i)}{2\hat{\mu}^{1/6}_i}, \hspace{0.25cm} i=1,\dots,n \].
	\item \textbf{Deviance residuals:} based on the deviance, they are defined by \[ r^d_i= \mathtt{sign}(Y_i-\hat{\mu}_i)\sqrt{2\{  \ell_i(Y_i,\psi)- \ell_i(\hat{\mu}_i,\psi)\}}, \hspace{0.25cm} i=1,\dots,n \]
	where $\ell_i$ is the log-likelihood corresponding to the $i$-th observation, so that $\sum_{i=1}^{n}(r^d_i)^2= D(Y, \hat{\mu})$). A standardised version of the deviance (as well as Pearson)
residuals are used: \[ \frac{r^d_i}{\sqrt{a(\hat{\psi})(1-h_i)}}, \hspace{0.25cm}, i=1,\dots,n\] 
	where $h_i = H_{i,i}$ with the hat matrix $H$ taking now the form $H = W^{1/2}X(X^t W X)^{-1}X^t W^{1/2}$, where W is the weight matrix from the Fisher scoring. In an adequate model the plot
of standardised residuals against $\hat{\eta}= X\hat{\beta}$ should show \textit{no patterns}. The \textbf{null pattern} is a distribution of residuals with mean zero and constant variance.
	
	\item \textbf{Akaikeinformation criterion (AIC) and Bayes information criterion (BIC):} These criterions can be used to compare models with different subset of parameters or even to compare two different
models (e.g., with different link functions or a non-linear and with a linear model). These
two criteria are most popular examples of penalised goodness-of-fit criteria
	\[AIC(M)= -2\ell(M)+ 2|M|\]
	\[BIC(M)= -2\ell(M)+ \mathtt{log}(n)|M|,\] where $\ell(M)$ denotes the log-likelihood corresponding to a model $M$ and $|M|$ is the number of parameters in that model $M$. The models, selected with these criteria are then
	\[ \hat{M}_{AIC}= \mathtt{arg}\min_{M\in \mathcal{M}}AIC(M)  \]
	\[ \hat{M}_{BIC}= \mathtt{arg}\min_{M\in \mathcal{M}}BIC(M)  \]	
\end{itemize}
\newpage

{\color{royalblue}\section{Results}}
For the exercise, the dataset \textit{student-mat.csv} can be found on \href{https://www.kaggle.com/uciml/student-alcohol-consumption}{Kaggle}. Variables G1, G2, G3 are first, second and final
grades in mathematics. The remaining variables are explanatory variables. We would like
to identify variables that explain grades in mathematics.

\begin{enumerate}[label=(\alph*)]
	\item First of all, we need to identify the distribution of G1, G2, and G3. From the Q-Q plots with normal theoretical distribution of figure \ref{fig 4.2}, we notice too many zero points and points away on the tails. Moreover, the emperical densities plots are skewed on the left, that is way different from the bell-shape of a normal distribution. Therefore G1, G2, and G3 are not normally distributed. On the other hand, the figure \ref{fig 4.3} of the Q-Q plot with Poisson as theoretical distribution, displays points along the identity line suggesting that we might have a Poisson distribution. However, there are some zero points (especially in G1 and G2), which is a sign of under-dispersion, and over-dispersion for G1. Actually since the variables are Poisson distributed, hence their means should be equal their variances. But different results (see table \ref{tableau41}) confirm the latter assumption.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1cm}|P{1cm}|P{1cm}|}
			\hline
			 & mean & var \\
			\hline
			G1 & 10.909 & 11.017 \\
			\hline
			G2 & 10.714 & 14.149 \\
			\hline
			G3& 10.415 & 20.989 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau41}Variances and means of G1, G2, and G3}
	\end{table} 
	
	\begin{figure}
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot1.tex}}
			\caption{Emperical densities}
			\label{fig 4.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot2.tex}}
			\caption{Q-Q plot with normal theoretical distribution}
			\label{fig 4.2}
		\end{subfigure}
		\caption{Checking normality assumption}
		\label{fig:roc_curve}
	\end{figure}
	
	\begin{figure}
		\centering
		\scalebox{0.75}{\input{Ex4plot3.tex}}
		\caption{Q-Q plot with Poisson as theoretical distribution}
		\label{fig 4.3}
	\end{figure}
	
\newpage
	
	\item A generalised linear model(Model 1) is fitted to explain G1 with all explanatory variables. With a $\alpha=0.05$, we notice that all covariates are not significant, which might be a sign of a bad fitted model. After calculating the Pearson residuals and Anscombe residuals, we come upn with the plots in figure \ref{fig 4.3}. We can clearly assume that these residuals are normaly distributed, because points on the Q-Q plots are distributed along the identity line. Furthermore, figure \ref{fig 4.4} we can see that the normality assumption is again fulfill and points on other plots present no pattern, therefore we can say that the fitted (generalised) linear model is adequate for the data, but since we have too many unsignificant covariates, we can conclude that this model is not adequate to the data.  
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.70}{\input{Ex4plot4.tex}}
			\caption{Pearson residuals}
			\label{fig 4.4}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.70}{\input{Ex4plot5.tex}}
			\caption{Anscombe residuals}
			\label{fig 4.5}
		\end{subfigure}
		\caption{Q-Q plots of residuals with normal theoretical distribution: Model 1}
		\label{fig}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\scalebox{0.70}{\input{Ex4plot6.tex}}
		\caption{Residuals analysis: Model 1}
		\label{fig 4.6}
	\end{figure}
	
	\newpage
	\item Now a another model, Model.2, is made by reducing variables from Model.1 to \textbf{sex}, \textbf{Fedu}, \textbf{studytime}, \textbf{failures}, \textbf{schoolsup},
\textbf{famsup}, \textbf{goout}. This time, all covariates are significant. \textbf{sex}, \textbf{Fedu}, \textbf{studytime} have a positive effect on grades, meaning that a female student whose father is well educated and who spend time studying has good grades. Whereas \textbf{failures}, \textbf{schoolsup}, \textbf{famsup}, \textbf{goout} have a negative effect on grades, meaning that a student who has already failed, with no extra educational support, no family educational support, and always going out has bad grades. Figure \ref{fig 4.7} show points along the identity of the Normal Q-Q plot, and no patterns in the residuals plots. Hence Model.2 is good model.
	
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex4plot7.tex}}
		\caption{Residuals analysis: Model 2}
		\label{fig 4.7}
	\end{figure}
	
	By comparing Model.1 and Model.2 with ANOVA, we have a p-value $= 0.1858 >\alpha$, then we reject Model.1. Model.2 delivers better fit than Model.1. A third model, Model.3 is created by replacing in Model.2 \textbf{goout} by \textbf{Walc}. Then we want to compare these two models, but since they are not nested, we cannot use ANOVA as we did previously. So we opt for residuals analysis and AIC comparison. From figures \ref{fig 4.7} and \ref{fig 4.8}, we can notice that the two models are pretty close, which makes difficult to make a choice. Nevertheless, the AIC of Model.2 is less than Model.3 AIC (respectively 1975.1 and 1977.2). Hence Model.2 delivers a better fit.
	
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex4plot8.tex}}
		\caption{Residuals analysis: Model 3}
		\label{fig 4.8}
	\end{figure}
	
\end{enumerate}





%55555555555555555555555555555555555555555555%

{\color{royalblue}\chapter{Survival analysis}}
{\color{royalblue}\section{Problem description}}
We want to analyze data where the outcome variable is the time until the occurrence of an event of interest. The event can be death, occurrence of a disease, marriage, divorce, etc. The time to event or survival time can be measured in days, weeks, years, etc. For example, if the event of interest is heart attack, then the survival time can be the time in years until a person develops a heart attack. subjects are usually followed over a specified time period and the focus is on the time at which the event of interest occurs. Why not use linear regression to model the survival time as a function of a set of predictor variables? First, survival times are typically positive numbers; ordinary linear regression may not be the best choice unless these times are first transformed in a way that
removes this restriction. Second, and more importantly, ordinary linear regression cannot effectively handle the censoring of observations. Why not compare proportion of events in your groups using risk/odds ratios or logistic regression? Simply because it ignores time. \\
To tackle these issues, we'll use some survival analysis.

{\color{royalblue}\section{Methods}}
Let $T$ be a non-negative random variable that represents the time to event. We assume its CDF as $F$ that has pdf $f$. Central concepts of the survival analysis are the \textit{survivor function} (the probability that a subject
will survive past time t) $S(t)=P(T>t)=1-F(t)$, the \textit{hazard function} $h(t)=\frac{f(t)}{1-F(t)}$ (loosely speaking, it is the probability density of failure at time
$t$, given survival to then), and the \textit{cumulative hazard function} (accumulated risk up to time t) $H(t)= \int_{0}^{t}h(s)ds= -log\{S(t)\}$. Thus we have $S(t)=\mathtt{exp}\{-H(t)\}$ and $f(t)= h(t)\mathtt{exp}\{-H(t)\}$.\\
\\
\textbf{Exemples:}Some common parametric distributions
\begin{enumerate}
	\item Exponential distribution: $h(t) =\lambda $ and $S(t) = \mathtt{exp}(-\lambda t)$
	\item Weibull distribution: $h(t) = \alpha\lambda^\alpha t^{\alpha-1}$ and $S(t) = \mathtt{exp}\{(-\lambda t)^\alpha\}$
\end{enumerate}

Ideally, we would have independent realisations of $T$: $t_1,\dots,t_n$. However, in practice
the failure time cannot always be observed due to various reasons. This phenomenon
is called \textit{censoring}. We have \textit{Type I censoring}, where $T$ is observed until some pre-determined time $c$. If $T<c$, we observe the value $t_i$ of $T$ , if $T>c$, we only know that $T$ survived beyond $c$. \textit{Type II censoring} (rarely used)
arises when $n$ independent variables are observed until there have been $r$ failures, so only
$0<T_{(1)}<\dots<T{(r)}$ are observed. These are all examples of \textit{right-censoring}. \textit{Left-censoring} (the time
of origin is not known) is less common.\\
\\
Under censoring one rather deals with $Y_i = \min\{T_i, C_i\}$ (the observed response), where $C_i$ denotes the censoring time for the $i$th subject. That is, a pair $(y_i; \delta_j)$ is observed, where Let $\delta$ denotes the event indicator.
\[
\delta_i = 
\begin{cases} 
0 & \text{if the event was observed } T_i \leq C_i \\
1 & \text{if the response was censored  } T_i > C_i
\end{cases}
\]
Note that $T$ and $C$ are independent.\\
\\
Let's assume that $T$ has a continuous distribution $F$ and there are $n$ data points available
$(y_1, \delta_1),\dots,(y_n, \delta_n)$, where $y_i = \min\{t_i, \delta_i\}$. Assume that $F(x) = F(x, \theta)$ is a some
parametric distribution and that censoring variables $C_i$ have CDF $G$
and pdf $g$, which are independent on $?$. The log-likelihood contribution from $y_i$ can be represent as
\[ \ell(\theta)= \sum_{i=1}^{n}[ \delta_i\mathtt{log}\{h(y_i;\theta)\}- H(y_i;\theta) ] \]

For exponential distribution, have 
\[ \ell(\theta)= \sum_{i=1}^{n} (\delta_ilog(\lambda)-\lambda y_i)= \log(\lambda)\sum_{i=1}^{n}\delta_i- \lambda \sum_{i=1}^{n}y_i \]
implying \[\hat{\lambda}_{ML}= \frac{\sum_{i=1}^{n}\delta_i}{\sum_{i=1}^{n}y_i}. \]
An
approximate confidence interval for $\lambda$ (using asymptotic normality of maximum likelihood
estimators) as
\[ [\hat{\lambda}(1- z_{\alpha/2}/\sqrt{r}) ,\hat{\lambda}(1+ z_{\alpha/2}/\sqrt{r} ], \hspace{0.25cm} r= \sum_{i=1}^{n}\delta_i .\]

A commonly used parametric distribution for modelling lifetimes
with monotone hazard is the Weibull distribution. Values for $\lambda$ and $\alpha$ can be estimated
by the maximum likelihood similarly to the exponential distribution, however, this has to
be done numerically.
\newpage

\textbf{Nonparametric estimators}\\
Often it is unclear which parametric model would be appropriate for the data (if any). A
standard tool for initial data inspection, for suggesting plausible models and for checking
their fit is a nonparametric estimator of the survivor function. For no censored
observations, we could estimate $\hat{S}(t)= n^{-1}\sum_{i=1}^{n}\mathbb{I}(T_i >t)$. For censored observation, let $0\leq \tau_1< \tau_n< \dots$ be the ordered uncensored failure times. Let $r_i$ denote the number
of units that are still in risk at $\tau_i$ (=not failed yet or censored) and $d_i$ the number of
units that fail at $\tau_i$. The \textbf{Kaplan-Meier estimator} for the survivor function $S$ is given by
\[ \hat{S}_{KM}(t)= \underset{ \{j:\tau_j<t\} }{\prod} \Big(1- \frac{d_j}{r_j} \Big) \]
A further estimator for $S$ is the \textbf{Fleming-Harrington estimator} $\hat{S}_{FH}(t)$. It is a plug in
estimator defined by
\[ \hat{S}(t)= \mathtt{exp}\{- \hat{H}(t) \}, \]
where $\hat{H}(t)= \underset{ \{j:\tau_j<t\} }{\sum} \frac{d_j}{r_j}$, is the \textit{Nelson-Aalen} estimator for $H$.\\
\\
\textbf{Confidence bands}\\
Assume that $\hat{S}$ is an estimator for $S$ (e.g. the Kaplan-Meier or the Fleming-Harrington estimator) and let $\hat{\mathtt{var}}(\mathtt{log}(\hat{S}))$ be some estimate for the variance of log(S). An approximate confidence band
(contained in $[0, 1]$) is given by 
\[ [\mathtt{exp}(-\mathtt{exp}(B^-), \mathtt{exp}(-\mathtt{exp}(B^+)]  ,\]
where
$B^\pm = \mathtt{log}(-\mathtt{log}\hat{S}(t))\pm z_{\alpha/2}\mathtt{log}^{-1} \hat{S}(t) \sqrt{\hat{\mathtt{var}}(\mathtt{log}(\hat{S}))}$\\
\\
\textbf{Log-rank test}\\
We wish to decide whether or not two (or more) samples stem from the same survivor
function or not. Assume that the failure times $\tau_1<\dots< \tau_k$ are realizations of two random variables $T_1$ and
$T_2$ corresponding to two groups of items (patients). For each observed failure time $\tau_j$ we
consider the contingency table
\begin{table}[H]
	\centering
	\begin{tabular}{c c c}
		\hline
		Groups & failure at time $\tau_j$ & items at risk at time $\tau_j$ \\
		\hline
		1 & $d_{1j}$ & $r_{1j}$ \\
		2 & $d_{2j}$ & $r_{2j}$ \\
		\hline
		$1 + 2$ & $d_j$ & $r_j$ \\ 
	\end{tabular}
\end{table}

Under the null-hypothesis that $T_1 = T_2$ the expected number of failures at time $\tau_j$ in
group 1 and 2 are hypergeometrically distributed with parameters $r_j, r_{1j}, d_j$ and $r_j, r_{2j}, d_{j}$
respectively. Thus, mean and variance of the number of failures in group 1 and 2 can be
computed as
\[ e_{1j}=\frac{d_j}{r_j}r_{1j} \hspace{0.25cm} \mathtt{and} \hspace{0.25cm} e_{2j}=\frac{d_j}{r_j}r_{2j} \] and
\[ v_{1j}= v_{2j}= \frac{d_j r_{1j} r_{2j}(r_j-d_j)}{r^2_j(r_j- 1)} \]
Under the null-hypothesis, the statistic
\[ \chi^2 = \frac{  \Big[ \sum_{j=1}^{k} (d_{1j}-e_{1j})\Big]^2 }{\sum_{j=1}^{k}v_{1j}} \]
is $\chi^2$-distributed with 1 degree of freedom.\\

\textbf{Graphical tool to check if the Weibull model is adequate}\\
Under the assumption that $T$ is Weibull distributed, one has
\[ \mathtt{log}\{-\mathtt{log}(S(t))\}= \alpha\mathtt{log}(t)+ \mathtt{log}(\lambda), \hspace{0.25cm}, t>0. \]
Now let $\hat{S}(t)$ be a nonparametric estimate for $S$ (e.g. the Kaplan-Meier estimator $\hat{S}_{KM}$).Then the plot $\mathtt{log}\{-\mathtt{log}(\hat{S}(t))\}$ against $\mathtt{log}(t)$ should approximately be a straight line with
slope $\alpha$ and intercept $-\mathtt{log}(\lambda)$.

\newpage
{\color{royalblue}\section{Results}}
The dataset of interest for the exercise is \textit{Thoracic.txt}, and can be found on \href{https://archive.ics.uci.edu/ml/datasets/Thoracic+Surgery+Data}{ UCI Machine Learning Repository}. We will be using the following three variables:\\
\begin{enumerate}
	\item[] \textbf{PRE30}: if a person is a smoker 
	\item[] \textbf{AGE}: patient age at surgery
	\item[] \textbf{Risk1Y}: is TRUE if a person has died within a year after the surgery
\end{enumerate}
We will consider failure times to be at AGE +1.

\begin{enumerate}[label=(\alph*)]
	\item After computing nonparametric estimators of the survivor function $S(t)$, we generate the figure \ref{fig 5.1}, with $95\%$ confidence bands.We notice that the two estimators are almost the same, but for a person above 80 years old, Fleming-Harrington estimate a survival probability greater than the one of Kaplan-Meier.
	
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex5plot1.tex}}
		\caption{Plot of nonparametric estimators of the survivor function}
		\label{fig 5.1}
	\end{figure}
	
	\newpage
	Then we fit the parametric (exponential and Weibull)
models to the data, and plot them with the Kaplan-Meier estimator (with its corresponding
confidence band) as shown in figure \ref{fig 5.2}. Both parametric estimators are decreasing, but the exponential estimator for $S(t)$ is linear, while the Weibull fit has almost the shape as the nonparametric fit Kaplan-Meier. Hence the exponential model is a very bad model one for this data, while the Weibull estimator would be appropriate, that is realistic.
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex5plot2.tex}}
		\caption{Plot of Kaplan-Meier \& parametric estimators of the survivor function}
		\label{fig 5.2}
	\end{figure}
	
	To check if the Weibull model is adequate for the data, we use the graphical method presented in the last paragraph of the Results section, and obtain figure \ref{fig 5.3}. We can see that the points follow $D0$ line pattern, meaning that the Weibull model is effectively appropriate for the data.
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex5plot3.tex}}
		\caption{Plot of $\mathtt{log}\{-\mathtt{log}(\hat{S}(t))\}$ against $\mathtt{log}(t)$, with the line $D0$ with slope $\alpha$ and intercept $-\mathtt{log}(\lambda_{weibull})$}
		\label{fig 5.3}
	\end{figure}
	
	\item Now we consider two groups of patients: smokers and non-smokers. Smokers make up $82.1\%$ in the sample. For each group, the Kaplan-Meier estimators are computed, and plotted together with the corresponding confidence bands in figure \ref{fig 5.4}. Then we want to test if the survival time depends on being a smoker. With the log-rank test, we have have the $\chi^2_1=2.7$ with p-value$=0.1>0.05$. Hence the two groups do not share the same survival function, in other words the survival time do depend on being a smoker.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{Ex5plot4.png}
		\caption{Plot of the Kaplan-Meier estimators fit for smokers and nonsmokers}
		\label{fig 5.4}
	\end{figure} 
	
	We fit the Weibull model to both groups. The results are in table \ref{tableau51}, and the figure \ref{fig 5.5} is the plot of the resulting parametric estimators for the
survivor function together with the corresponding Kaplan-Meier estimators.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.9cm}|P{2cm}|P{2cm}|}
			\hline
			& Scale & Shape  \\
			\hline
			smokers & 0.01205778 & 8.161903  \\
			\hline
			nonsmokers & 0.01181161 & 11.13959 \\
			\hline
		\end{tabular}
		\caption{\label{tableau51}Weibull model estimated parameters for each group}
	\end{table} 
	
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex5plot5.tex}}
		\caption{Plot of the nonparametric $\&$ parametric fit of the survivor function for each group}
		\label{fig 5.5}
	\end{figure} 
	
	To check Weibull model assumption in both groups, we use figures in \ref{figuuu}. We can see that the points in plot \ref{fig 5.6} follow $D1$ line pattern, meaning that the Weibull model is effectively appropriate for the smokers' group. However, the plot \ref{fig 5.7} highlight many points on the x-axis causing a different pattern with the line $D2$. Hence we can assume that the Weibull model is not appropriate for the nonsmokers group.
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex5plot6.tex}}
			\caption{Smokers: $D1$ is the line with slope $\alpha$ and intercept $-\mathtt{log}(0.01205778)$}
			\label{fig 5.6}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex5plot7.tex}}
			\caption{Nonsmokers: $D2$ is the line with slope $\alpha$ and intercept $-\mathtt{log}(0.01181161)$}
			\label{fig 5.7}
		\end{subfigure}
		\caption{Checking Weibull model assumption in both groups}
		\label{figuuu}
	\end{figure}
	
	
\end{enumerate}





%66666666666666666666666666666666666666666666%

{\color{royalblue}\chapter{Kernel density estimation}}
{\color{royalblue}\section{Problem description}}

Consider observations which are realizations of univariate random variables,
$X_1, \ldots, X_n \sim F$ where $F$ denotes an unknown cumulative distribution function. The goal is to estimate the distribution $F$. In particular, we are interested in estimating the density $f = F^{'}$, assuming that it exists.\\
Instead of assuming a parametric model for the distribution (e.g. Normal distribution with unknown expectation and variance), we rather want to be "as general as possible": that is, we only assume that the density exists and is suitably smooth (e.g. differentiable). It is then possible to estimate the unknown density function $f(\cdot)$.

{\color{royalblue}\section{Methods}}

\textbf{Definition}\\
\\
Let $X_1,\dots,X_n \overset{iid}{\sim} F$ with a given density $F^{'} = f$. A \textbf{kernel density estimator} for $f$
is defined via
\[ \hat{f}(x;h)= \sum_{i=1}^{n}K\Big( \frac{x-X_i}{h} \Big), \hspace{0.25cm} x\in \mathbb{R}, \hspace{0.25cm} h>0.\]
Thereby $K :\mathbb{R} \rightarrow \mathbb{R}$, such that $\int_{-\infty}^{\infty} K(x)dx=1$ is known as \textbf{kernel} and $h > 0$ is called
 \textbf{bandwidth}.
\\
Some classical kernels:
\begin{enumerate}
	\item 0.5$\mathbb{I}(|x|\leq 1)$ (the rectangular or uniform kernel)
	\item $(1-|x|)\mathbb{I}(|x|\leq 1)$ (the triangular kernel)
	\item $0.75(1-x^2)\mathbb{I}(|x|\leq 1)$ (the Epanechnikov kernel)
	\item $2^{-1/2}\mathtt{exp}(-x^2 /2)$ (the Gaussian kernel)
\end{enumerate}
Now we want to find a practical way of choosing $K$ and $h$. The optimal bandwidth is given by $h_{CV}= \mathtt{arg}\min_{h>0} CV(h)$, where $CV(.)$ is the \textbf{(leave-one-out) cross-validation criterion}.  
\[ CV(h)= \int\{ \hat{f}(x;h) \}^2 dx -2\frac{1}{n(n-1)h}\sum_{i=1}^{n}\sum_{j\ne i}K(\frac{X_j -X_i}{h}).\]

Then, the cross-validation kernel density
estimator is define via
\[ \hat{f}(x;h_{CV})= \frac{1}{nh_{CV}}\sum_{i=1}^{n}K(\frac{X_i -x}{h_{CV}}) \]

\newpage
{\color{royalblue}\section{Results}}
The dataset used for the exercise is \textit{StudentsPerformace.csv} and can be found on \href{https://www.kaggle.com/spscientist/students-performance-in-exams}{Kaggle datasets}. In our analysis we will only consider the
following variables:\\
$\mathtt{test.preparation.course}$: If a student took part at the preparation course\\
$\mathtt{math.score}$: Score on the math exam (0-100)\\
$\mathtt{reading.score}$: Score on the reading exam (0-100)\\
$\mathtt{ writing.score}$: Score on the writing exam (0-100)\\

\begin{enumerate}[label=(\alph*)]
	\item After the implement of the kernel density estimation in an \textbf{R} function, we have the following plot \ref{fig 6.1}. Since we want to avoid under- or oversmoothing, the ideal bandwidth would be 8. This Bandwidth is then used to plot \ref{fig 6.2} with four different kernels. We can notice that the Epanechnikov kernel function would be ideal for the kernel density estimation because its curve is not too smooth or too rough. However, the kernels curves are pretty close, which is not the case for the bandwidths. Hence, the choice of the kernel does not matter very much as the choice of the bandwidth.
	
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot1.tex}}
			\caption{ with the Epanechnikov kernel and 4 different bandwidths}
			\label{fig 6.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot2.tex}}
			\caption{ with four kernel functions: bandwidth = 8 }
			\label{fig 6.2}
		\end{subfigure}
	    \caption{Plots of kernel density estimators of math.score}
		\label{figg}
	\end{figure}
	
	\item After the implemention of the cross-validation (CV) criterion to find the optimal bandwidth, we use it to find the optimal bandwidth in density for all three scores math.score, reading.score and writing.score. The same is done with \textbf{R} built-in functions $\mathtt{bw.ucv}$, and $\mathtt{bw.bcv}$, then we have the table \ref{tableau61}. The cross-validation criterion returns the highest optimal bandwidth for all three scores. We can notice that the obtained bandwidth with the implemented CV are greater than the ones of \textbf{R} built-in functions. 
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.9cm}|P{1cm}|P{1cm}|P{1cm}|}
			\hline
			& CV & $\mathtt{bw.ucv}$ & $\mathtt{bw.bcv}$ \\
			\hline
			math.score & 5.506 & 4.644 & 4.257 \\
			\hline
			reading.score & 4.465 & 3.756 & 4.393 \\
			\hline
			writing.score & 5.489 & 4.265 & 4.175 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau61}Optimal bandwidth for each samples with different methods}
	\end{table} 
	
	%the following plots are from the next question, but I want them on this page
	\item 
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot3.tex}}
			\caption{}
			\label{fig 6.3}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot4.tex}}
			\caption{}
			\label{fig 6.4}
		\end{subfigure}
	
	    \begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
	    	\centering
	    	\scalebox{0.75}{\input{Ex6plot5.tex}}
	    	\caption{}
	    	\label{fig 6.5}
	    \end{subfigure}
		\caption{Densities plots of all three scores of the students that did not take part in the preparation course with the students who attended the preparation course}
		\label{figgg}
	\end{figure}
	
	From figure \ref{figgg}, we notice that the densities are skewed to the left. We also notice that they have the same shape, with different modes. This means that the two groups may have same distribution with different parameters. 	
	
	
	
	
	
\end{enumerate}







%77777777777777777777777777777777777777777777%

{\color{royalblue}\chapter{Nonparametric regression: local polynomials}}
{\color{royalblue}\section{Problem description}}

To study the relation between a dependent variable $Y$ and an independent variable $X$, the common method used is linear regression. When appropriate, this method is very useful as its suppose a simple model of the form 
\begin{equation}
Y = \beta_{0} + \beta_{i}x_{i} + \epsilon_{i}
\end{equation}
This is advantageous since it is easy to interpret and to calculate. Moreover, when the assumptions on the residues $\epsilon_{i}$  are verified, we can run some tests on the parameters. \\
However, the restricted assumption of linearity is frequently not fulfilled, eventually when the data set is very large. In that case, we would like to find a complex model that will better highlight the relation between $Y$ and $X$. A first approach for this aim would be to specify another parametric form for this relation, for example a transformation of the observations or a polynomial regression. Nonetheless it remains difficult to find the suitable relation since the form of the data does not really change after these transformations. That is why in this section, we opt for a non-parametric regression technique (local polynomials) in which data choose their own form of relation (the predictor does not take a predetermined form but is constructed according to information derived from the data) making things more flexible.

{\color{royalblue}\section{Methods}}
Let $(Y_1, X_1),\dots,(Y_n, X_n)$ be iid as $(Y, X)$ random variables, $Y \in \mathbb{R}$ and $X \in \mathbb{R}^d$.
Consider a random design
nonparametric regression model
\[ Y_i= f(X_i)+ \epsilon_{i}, \hspace{0.25cm} i=1,\dots,n \]
\[ \mathtt{E}(\epsilon_{i}|X_i)= 0, \hspace{0.25cm}  \mathtt{E}(\epsilon_{i}^2|X_i)= \sigma^2 .\]
Let$K:\mathbb{R}^d \rightarrow \mathbb{R}_{+}$ be a kernel function, and denote $e_k = (0,\dots,0,1,0,\dots,0) \in \mathbb{R}^(\ell+1) $ a unit vector with $1$ at $k$-th position, $k =
1,\dots,\ell+1$. Moreover, we define
\[ P(X_i-x)= \{1,(X_i-x),\dots,(X_i-x)^\ell  \}^t \]

\[X = 
\begin{pmatrix}
1 & (X_1-x) & \cdots & (X_1-x)^\ell \\
\vdots  & \vdots  & \ddots & \vdots  \\
1 & (X_n-x) & \cdots & (X_1-x)^\ell 
\end{pmatrix}, \hspace{0.25cm} 
Y=
\begin{pmatrix}
Y_1 \\
\vdots  \\
Y_n 
\end{pmatrix} \]

\[V=\mathtt{diag}\bigg\{K\bigg(\frac{X_1-x}{h}\bigg),\dots,K\bigg(\frac{X_n-x}{h}\bigg) \bigg\} \]
Then a \textbf{local polynomial estimator} of $f^{(k-1)}(x)$ is a linear estimator
\[ \hat{f}^(k-1)(x)=(k-1)!e^t_k \bigg( \frac{1}{nh}X^tVX \bigg)^{-1}P(X_i-x)
K\bigg(\frac{X_i-x}{h}\bigg)=\sum_{i=1}^{n}W_{k,i}(x)Y_i \]
with the weight function
\[ W_{k,i}(x)= \frac{(k-1)!}{nh}e^t_k\bigg( \frac{1}{nh}X^tVX \bigg)^{-1} 
P(X_i -x)K\bigg( \frac{X_i -x}{h} \bigg) .\]
The bandwiddth $h$ can be chosen efficiently with the following GCV(generalized cross-validation)
\[GCV(h)= \frac{\sum_{i=1}^{n} \Big\{ Y_i- \hat{f}(X_i;h) \Big\}^2}{\{ 1-n^{-1}\sum_{i=1}^{n}W_{k,i}(h) \}^2} \]

\newpage
{\color{royalblue}\section{Results}}
We use the dataset from Exercise 1 on Kenyan children. We are interested in the

following two variables\\
\textbf{hypage}: Age of a child
\\
\textbf{zwast}: Z-score for wasting\\
\\
Z-score for wasting is defined as the weight of a child standardised with the median and
standard deviation of children with the same height from the healthy population. We would
like to investigate how the Z-score for wasting changes with age, that is we consider the
model 
\[\mathtt{ zwast}_i= f(\mathtt{hypage}_i)+ \epsilon_{i}, \hspace{0.25cm}  \mathtt{for} \hspace{0.25cm}
\epsilon_{i} \sim \mathcal{N}(0,\sigma^{2}), \hspace{0.25cm}  i=1,\dots,n. \]

\begin{enumerate}[label=(\alph*)]
	\item After implementing a local polynomial fit and by fixing the polynomial degree to 1, we have the plots in figure \ref{figggg}. Since we want to avoid under- or oversmoothing, the ideal bandwidth from the plot \ref{fig 7.1} would be 8. This bandwidth is then used to plot the estimate of $f$ with 4 different bandwidths (figure \ref{fig 7.2}). It is clear that the curves of the estimator present the same shape, and almost the same level of smoothness. In this case, all the kernels might be used for further analysis.
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex7plot1.tex}}
			\caption{ with the Epanechnikov kernel and 4 different bandwidths}
			\label{fig 7.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex7plot2.tex}}
			\caption{ with four kernel functions: bandwidth = 8 }
			\label{fig 7.2}
		\end{subfigure}
		\caption{Plots of local polynomial estimator of $f$ }
		\label{figggg}
	\end{figure}
	
	 \newpage
	 \item Now we want to find the optimal bandwidth with Generalised Cross Validation (GCV). For this aim, we implement a function that calculates the GCV, then with used Epanechnikov kernel and obtained GCV-bandwidth to estimate f using
polynomial degrees from 1 to 4. The results are in table \ref{tableau71}. We note that the GCV-bandwidths are less than 8, and are different. This means that we will have different estimators for different polynomial degrees.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.9cm}|P{1.25cm}|P{1.25cm}|P{1.25cm}|P{1.25cm}|}
			\hline
			Polynomial degree & 1 & 2 & 3 & 4 \\
			\hline
			GCV-bandwidth & 4.999956 & 10.99995 & 10.99994 & 8.403991 \\
			\hline 
		\end{tabular}
		\caption{\label{tableau71}Optimal bandwidth for each polynomial degree}
	\end{table} 
	
	The plot of all four fits are in figure \ref{fig 7.3}. We notice that the curves follow the same pattern in general, with less smoothness meaning that the obtained GCV-bandwidths are completely reasonable. However, the fitting curve for the polynomial degree 1 is very smooth compared to the others, especially with the fitting curve of the polynomial degree which is less smooth than the others. Moreover, the boundaries highlight slight signs of over-fitting(for polynomial degree 4), or under-fitting (for polynomial degree 2). Overall, using the polynomial degree 3 with the corresponding optimal bandwidth would be ideal. 
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex7plot3.tex}}
		\caption{Plot of all four fits obtained using the GCV-bandwidths from table \ref{tableau71}}
		\label{fig 7.3}
	\end{figure}
	
	\newpage
	\item In this question, we use the function \textbf{localpoly.reg} of library \textit{NonpModelCheck} to calculate the first derivative of the function of $\mathtt{ zwast}$ with the GCV-bandwidth and polynomial degrees from 1 to 4. Figure \ref{fig 7.4} shows the plot of all four derivative fits. We can see noticeable differences on boundaries, since the curves  start at very different $\mathtt{ zwast}$ values. This suggest that some derivative fits(especailly the one from the polynomial degree 4) takes into account a lot of outliers, while others might use them less. Nevertheless, we do have signs of better $\mathtt{ zwast}$ after 2 years. Actually, after 2 years, the derivative fitting curves head towards 0 (and we also have some fluctuations around 0). Finally, the derivative fits curves we got here are too smooth, therefore GCV-bandwidths from \ref{tableau71} are not reasonable or not optimal. We may solve this issue just by using the corresponding derivatives for each polynomial degree to find the optimal bandwidths.
	
	
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex7plot4.tex}}
		\caption{Plot of all four derivative fits obtained using the GCV-bandwidths from table \ref{tableau71}}
		\label{fig 7.4}
	\end{figure}
	
	
	
\end{enumerate}


%88888888888888888888888888888888888888888888%

{\color{royalblue}\chapter{Nonparametric regression: splines}}
{\color{royalblue}\section{Problem description}}
In the previous chapter, we have explored the local polynomials regression method. However, this method suffers from Runge's Phenomena (especially when having lots of data). To solve this problem, we will generalized the local polynomials regression with splines.

{\color{royalblue}\section{Methods}}
Denote
\[ \mathcal{P}_m= \{p:p(x)= \sum_{i=1}^{m}c_ix^{i-1},\hspace{0.25cm}c_1,\dots,c_m,x \in \mathtt{R} \} \]
Next, consider an interval $[a, b) (a, b \in \mathbb{R}, -\infty < a < b < \infty)$.\\
Let $a=\tau_0<\tau_1<\dots<\tau_k<\tau_{k+1}=b$, $\tau_i \in \mathtt{R}$ and denote $\Delta= \{\tau_i \}^{k+1}_0$ a partition of $[a,b)$ into $k+1$ subintervals $[\tau_i,\tau_{i+1})$, $i=0,\dots,k$.\\
\textbf{Definition} Let $\Delta_k$ be a partition of $[a,b)$, then\\
$\mathcal{P}\mathcal{P}_m(\Delta_k)= \{p:\exists p_0,\dots,p_k \in \mathcal{P}_m$ such that $ p(x) = p_i(x)$ for $x \in [\tau_i; \tau_{i+1})$, $i =0,\dots,k \}$\\
is the \textbf{space of piecewise polynomials of order} $m$ \textbf{based on} $\Delta_k$.\\
\\
\textbf{Definition}\\
Let $\Delta_k$ be a partition of $[a,b)$. Let $m\in \mathbb{N}$ and $M=(m_1,\dots,m_k)$ be a vector of integers with $1\leqslant m_i\leqslant m$, $i=1,\dots,k$. Then, the space\\
$\mathcal{S}_m(M,\Delta_k)= \Big\{s:\exists s_0,\dots,s_k \in \mathcal{P}_m$ such that $s(x)=s_i(x)$ for $x\in [\tau_i; \tau_{i+1})$, $i=0,\dots,k$ and $s^{(j)}_{i-i}(\tau_i)=s^{(j)}_{i}(\tau_i)$, $j=0,1,\dots,m-1-m_i$, $i=0,\dots,k\Big\}$\\
is the \textbf{space of polynomial splines of order} $m$ \textbf{and multiplicities} $M$ \textbf{based on} $\Delta_k$.
Further we will deal with $\mathcal{S}_m(M = (1,\dots,1), \Delta_k)$ only and will denote it just $\mathcal{S}_m(\Delta_k)$.\\
\textbf{Definition} B-splines $N_i(x)$ of order $m$ can be defined as:
\begin{enumerate}
	\item $m$ polynomials pieces of degree $m-1$, which join at $m-1$ inner knots
	\item $N_i(x) > 0$ for $x \in [\tau_i, \tau_{i+m})$ and is zero for $x$s outside of this interval
	\item $N_i(x)$ overlaps with $2(m-1)$ pieces of its neighbors
	\item $\sum_{i=j-m+1}^{j}N_i(x)=1$, $x\in [\tau_j, \tau_{j+1})$
\end{enumerate}
Now consider a fixed design regression model with deterministic $\{x_i\}^n_{i=1} \in [0,1]$
\[ Y_i=f(x_i)+ \epsilon_{i}, \hspace{0.25cm}\mathtt{cov}(\epsilon_{i}\epsilon_{j})=\sigma^2\delta_{ij}, \hspace{0.25cm}\mathtt{E}(\epsilon_{i})=0,\hspace{0.25cm}i=1,\dots,n.\]
Regression function f is estimated by regression splines, that is
\[ \hat{f}_n= N(\cdot)\mathtt{ arg}\min_{\beta \in \mathbb{R}^{k+m}}(Y-N\beta)^t(Y-N\beta)=N(\cdot)(N^tN)^{-1}N^tY,\]
where $N=\{ N(x_1)^t,\dots,N(x_n)^t \}^t$ is the basis matrix with $N(x)=\{ N_1(x),\dots,N_{k+m}(x) \}$ as some basis of $\mathcal{S}_m(\Delta_k).$\\
\\
Furthermore, in order to find the optimal knot, one can use the following \textbf{generalized cross validation},
\[ \mathtt{GCV}(k)= \frac{||\hat{f}_n-f||^2}{[1-tr\{N(N^tN)^{-1}N^t\}/n]^2}=
\frac{||\hat{f}_n-f||^2}{(1-k/n)^2}. \]

\newpage
{\color{royalblue}\section{Results}}
The dataset \textit{stemcells.txt} contains 144 observations of the order parameter of a living stem
cell observed every 10 minutes over 24 hours. We would like to understand how the order
parameter evolves over time, i.e., we consider the model $OP_i = f(t_i) + \epsilon_{i}$, $i = 1,\dots,144$, where $t_i$ are the time points. Thereby, $\epsilon_{i}$ are not independent and identically distributed,
but rather follow an autoregressive process of the first order. That is, $\epsilon_{i} = \alpha \epsilon_{i?1} + \xi_i$ for $\xi_i \sim \mathcal{N}(0,\sigma^2)$ and $\alpha \in (0,1)$. Note that the correlation matrix $R$ of $(\epsilon_1,\dots,\epsilon_{n})^t$ is given by $R_{i,j}= \alpha^{|i-j|}$. For this data you can take $\alpha=0.55$. 

\begin{enumerate}[label=(\alph*)]
	\item First we implemented an R function that depends on the response, covariate, number of knots and
spline degree that calculate a regression spline fit for $f$. By fixing the spline degree to 2
and estimate $f$ with various number of knots, we get obtain the following figure \ref{fig 8.1}. It noticeable that the fitting curve is too smooth for small number of knots(which may cause underfitting) and less smooth for larger values (which may cause overfitting). Then by fixing the number of knots to 4, we estimate $f$ using
splines of degree from 1 to 4 and obtain the plot \ref{fig 8.2}. We can notice that per intervals, we have a line for the degree 1, which is curved as the the number increases. Hence for a good fit, we need to find the optimal number of knots and the spline degree.
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex8plot1.tex}}
			\caption{with spline degree 2, and various number of knots}
			\label{fig 8.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex8plot2.tex}}
			\caption{ with 4 number of knots and four spline degrees }
			\label{fig 8.2}
		\end{subfigure}
		\caption{Plots of regression spline fit for $f$}
		\label{figu}
	\end{figure}

    
    \newpage 
    \item In order to estimate the optimal number of (equidistant) knots, a function using the Generalised Cross Validation (GCV) is implemented. First, we ignore that the data are dependent, and obtain the results in table \ref{tableau81}. Then we calculate the fits with the number of knots obtained with GCV and spline degrees from 1 to 4, and the resulting estimators are represented in plot \ref{fig 8.3}. It is noticeable that we are in case of overfitting, making these estimators unreasonable.
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{3.25cm}|P{0.5cm}|P{0.5cm}|P{0.5cm}|P{0.5cm}|}
    		\hline
    		Spline degree & 1 & 2 & 3 & 4 \\
    		\hline
    		GCV knots' number & 29 & 33 & 35 & 21 \\
    		\hline 
    	\end{tabular}
    	\caption{\label{tableau81}GCV knots' number for each spline degree}
    \end{table}
    
    \begin{figure}[H]
    	\centering
    	\scalebox{1}{\input{Ex8plot3.tex}}
    	\caption{Spline regression estimators with obtained GCV knots' number}
    	\label{fig 8.3}
    \end{figure}
    
    \newpage
    \item In this question, we update the functions for regression
splines and GCV so that they take into account that the errors $\epsilon_{i}$ follow and autoregressive process of order one, and have the results in table \ref{tableau82}. Then we calculate the fits with the number of knots obtained
with this updated GCV criterion and spline degrees from 1 to 4. The resulting
estimators are plotted in figure \ref{fig 8.4}. We notice that the estimators fit here are more relevant, since for each spline degree, the smoothness minimizes over- or under-fitting. 
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{3.25cm}|P{0.5cm}|P{0.5cm}|P{0.5cm}|P{0.5cm}|}
    		\hline
    		Spline degree & 1 & 2 & 3 & 4 \\
    		\hline
    		GCV knots' number & 6 & 3 & 1 & 1 \\
    		\hline 
    	\end{tabular}
    	\caption{\label{tableau82}GCV knots' number (auto-regressive) for each spline degree}
    \end{table}
    
    \begin{figure}[H]
    	\centering
    	\scalebox{1}{\input{Ex8plot4.tex}}
    	\caption{Spline regression estimators with obtained auto-regressive GCV knots' number}
    	\label{fig 8.4}
    \end{figure}
    
    \newpage
    Furthermore, we add to the plot a parametric fit, such
that $f(t_i)= \sum_{j=0}^{4}\beta_jt^j_i$.
    \begin{figure}[H]
    	\centering
    	\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
    		\centering
    		\scalebox{0.75}{\input{Ex8plot5.tex}}
    		\caption{Parametric fit plot added to the plot \ref{fig 8.4} }
    		\label{fig 8.5}
    	\end{subfigure}
    	\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
    		\centering
    		\scalebox{0.75}{\input{Ex8plot6.tex}}
    		\caption{ Plot of the parametric for various degrees }
    		\label{fig 8.6}
    	\end{subfigure}
    	\caption{}
    	\label{figuu}
    \end{figure}
    We can notice from \ref{fig 8.5} that the parametric fit of degree 4 is very close to the splines fit with degrees 2, 3 and 4. Hence it is resonable to use a fitting model. Figure \ref{fig 8.6} indicates that it also works for polynomial of 5th degree, but not for 3rd degree, because an underfitting problem.
    
\end{enumerate}



%99999999999999999999999999999999999999999999%

{\color{royalblue}\chapter{Mixed models}}
{\color{royalblue}\section{Problem description}}
Let's illustrate the problem in this section with the following exemple. The (simple) linear model state there is a linear dependence between explanatory variables $Y_{i,t}$, and the time $t$ (dependent variable), with an error term $\epsilon_{i,t}$
\[ Y_{i,t} = \alpha + \beta_t + \epsilon_{i,t}, \hspace{0.25cm}
 \epsilon_{i,t}\sim \mathcal{N}(0,\sigma^2)\]
$t,\alpha ,\beta $: fixed effects.
$\alpha ,\beta, \sigma$: parameters to estimate.
One limit of this model is that it considers $Y_{i,t}$ to be independent, which is not often the case as they can be correlated. Therefore, take this into account, a random effect is added to the fixed effects. We obtain a (linear) mixed model

{\color{royalblue}\section{Methods}}
The data we consider are of the form $Y_{ij}$, $i = 1,\dots, n$, $j= 1,\dots,k_i$. That is, there are $n$
subjects (clusters) and for each subject i there are $k_i$ observations.
A mixed model can be represented as
	\[ Y= X\beta+ Zu+ \epsilon, \hspace{0.25cm}
\mathtt{E}
\begin{pmatrix}
u \\
\epsilon
\end{pmatrix}=
\begin{pmatrix}
0 \\
0
\end{pmatrix}, \hspace{0.25cm}
\mathtt{cov}
\begin{pmatrix}
u\\
\epsilon
\end{pmatrix}=
\begin{pmatrix}
\sigma^2_u D & 0 \\
0 & \sigma^2_\epsilon \Sigma
\end{pmatrix}, \]
for $Y, \epsilon \in \mathbb{R}^N$, $N=\sum_{i=1}^{n}k_i$, $Z= \mathtt{blockdiag}(Z_1,\dots,Z_n)\in \mathbb{R}^{N\times nm}$, $u\in \mathbb{R}^{nm}$
Typically, a normality assumption is made both for $\epsilon$ and $u$.\\
A good exemple of mixed models' application is in \textbf{small area estimation}, where it is assumed that the means in "small areas" differ for a random amount:
\[ Y_i= X_i\beta+ \mathbb{I}_{n_i}u_i+ \epsilon_{i}, \hspace{0.25cm}
  \epsilon_{i} \sim \mathcal{N}_{k_i}(0_{k_i},\sigma^2I_{k_i}), \hspace{0.25cm}
  u_i\sim \mathcal{N}(0,\sigma^2_u), , \hspace{0.25cm} i=1,\dots,n, \]
where in each $i$th small area there are $k_i$ observations available.

\newpage
In the exercise, we are interested in the estimation of $u$. Since it is assumed to be random, one typically uses the term "predictor" for $\hat{u}$, which is obtained as a \textit{best linear
unnbiased predictor} (\textbf{BLUP})
\[ \hat{u}= \frac{1}{\lambda}Z^t V^{-1}(Y-X\hat{\beta}) \]
\[ \hat{\beta}= (X^tV^{-1}X)^{-1}X^tY,\]
where $V= I_{N}+ ZZ^t/\lambda$ for $\lambda= \frac{\sigma^2_\epsilon}{\sigma^2_u}$, $N=\sum_{i=1}^{n}k_i>p$

\newpage
{\color{royalblue}\section{Results}}
For the data, we consider the survey and satellite data measuring the area for corn and soy fields in NorthCentral Iowa from 1978. The data set is available
as \textit{landsat} in the R-package JoSAE. We are interested in obtaining reliable estimates for the total size of corn and soy production for each of the 12 counties in the data set, respectively.
Variables of interest:
\begin{enumerate}
	\item[] $\mathtt{SegmentsInCounty}$: number of of segments of county.
	\item[] $\mathtt{SegementID}$: identificator for segment.
	\item[] $\mathtt{HACorn}$: hectares of corn for given segment.
	\item[] $\mathtt{HASoybeans}$: hectares of soybeans for given segment.
	\item[] $\mathtt{PixelsCorn}$: pixels for corn for given segment.

	\item[] $\mathtt{PixelsSoybeans}$: pixels for soybeans for given segment.
	\item[] $\mathtt{MeanPixelsCorn}$: mean of pixels for corn over all segments in given county.
	\item[] $\mathtt{MeanPixelsSoybeans}$: mean of pixels for soybeans over all segments in given county.
	\item[] $\mathtt{CountyName}$: county identificator of the segment.
\end{enumerate}

\begin{enumerate}[label=(\alph*)]
	\item In order to fit a linear model to both the hectares of corn and soybeans for segment for
each county, we chose $\mathtt{PixelsCorn}$ and $\mathtt{PixelsSoybeans}$ variables since we want to estimate for the total size of corn and soy production. The limitations
of this linear model is that it could not estimate the hectares of corn and soybeans for segment in some counties; probably because of a random effect that the model cannot take into consideration.
	
	\item We fit a linear mixed model $y_{ij} = x^t\beta + v_i + e_{ij}$ for both crops such that segments share
the same countywide random effect. We can interpret from table \ref{tableau91} that Soybeans tend to have 
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|P{2.5cm}|P{2.5cm}|}
			\hline
			& \multicolumn{2}{c|}{Fixed effects} & \multicolumn{2}{c|}{Random effects (StdDev)} \\
			\hline
			& (Intercept) & Pixels & (Intercept) & Residual \\
			\hline
			Corn & 5.4661894 & 0.3878358 & 7.926246 & 17.03993 \\
			\hline 
			Soybeans & -3.8223556 & 0.4756781 & 15.46753 & 13.41709 \\
			\hline 
		\end{tabular}
		\caption{\label{tableau91}Results of the linear mixed model}
	\end{table}
	Figure \ref{figuuuuuu} shows that the Q-Q plot indicate points falling on the identity line, which means that the residuals are well normally distributed.
		 
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex9plot1.tex}}
			\caption{Corn linear mixed model}
			\label{fig 9.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex9plot2.tex}}
			\caption{Soybean linear mixed model}
			\label{fig 9.2}
		\end{subfigure}
	    \caption{Residuals Q-Q plot for normality assumption}
	    \label{figuuuuuu}
    \end{figure}
	
	Plots in figure \ref{figuuuuuuu} show points randomly displaced, which means that the linearity assumption is fulfill.
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex9plot3.tex}}
			\caption{Corn linear mixed model}
			\label{fig 9.3}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex9plot4.tex}}
			\caption{Soybean linear mixed model}
			\label{fig 9.4}
		\end{subfigure}
		\caption{Plots of explanatory variable of each model, against their residuals to check linearity assumption}
		\label{figuuuuuuu}
	\end{figure}
	Finally, homoskedasticity of the models is also fulfill, because plots in figure \ref{figuuuuuuuu} show no patterns.
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex9plot5.tex}}
			\caption{Corn linear mixed model}
			\label{fig 9.5}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex9plot6.tex}}
			\caption{Soybean linear mixed model}
			\label{fig 9.6}
		\end{subfigure}
		\caption{Fitted values vs residuals for each model}
		\label{figuuuuuuuu}
	\end{figure}
	
	\item Now we want to compare and evaluate with respect to their reliability, in order to obtain predictions for $\mu_i= \bar{x}^t_{ip}\beta+ v_i$. Here, for the $i$th county and a specified crop, $x_{ip}$ is the population mean of the explanatory variables and $\bar{x}_i$ the mean over the observed segments only. Also $\hat{\beta}$ is the weighted least-squares estimator for $\beta$ and $\gamma_i= \sigma^2_v(\sigma^2_v+ n^{-1}_i\sigma^2_\epsilon)^{-1}$ where $n_i$ the number of observations in the $i$th county, $\sigma^2_v$ and $\sigma^2_\epsilon$ the variances of random effect and error, respectively.
	\begin{itemize}
		\item Regression predictor: $\mu^0_i=\bar{x}^t_{ip}\hat{\beta}$.
		\item Adjusted survey predictor: $\mu^1_i=\bar{x}^t_{ip}\hat{\beta}+ (\bar{y}_i- \bar{x}^t_{i}\hat{\beta})$
		\item (Empirical) BLUP: $\mu^\gamma_i=\bar{x}^t_{ip}\hat{\beta}+ \hat{\gamma_i}(\bar{y}_i- \bar{x}^t_{i}\hat{\beta})$
		\item Survey predictor: $\bar{y}_i= n^{-1}_i\sum_{n_i}^{j=1}y_{ij}$
	\end{itemize}
	An estimate for the mean squared error $\mathtt{MSE}_{\mu_i}(\mu^d_i)= \mathtt{E}(\mu_i- \mu^d_i)$ for $\mu^d_i$ is given by
	\[ \widehat{\mathtt{MSE}}_{\mu_i}(\mu^d_i)= (1-d)^2\hat{\sigma}^2_v+ \frac{d^2\hat{\sigma}^2_\epsilon}{n_i}+ 2(d-\hat{\gamma_i})(\bar{x}_{ip}- d\bar{x}_i)^t\hat{V}(\hat{\beta})\bar{x}_i\]
	\[ \hspace{4.75cm} +(\bar{x}_{ip}- d\bar{x}_{i})^t \hat{V}(\hat{\beta})(\bar{x}_{ip}- d\bar{x}_{i}) ,\]
	where $\hat{V}(\hat{\beta})$ is the covariance matrix of $\hat{\beta}$. Tables \ref{tableau92} and \ref{tableau93} the respective MSE. We can notice the results really differ form one county to another, which tells us that in order ot have good predictions for $\mu_i$, it might better to use different predictors fo each County.
	
	\newpage
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}
			\hline
			\textbf{Counties} & $\mu^0_i$ & $\mu^1_i$ & $\mu^\gamma_i$ & $\bar{y}_i$ \\
			\hline
			Cerro Gordo & 72.05803 & 283.61578 & 61.66718 & 302.12264 \\
			\hline
			Hamilton & 71.97262 & 280.62908 & 61.72427 & 306.22149 \\
			\hline 
			Worth & 71.73962 & 287.68234 & 61.13340 & 292.90282 \\
			\hline 
			Humboldt & 69.41864 & 138.57817 & 53.48326 & 167.63961 \\
			\hline 
			Franklin & 65.34791 & 95.65654 & 3.27852 & 97.63366 \\
			\hline 
			Pocahontas & 64.34482 & 92.96560 & 43.50445 & 99.09076 \\
			\hline 
			Winnebago & 65.82172 & 96.77393 & 43.28370 & 96.78756 \\
			\hline 
			 Wright & 65.61789 & 95.44570 & 43.89862 & 101.67489 \\
			\hline 
			Webster & 64.08903 & 72.66995 & 38.47434 & 72.59204 \\
			\hline 
			Hancock & 63.18738 & 59.00156 & 34.43863 & 59.81280 \\
			\hline 
			Kossuth & 62.27490 & 58.09003 & 33.53265 & 58.50767 \\
			\hline 
			Hardin & 63.19271  & 50.62384 & 32.28565 & 51.50612 \\
			\hline 
		\end{tabular}
		\caption{\label{tableau92}Results of estimated predictors' MSE for Corn}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}
			\hline
			\textbf{Counties} & $\mu^0_i$ & $\mu^1_i$ & $\mu^\gamma_i$ & $\bar{y}_i$ \\
			\hline
			Cerro Gordo & 233.9442 & 182.87015 & 116.21477 & 208.61640 \\
			\hline
			Hamilton & 235.5158 & 180.10725 & 107.79506 & 180.73675 \\
			\hline 
			Worth &  234.7851 & 179.91494 & 108.30543 & 183.17044 \\
			\hline 
			Humboldt & 231.4987 & 96.67826 & 74.44887 & 100.92499 \\
			\hline 
			Franklin & 223.0584 & 60.24062 & 49.31044 & 60.55884 \\
			\hline 
			Pocahontas & 219.5358 & 59.75710 & 49.03094 & 60.22818 \\
			\hline 
			Winnebago & 222.6925 & 60.45358 & 49.56226 & 61.04741 \\
			\hline 
			Wright & 225.2767 & 61.91758 & 50.95105 & 62.20606 \\
			\hline 
			Webster & 217.4335 & 44.87112 & 38.54084 & 45.10319 \\
			\hline 
			Hancock & 219.6162 & 37.21818 & 32.99170 & 37.69316 \\
			\hline 
			Kossuth & 220.0136 & 36.18285 & 31.92317 & 36.19472 \\
			\hline 
			Hardin & 220.1881 &  31.62848 & 28.61561 & 31.80195 \\
			\hline 
		\end{tabular}
		\caption{\label{tableau93}Results of estimated predictors' MSE for Soybeans}
	\end{table}
	
	\item The estimation of the total county field size for both crops, using the BLUP and Survey from (c) is highlighted in the table \ref{tableau94} and on maps \ref{fig 9.7} and \ref{fig 9.8}. We can notice that the size given by the BLUP and the Survey are completely different. This confirms the fact that we need to chose the appropriate predictor in each County(by taking the one with the lowest MSE for each County).
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}
			\hline
			& \multicolumn{2}{c|}{Corn} & \multicolumn{2}{c|}{Soybeans} \\
			\hline
			\textbf{Counties} & \textbf{BLUP} & \textbf{Survey} & \textbf{BLUP} & \textbf{Survey} \\
			\hline
			Cerro Gordo & 36232.59 & 48947.27 & 14850.16 & 1534.673 \\
			\hline
			Hamilton & 37163.87 & 28934.53 & 18334.02 & 20850.800 \\
			\hline 
			Worth &  32692.98 & 22032.77 & 17913.60 & 21267.008 \\
			\hline 
			Humboldt & 33537.21 & 43869.76 & 18034.58 & 7739.632 \\
			\hline 
			Franklin & 43710.09 & 50475.53 & 12457.91 & 9868.135 \\
			\hline 
			Pocahontas & 28151.91 & 26365.93 & 27971.91 & 29333.507 \\
			\hline 
			Winnebago & 33973.49 & 32903.88 & 18073.97 & 16418.839 \\
			\hline 
			Wright & 37119.98 & 43470.81 & 24957.12 & 21649.008 \\
			\hline 
			Webster & 29310.33 & 30829.88 & 27162.40 & 27916.228 \\
			\hline 
			Hancock & 39007.80 & 34376.57 & 19952.53 & 23338.179 \\
			\hline 
			Kossuth & 33598.79 & 32926.76 & 24392.10 & 24112.061 \\
			\hline 
			Hardin & 42742.30 & 37426.91 & 13180.96 & 15894.074 \\
			\hline 
		\end{tabular}
		\caption{\label{tableau94}Results of the estimation the total county field size for both crops, by the BLUP
and survey form part (c)}
	\end{table}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{Ex9plot7.png}
		\caption{Map of Iowa with estimated the total county field size for corn}
		\label{fig 9.7}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{Ex9plot8.png}
		\caption{Map of Iowa with estimated the total county field size for soybean}
		\label{fig 9.8}
	\end{figure}
	
	The huge differences between the slopes and intercepts of the models describe notable differences between the crops.
	
	
	
	
	
	
\end{enumerate}


%10101010101010101010101010101010101010101010%

{\color{royalblue}\chapter{Partial least squares}}
{\color{royalblue}\section{Problem description}}
In some problems of linear regression or prediction phenomenon, explicable variables can be correlated, sometimes causing  multicollinearity. This phenomenon that occurs most of the time in big data analysis, is a consequence of bad results related to regressions' coefficients estimated with least squares. To solve this problem, we have principal component analysis (PCA), and the partial least squares (PLS). We are using the latter.

{\color{royalblue}\section{Methods}}
Let $x \in \mathbb{R}^m$ and $y \in \mathbb{R}$ be two zero-mean random variables. Denote $\sigma = cov(x)$ and
$\delta = cov(x, y)$. Assume $y = x\beta + e$, $\mathtt{E}(e) = 0$ and $cov(e) = \sigma^2$. This is called a population
model.\\
Now, let $Y = (y_1,\dots,y_n)^t \in \mathtt{R}^n$ be a vector of $n$ independent copies of $y$ and $X = (x^t_1,\dots,x^t_n)^t \in \mathtt{R}^{n\times m}$ be a matrix of $n$ independent copies of $x$. In particular, $Y= X\beta + \epsilon$, where $\mathtt{E}(\epsilon)= 0_n$ and $cov(\epsilon)= \sigma^2 I_n$.\\
Denote $A = X^tX$ and $b = X^tY$ ; note that $A$ and $b$ are proportional to the sample estimators of $\sigma$ and $\delta$, respectively.\\
Denote now $K_d(b; A) = span(b, Ab,\dots, A^{d-1}b)$ the $d$-dimensional Krylov space, where $d \leq rk(A)$ should be chosen data-driven. Then, the
partial least squares estimator can be defined as 
\[ \hat{\beta}^d_{PLS}= \mathtt{arg}\min_{\beta \in K_d}= ||Y-X\beta||^2 \]
There are several ways to compute PLS, but in \textbf{R} it can be done with libraries \textbf{plsr} and \textbf{predict.plsr}.

\newpage
{\color{royalblue}\section{Results}}
The data of interest for this exercise can be find on the page \href{https://sites.google.com/michalkosinski.com/mypersonality/publications}{ myPersonality.org }. There are three files:
\begin{enumerate}
	\item [] \textit{users.csv} contains psychodemographic user profiles
	\item [] \textit{likes.csv } contains anonymised IDs and names of Facebook Likes
	\item [] \textit{users-likes.csv} contains the associations between users and their Likes
\end{enumerate}
The seed value for used in for the exercise is 1122.
\begin{enumerate}[label=(\alph*)]
	\item Check the R code related to this exercise
	\item With the PLS, we would like to find a model that allows
to predict the user?s age based on Likes (s)he made. First we split the dataset into a test and training set, by sampling randomly two thirds of all rows to include into the training set and the rest
will be the test set. Then on the training set fit PLS regression models with age as a response variable
and with up to 50 PLS components. For each PLS model dimension (from 1 to 50), we obtain the prediction on the test set and compare it with the true age values from the test set, and calculating the Pearson correlation coefficient. The plot \ref{fig 10.1} clearly shows that we have a maximal value $d_{opt}=$, which is then used as the model of dimension. The plot \ref{fig 10.2} is the Plot the values predicted by the PLS
model of dimension $d_{opt}=8$ on the test set against corresponding age values from the test
set. This plot shows that the model is pretty accurate since points are along the identity line.
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\includegraphics[scale=0.25]{Ex10plot1.png}
			\caption{Plot of the Pearson correlation coefficients against model dimension}
			\label{fig 10.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\includegraphics[scale=0.25]{Ex10plot2.png}
			\caption{Plot of the values predicted by the PLS model of dimension $d_{opt}$ on the test set against corresponding age values from the test set}
			\label{fig 10.2}
		\end{subfigure}
		\caption{}
		\label{figuuuu}
	\end{figure}
	
	\newpage
	\item Now we investigate which Likes predict the age best, using the best predictive model identified in (b). From figure \ref{figuuuuu}, we can notice that likes with the largest positive effect on the age, that is elderly people likes, are effectively topics of elders. While likes with the largest negative effect on the age, that is youngsters likes, are also things for young people. Hence we can assume that the model is accurate.
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\includegraphics[scale=0.25]{Ex10plot3.png}
			\caption{The 6 Likes having the largest positive effect on the age}
			\label{fig 10.3}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\includegraphics[scale=0.25]{Ex10plot4.png}
			\caption{The 6 Likes that have the largest negative effect on the age}
			\label{fig 10.4}
		\end{subfigure}
		\caption{}
		\label{figuuuuu}
	\end{figure}
	
	\item Now we repeat the analysis removing users that made less than 60 Likes and Likes that have
less than 120 users in (a). Due to the huge matrix' size, I could not run de model. However, it is expected to have the same likes and users as in figure \ref{figuuuuu}, but with more significant coefficients. For a plot line the one in figure \ref{fig 10.2}, we should have points more closer to the identity line.
	
\end{enumerate}




\end{document}