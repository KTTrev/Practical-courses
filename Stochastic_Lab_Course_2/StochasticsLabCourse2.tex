\documentclass{report}
%chapter style Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, %Bjornstrup
\usepackage[Bjarne]{fncychap}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\numberwithin{equation}{section} %for equations numbering
\usepackage{amssymb}
\usepackage[top=5cm, bottom=5cm, left=3cm, right=3cm]{geometry}
\pagestyle{headings}
\usepackage{lmodern}
\usepackage{enumitem}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{float, graphicx, subcaption}
\usepackage{flafter}
\usepackage[%  
colorlinks=true,
pdfborder={0 0 0},
linkcolor=red
]{hyperref}
\usepackage{fancyhdr}
\usepackage{caption} %to fix referencing to exact the label
\usepackage[dvipsnames]{xcolor}
\DeclareMathOperator{\argmax}{argmax}
\newtheorem{theorem}{Theorem}
%define my own color
\definecolor{royalblue}{rgb}{0.0, 0.14, 0.4}

\usepackage{tikz} %for .tex vector images

%pagestyle
\pagestyle{fancy}
\fancyhead[RE,LO]{{\color{royalblue}Stochastics Lab Course II}}
\fancyhead[LE,RO]{\leftmark}
\fancyfoot[RE,LO]{\rightmark}
{\color{royalblue}\fancyfoot[LE,RO]{\thepage}}
\cfoot{}

%decorative lines
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headrule}{\hbox to\headwidth{
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\footrule}{\hbox to\headwidth{%
		\color{royalblue}\leaders\hrule height \headrulewidth\hfill}}

\title{\textsc{Stochastics Lab Course II}}
\author{Khwam Tabougua Trevor}
\date{March 2019}
\begin{document}
	
\maketitle

{\color{royalblue}\chapter*{Introduction}}

The "Stochastics Lab course II" is an Introductory Course for
statistics and stochastics applications with R programming language. The course lasted for two weeks in March 2019. The report written on \LaTeX, contains a description of the problem, a description of the methods usedto solve the problem and a detailed discussion of the results.

%1111111111111111111111111111111111111111111111%
\tableofcontents
{\color{royalblue}\chapter{Tidyverse}}
{\color{royalblue}\section{Problem description}}
R base tools can accomplish "almost" every programming tasks. However, when using large datasets or when implementing complex tasks(like graphs, maps, tidying, etc), things get complicated. We want to enhance our algorithms fo batter results or productivity. To this aim, we will use the Tidyverse package.

{\color{royalblue}\section{Methods}}
Tidyverse is a collection of packages for data manipulation, exploration and visualization. The core packages are \textbf{ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats}, but we will only be using ggplot2, dplyr, tidyr, and tibble.
\begin{itemize}
	\item[--] \textbf{ggplot2} is a system for declaratively creating graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
	
	\item[--] \textbf{dplyr} is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges such as adding new variables (that are functions of existing variables), picking variables based on their names, selecting rows (based on their value), reducing multiple values down to a single summary, and changing the ordering of the rows.
	
	\item[--] \textbf{tidyr} package goal is to help you create tidy data. Tidy data is data where each variable is in a column, each observation is a row, and Each value is a cell.
	
	\item[--] \textbf{tibble} package goal is to use tibbles, which are modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors).
	
	
\end{itemize}

{\color{royalblue}\section{Results}}

\begin{enumerate}[label=(\alph*)]
	\item After loading and filtering the data childrenfinal.dta, we convert some variables (namely $\mathtt{tetanusmother}$, $\mathtt{breastfeeding}$, $\mathtt{wantedchild}$, $\mathtt{anetalvisits}$, and $\mathtt{placedelivery}$) into double labeled \textit{<dbl>} (doubles, or real numbers).
	
	\item 
	\item[°]The smooth line in figure \ref{f} indicates that the $\mathtt{zstunt}$ is getting between the ages of 0 and 20, before stabilizing.
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex1plot1.tex}}
		\caption{Scatter plot of $\mathtt{zstunt}$ against $\mathtt{hypage}$ with a smooth line (in purple)}
		\label{f}
	\end{figure}
	
	\newpage
	\item[°]gjdhgdhdg
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex1plot2.tex}}
			\caption{For each gender}
			\label{fig 1.2}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex1plot3.tex}}
			\caption{For each area}
			\label{fig 1.3}
		\end{subfigure}
		\caption{Smooth plots
of $\mathtt{zstunt}$ against $\mathtt{hypage}$ }
		\label{fi}
	\end{figure}
	
	\item[°]gjdhgdhdg
		
	\item 
	\begin{figure}[ht]
		\centering
		\includegraphics[scale=0.5]{Ex1plot4.png}
		\caption{Scatter plot of zstunt against hypage with smooth line (in purple)}
	\end{figure}	
		
		
\end{enumerate}	
%222222222222222222222222222222222222222222222%

{\color{royalblue}\chapter{Random number generation}}

{\color{royalblue}\section{Problem description}}
Generating a random variable from any distribution is very essential for stochastics studies. However, true random numbers are not always available, but we can use some algorithms that generate some pseudo-random numbers.\\

{\color{royalblue}\section{Methods}}
Pseudo-random numbers are a sequence of numbers that appear "random" or approximate properties of random numbers with the following properties:\\
\begin{itemize}
	\item Good approximation of the properties of random numbers.
	\item Number can be easily and efficiently generated.
	\item Reproducibility (truly random numbers never satisfy this).
\end{itemize}

With the help of some probability properties, it is typically enough to be able to use a uniform distributed random variable, in order to generate any pseudo-random numbers from a given distribution.\\
The simplest idea for generating uniformly distributed pseudo-random numbers is using a \textit{linear congruent generators} (LCG):\\
\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Choose positive integer parameters a, c and m.
				\item Choose an initial value $x_0 \in \{0, 1, \ldots, m - 1\}$ (this value is called the seed).
				\item For each $n \in \mathbb{N} $, compute\\
				\begin{equation}
				x_{n+1} := ax_{n} + c \hspace{0.2cm}
				\text{mod} \hspace{0.2cm} m, \hspace{1cm} \forall n \in \mathbb{N} 
				\end{equation}
				\item The psuedorandom numbers are the sequence $x_1; x_2; x_3; \ldots $.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We make some observations regarding the LCG algorithm.\\
\begin{itemize}
	\item The LCG can generates at most m distinct numbers which are contained in $\{0, 1, \ldots, m - 1\}$
	\item As soon as some number in the sequence, say ${x_n}_0$, is repeated (i.e.,$\exists p$ such that $x_{n_{0} + p} = {x_n}_0$),

	then the same is true for the entire sequence:
	\begin{equation}
	x_{n_{0} +p+j} = x_{n_{0} + j}, \hspace{0.5cm} \forall j \geqslant 1
	\end{equation}
	The number $p$ is called the period of the sequence. This musst be less than or equal to $m$.
	\item Suppose that (under the right conditions) the LCG approximates a uniform distribution with

	parameters $0$ and $m$. In this case, $x_n / m$ would approximate a uniform distribution with parameters
0 and 1.
\end{itemize}
To generate uniformly random numbers, the period has to be maximal
(i.e., $p = m$), so that we sample every value in the sequence before repeating any. One can show that LCG has a full period $m = 2^{b}$, $b \geqslant 2$ if and
only if $c \in (0; m)$ is odd and $a$ mod $4 = 1$.\\

After generating uniform pseudo-random numbers, we can easily obtain random variables from other distributions. The \textbf{inversion method} is one way of doing so, with the help of the following theorem:
\begin{theorem}
	Let F be a distribution function on $\mathbb{R}$. The quantile function $F^{-1}$ is defined by
	 \[ F^{-1}(u) = \text{inf} \{x: F(x)\ge u, 0 < u < 1 \} \]
	If $U \sim U_{[0; 1]}$, then $F^{-1}(u)$ has a distribution function
$F$.
\end{theorem}
Hence, for continuous distributions (exponential, Pareto, standard Cauchy, etc) where $F^{-1}$, we simply simulate $U_{i}$ (with LCG) and set $X_{i} = F^{-1}(U_i)$. If $F^{-1}$ cannot be inverted analytically, appropriate
numerical methods can be applied.\\

Let $X$ be a discrete random variable with ordered possible values $\{x_{1}, x{2},\ldots \}$, so that $F(x) = \displaystyle\sum_{i:x_{i} \leqslant x} P(X = x_{i})$ and
\[F^{-1}(r) =  \text{min} \{x_k \in \{x_{1}, x_{2},\ldots \}: \displaystyle\sum_{j=1}^{k} P(X = x_{j}) = \displaystyle\sum_{j=1}^{k}p_{j} \geqslant r\}\]
Then the inverse method becomes: set $X = x_{1}$ if and only if $U_{i} \in [0, p_{1})$ and $X = x_{k}$ if and only if
$U_{i} \in \Big[ \displaystyle\sum_{j=1}^{k-1}p_{j}, \displaystyle\sum_{j=1}^{k}p_{j}\Big)$, $k = 2, 3, \ldots$. Note that

\[P(X = x_{k}) = P\Big(\displaystyle\sum_{j=1}^{k-1}p_{j} \leqslant U_{i} < \displaystyle\sum_{j=1}^{k}p_{j} \Big) = \displaystyle\sum_{j=1}^{k}p_{j} - \displaystyle\sum_{j=1}^{k-1}p_{j} = p_{k} \]

For example, to simulate a Bernoulli random variable $Ber(p)$, generate $U \in U_{[0, 1]}$ and

set $X = 0$, if $U \leqslant 1 - p$ and $X = 1$ if $U > 1 - p$.\\

Another general approach to pseudo-random variables generation is the \textbf{acceptance-rejection method}.

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item [] \textbf{Data}: Two probability densitiy functions: $f$ for $X$ and $g$ for $Y$
				\item Find a constant $M > 0$ such that $sup_{x} \frac{f(x)}{g(x)} \leqslant c$ ;
				\item Obtain a sample $y$ from $Y$ ;
				\item Obtain a sample $u$ from the uniform distribution on $[0, 1]$;
				\item \textbf{if} $u < \dfrac{f(y)}{cg(y)}$ \textbf{then}
				\item $\bigg|$ Accept $y$ as a sample drawn from $f$;
				\item \textbf{else}
				\item $\bigg|$ Reject the value of $y$ and return to the sampling step (line 2);
				\item [] \textbf{Result:} $y$, a sample drawn from $f$ (using $g$)
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

\textbf{NB:} Computing $c$ could be difficult, but one can show that $c = sup_{x} {\frac{f(x)}{g(x)}}$\\

{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item 
	With the Wichmann-Hill pseudo-random number generator in R, we Simulate $N =
1000 $ binomial random variables $B(n = 10, p = 0.4)$ using three approaches: inversion

	method, by simulating corresponding Bernoulli random variables by inversion method
and using R built-in function rbinom. From the figure \ref{fig 2.1}, the histograms of the three samples present the same shape but are different. This proves that the "random" numbers generated by our methods are just approximations of true random numbers.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot1.tex}}
		\caption{Histogram of the empirical CDF of all
three samples}
		\label{fig 2.1}
	\end{figure}
	
	\item To use accept-reject method (and a generator for uniform random variables only), of $N = 10000$ standard normal distributed random variables
with density 
	$f(x) = (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}} $, the density of the
standard Cauchy distribution is used: 
	$ g(x)=\{{\pi(1+x^2)}^{-1}\} $.
	
	\item[°] The constant value $c$ for this method is given by $sup_{x} {\frac{f(x)}{g(x)}} = 1.520347 $
	\item[°] Then after computing the $N$ standard normal random variables, we notice that the estimated and theoretical acceptance probabilities are almost equal. This is well depicted with in the figure \ref{fig 2.2}, where the histogram of the obtain sample is symmetric and has the same shape as the standard normal density curve.
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot2.tex}}
		\caption{Histogram of the obtained sample and the standard
		normal density (in blue)}
	\label{fig 2.2}
	\end{figure}
	
	\item[°] The QQ-plot in figure \ref{fig 2.3} shows points following the identity line. Hence the accept-reject method used to simulate a standard normal distributed sample (using the the standard Cauchy density) is well accurate.
	
	\newpage
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex2plot3.tex}}
		\caption{QQ-plot}
		\label{fig 2.3}
	\end{figure}
	
	\item[°] However, it is not possible to simulate ample distributed from the standard Cauchy density using
the accept-reject method with a standard normal candidate density, simply because cannot find a $c$ such that $g(x) \leqslant cf(x)$ is verified ( because $ sup_{x} {\frac{g(x)}{f(x)}} = \infty $).
	
\end{enumerate}

%333333333333333333333333333333333333333333333%

{\color{royalblue}\chapter{Bootstrap}}
{\color{royalblue}\section{Problem description}}
Suppose that a sample $ \mathbf{X} = \{X_1,\ldots ,X_n \} $ is used to estimate a parameter $\theta$ of the distribution $P$ (which is unknown) and let $\hat{\theta} = S(\mathbf{X})$ be a statistic that estimates $\theta$. For the purpose of statistical inference on $\theta$,  we are interested in the sampling distribution of $\hat{\theta}$ (or certain aspects of it) so as to assess the accuracy of our estimator or to set confidence intervals for our estimate of $\theta$. If the true distribution $P$ were known, we could draw samples $\mathbf{X}_{l}, l = 1,\ldots, R \in \mathbb{N}$ from P and use Monte Carlo methods to estimate the sampling distribution of our estimate $\hat{\theta}$. The problem is that $P$ is unknown and we cannot sample from it.\\
The following section explains how to use bootstrap to make the interference on $\hat{\theta}$.

{\color{royalblue}\section{Methods}}
The bootstrap is a computerintensive resampling method, which  principle can be summarized by the following schematic diagram:
\begin{center}
	\begin{tabular}{|P{3.5cm} P{3.5cm} P{3.5cm} P{3.5cm}|}
		\hline
		\multicolumn{2}{|c}{\textbf{Real World}} & \multicolumn{2}{c|}{\textbf{Bootstrap World}} \\
		Unknown probability distribution & Observed random sample  & Empirical distribution & Bootstrap sample \\
		\multicolumn{4}{|c|}{$P \longrightarrow \mathbf{X} = \{X_1,\ldots ,X_n \}  \hspace{1cm} \Longrightarrow \hspace{1cm} \hat{P} \longrightarrow \mathbf{X}^{*} = \{{X_1}^{*},\ldots ,{X_n}^{*} \} $} \\
		\multicolumn{2}{|c}{$\Big\downarrow$} & \multicolumn{2}{c|}{$\Big\downarrow$} \\ 
		\multicolumn{2}{|c}{$\hat{\theta} = S(\mathbf{X})$} & \multicolumn{2}{c|}{$\hat{\theta}^{*} = S(\mathbf{X}^{*})$} \\  
		\multicolumn{2}{|c}{Statistic of interest} & \multicolumn{2}{c|}{Bootstrap replication} \\
		\hline  
	\end{tabular}
\end{center}

Then The idea is to sample from an empirical
distribution function. Recall that for random variables $Y = \{Y_1,\ldots ,Y_n \}$, the empirical distribution function is defined via $F_n(y) = n^{-1} \displaystyle\sum_{i=1}^{n}\mathbb{I}(Y_i \leqslant y)$ (we will use the notation $F_B$ for the bootstrap empirical distribution). If the sample of size $n$ is from a continuous distribution, then each
 observation has a probability $1/n$ and sampling from $F_n$ would be equivalent to draw with replacement from the sample. Hence the following algorithm:

\fbox{
	\begin{minipage}{1\textwidth}
		\begin{center}
			\begin{enumerate}
				\item Draw $n$ times with replacement from $\mathbf{X}$ to get a bootstrap sample $\mathbf{X}^{*}_{1}$ of size $n$. Repeat $R$ times to get $R$ bootstrap samples $\mathbf{X}^{*}_{1},\ldots,\mathbf{X}^{*}_{R}$, each of size $n$.
				\item Compute bootstrap statistics $S(\mathbf{X}^{*}_{1}),\ldots, S(\mathbf{X}^{*}_{R})$.
				\item Make inference about $\theta$ based on $S(\mathbf{X}^{*}_{1}),\ldots, S(\mathbf{X}^{*}_{R})$.
			\end{enumerate}
		\end{center}
	\end{minipage}
}\\

We can also evaluate the goodness of the estimators (point or interval) based on the bootstrap sample. We construct confidence intervals for $\theta$ from the bootstrap replications (see step 2 in the above algorithm). \\

First recall the definition of a confidence interval. Let $\mathbf{X} = (X_1, \ldots, X_n)$ be a sample from a population with distribution $P \in \mathcal{P} = \{ P_\theta :\theta \in \Theta \subset \mathbb{R}^d \}$. Let $C(\mathbf{X})$ depend only on the sample $\mathbf{X}$ and $\theta \in \Theta$ be an unknown parameter
of interest. If
\[ \inf_{P \in \mathcal{P}} P(\theta \in C(\Theta)) \geqslant 1-\alpha \]
for a fixed $\alpha \in (0,1)$, then $C(\Theta)$ is a \textbf{confidence set} for $\theta$ with \textbf{level of
significance} $1-\alpha$. If the parameter $\theta$ is real-valued, then 
$C(\Theta) = [\underline{\theta}(\mathbf{X}), \bar{\theta}(\mathbf{X})]$, for a pair of real-valued
statistics $\underline{\theta}$ and $\bar{\theta}$ is called a confidence interval for $\theta$.\\
Therefore, a natural way to construct the bootstrap confidence interval is to use empirical quantiles of the bootstrap distribution of $S(\mathbf{X})$: compute $\hat{\theta}^{*}_i = S(\mathbf{X}^{*}_{i})$, $i = 1,\ldots,R$ bootstrap statistics and set the confidence interval for $\theta$ by $[\theta^{*}_{L}, \theta^{*}_{U}]$, where $\theta^{*}_{L}$ and $\theta^{*}_{U}$ are respectively $\lfloor R(\frac{1-\alpha}{2}) \rfloor$-th and $\lfloor R(1-\frac{1-\alpha}{2}) \rfloor$-th value in the ordered list of $\hat{\theta}^{*}_i$. Such confidence intervals are
called \textbf{bootstrap percentile}  confidence intervals. By defining $F_{B}(x) = P (\hat{\theta}^{*} \leqslant x)$, note that we have $P (\hat{\theta}^{*} \leqslant \hat{\theta}^{*}_L ) \approx \frac{1}{2} \alpha$ and $P (\hat{\theta}^{*} \geqslant \hat{\theta}^{*}_U ) \approx \frac{1}{2} \alpha$, which makes a coverage probability of $1-\alpha$.\\

The confidence interval should have equal probability to both sides of $\hat{\theta}^{*}$, that is $P(\hat{\theta}^{*} \leqslant \theta \leqslant \hat{\theta}^{*}_U) = P(\hat{\theta}^{*}_L \leqslant \theta \leqslant \hat{\theta}^{*}) $. If $\hat{\theta}^{*}$ is not the median of the bootstrap distribution, this condition is not fulfilled. An appropriate correction is given by $ \hat{\theta}^*_{LC} = F^{-1}_B(\Phi[z_\frac{\alpha}{2} + 2\hat{z}_0]) $ and $ \hat{\theta}^*_{UC} = F^{-1}_B(\Phi[z_{1-\frac{\alpha}{2}} + \hat{z}_0]) $ (respectively  the bias-corrected lower and upper confidence bound for $\theta$), where $\Phi(.)$ is the cdf of the standard normal distribution and $\hat{z}_0 = \Phi^{-1}{\{ F_B(\hat{\theta})\}} $. This interval is the \textbf{bias corrected percentile interval}. In practice, $\hat{\theta}^*_{LC}=\lfloor R\alpha_1 \rfloor $ and $\hat{\theta}^*_{UC}=\lfloor R\alpha_2 \rfloor $,
with $\alpha_1 = \Phi(z_\frac{\alpha}{2} + 2\hat{z}_0)$ and $\alpha_2 = \Phi(z_{1-\frac{\alpha}{2}} + 2\hat{z}_0)$. \\

An extension of the bias corrected percentile confidence interval, the $BC_a$ (\textbf{bias-corrected accelerated} bootstrap) confidence interval described as follow: 
\newpage

\[\alpha_1=\Phi\Big(\hat{z}_0 + \frac{\hat{z}_0 + z_{\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{\alpha/2})}\Big) \]
\[\alpha_2=\Phi\Big(\hat{z}_0 + \frac{\hat{z}_0 + z_{1-\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{1-\alpha/2})}\Big) ,\]
where
\[\hat{a}=\frac{\sum_{i=1}^{n}(\bar\theta_J - \hat{\theta}_i)^3}{6\Big\{\sum_{i=1}^{n}(\bar\theta_J - \hat{\theta}_i)^2 \Big\}^{3/2}} \]
With $\bar{\theta}_J=n^{-1}\sum_{i=1}^{n}\hat{\theta}_{(i)}$, for $\hat{\theta}_{(i)}$ as the estimator of $\theta$ obtained without observation $i$, i.e., $\hat{\theta}_{(i)} = S(X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n)$. It is easily performed in \textbf{R} with \textbf{bootstrap::bcanon}.

\newpage
{\color{royalblue}\section{Results}}
\begin{enumerate}[label=(\alph*)]
	\item Let's consider a Weibull distribution with scale parameter $\lambda$, shape parameter $k$, variance $\sigma^2$ and median $x_{med}$. From a sample $(x_1,\ldots,x_n)$ simulated from the Weibull distribution with $\lambda=13$ and $k=1$, we aim to to build confidence intervals for $\sigma$ based on a statistics $\hat{s}^2=(n-1)^{-1}\sum_{i=1}^{n}(x_i-\bar{X})^2$, and $x_{med}$ based on the sample median.
	\begin{itemize}
		\item First, the sample size is set as $n = 100$, the number of bootstrap replications $R = 1000$
and the number of Monte Carlo samples $M = 1000$. We build two-sided bootstrap percentile confidence intervals for $\sigma$ and $x_{med}$ at the
significance level $\alpha=0.05$, and  Use $M$ Monte Carlo samples to estimate the coverage
probability(CP) and the average interval length(AIL) for both confidence intervals(CI). We get the following results:\\
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
				\hline
				& $x_{med}$ CI & $\sigma$ CI \\
				\hline
				CP & 0.944  & 0.856 \\
				\hline  
				AIL & 5.103662 & 6.006730 \\
				\hline  
			\end{tabular}
		\caption{\label{tableau31}Confidence intervals coverage probability and average interval length: $n = 100$, $R = 1000$}
		\end{table}
	The coverage probability for $x_{med}$ confidence interval is pretty close to $1-\alpha = 0.95$, the same for $\sigma$ confidence interval, but less than the CP $x_{med}$ CI. This might suggest that the bootstrap percentile confidence interval approximates $x_{med}$ CI more than $\sigma$ CI.
	
	\item Now, use the following settings: $n = R = 1000$ to get the results in table \ref{tableau32} and $n = 100$, $R = 5000$ and obtain the corresponding results in table \ref{tableau33}
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.947 & 0.935 \\
			\hline  
			AIL & 1.620852 & 2.211146 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau32}Confidence intervals coverage probability and average interval length: $n = R = 1000$}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.946 & 0.848 \\
			\hline  
			AIL & 5.101970 & 5.981314 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau33}Confidence intervals coverage probability and average interval length: $n = 100$, $R = 5000$}
	\end{table}
	We can notice that the CP value for both confidence intervals are again close to $0.95$, but huge differences with the AIL. Actually, we want the length of the confidence intervals to be narrow as possible, and the AIL in table \ref{tableau32} are the smallest AIL, and the CP are the largest. Hence, increasing the sample size and the number of bootstraps replications improves the accuracy of the bootstrap.
	
	\item With  $n = 100$, $R = 1000$ and $M=1000$, we build bootstrap accelerated
bias-corrected ($bc_a$) confidence intervals both for $\sigma$ and $x_{med}$, and Use $M$ Monte Carlo samples to assess the coverage probability and the average length of the confidence intervals to obtain the following table.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
			\hline
			& $x_{med}$ CI & $\sigma$ CI \\
			\hline
			CP & 0.956 & 0.912 \\
			\hline  
			AIL & 5.030710 & 6.661682 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau34}Confidence intervals coverage probability and average interval length: $bc_a$}
	\end{table}
	The CP values for the $bc_a$ confidence intervals are more closer to $0.95$ (especially for $\sigma CI$)than the ones of the bootstrap percentile confidence intervals(see table \ref{tableau31}). We also notice a slight difference in the AIL for both confidence intervals in both methods. The following table 
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{0.5cm}|P{2.5cm}|P{2cm}|}
			\hline
			& $x_{med}$ & $\sigma$ \\
			\hline
			$\hat{z}_0$ & $-0.04063$ & 0.1113 \\
			\hline  
			$\hat{a}$ & $-1.475\times10^{-15}$ & 0.09085 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau35}Average $\hat{z}_0$ and $\hat{a}$}
	\end{table}
	
	\end{itemize}
\newpage
    \item From the dataset \textit{shhs1.txt} has been obtained from \href{https://sleepdata.org/datasets/shhs}{Sleep Heart Health Study}, we are using the variable \textbf{rdi4p}: respiratory disturbance index. Figure \ref{fig 3.1}, we notice that the \textbf{rdi4p} is skewed on the left.
    
    \begin{figure}[ht]
    	\centering
    	\scalebox{1.2}{\input{Ex3plot.tex}}
    	\caption{Histogram of \textbf{rdi4p} with the empirical distribution}
    	\label{fig 3.1}
    \end{figure}
    
    By building bootstrap percentile and bootstrap accelerated bias-corrected confidence intervals for the standard deviation and
median, we get the following results (with $R = 1000$)
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|}
    		\hline
    		& $x_{med}$ & $\sigma$ \\
    		\hline
    		CI & [3.951, 4.419] & [11.785, 13.045] \\
    		\hline
    		CI$_{length}$ & 0.498 & 1.26 \\
    		\hline  
    	\end{tabular}
    	\caption{\label{tableau36}Results for bootstrap percentile confidence interval}
    \end{table}
    
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|P{2cm}|P{2.5cm}|P{2.5cm}|}
    		\hline
    		& $x_{med}$ & $\sigma$ \\
    		\hline
    		CI & [3.944, 4.429] & [11.792, 13.103] \\
    		\hline
    		CI$_{length}$ & 0.485 & 1.311 \\
    		\hline
    		$\hat{z}_0$ & $0.00251$ & $-0.00251$ \\
    		\hline  
    		$\hat{a}$ & 0 & 0.0272 \\
    		\hline  
    	\end{tabular}
    	\caption{\label{tableau37}Results for $bc_a$}
    \end{table}
    The median of the variable \textbf{rdi4p} is 4.193012 and its standard deviation is 12.43283
    
\end{enumerate}

%44444444444444444444444444444444444444444444%

{\color{royalblue}\chapter{Generalised linear models}}
{\color{royalblue}\section{Problem description}}
In its simplest form, a linear model specifies the (linear) relationship between a dependent variable $Y$ (normally distributed), and a set of independent variables $X_i$, $i=1,\ldots,k\in \mathbb{N}$, so that $Y= b_0 + b_1X_1 +\ldots+ b_kX_k$, where $b_0$ is the regression coefficient for the intercept and the $b_i$ values are the regression coefficients (for variables 1 through k). However, there are many relationships that cannot adequately be summarized by a simple linear equation, for two major reasons:
\begin{itemize}
	\item  \textit{Distribution of the dependent variable.} The dependent variable of interest may have a non-continuous distribution, and thus, the predicted values should also follow the respective distribution; any other predicted values are not logically possible.
	\item \textit{Link function.} The second reason why the linear model might be inadequate to describe a particular relationship is that the effect of the predictors on the dependent variable may not be linear in nature.
\end{itemize}
Generalized linear models (GLMs) extend linear models to accommodate both non-normal response distributions and transformations to linearity.

{\color{royalblue}\section{Methods}}

Let $(Y_1, X_1),\ldots,(Y_n, X_n)$ be independent pairs of observations, where $Y_i$ is real-valued
and $X_i$ are $\mathbb{R}^k$-valued random variables. Generalised linear models (GLMs) have the following
three-part specification:

\begin{itemize}
	\item \textbf{The random component} (=response from an overdispersed exponential family). The data $Y_1,\ldots,Y_n$ are such that $Y_1|X_1,\ldots, Y_n|X_n$ are independent and $Y_i|X_i$ has
the p.d.f.
	\[ f_{\eta,\psi}(y_i|x_i) = \mathtt{exp}\bigg\{   \frac{\eta_iy_i - \kappa(\eta_i)}{\psi_i} \bigg\}h(y_i,\psi_i), \hspace{0.5cm} i=1,\ldots,n, \]
	where $\eta_i$ is called canonical parameter and i is an unknown scale or dispersion parameter. Functions $\kappa$ and $h$ are known and $\kappa^{''}(\eta) > 0$ is assumed. Note that 
	\[ \mu(\eta_i):=\mathtt{E}(Y_i|X_i) = \kappa_0(\eta_i) \hspace{0.25cm}\mathtt{and}\hspace{0.25cm} var(Y_i|X_i)=\psi_i\kappa^{''}(\eta), \hspace{0.5cm} i=1,\ldots,n \]
	\item \textbf{The systematic component} (=linear predictor) Canonical parameter $\eta_i$ is assumed to be related to $X_i$. The term $X^t_i\beta$ for unknown
$\beta \in \mathbb{R}^d$ is called the \textbf{linear predictor or systematic component}.
	\item The \textbf{link function} between random and systematic components. The relationship between $\eta_i$ and $X^t_i\beta$ is described through
	\[ g\{\mu(\eta_i)\} = X^t_i\beta, \hspace{0.5cm} i=1,\ldots,n \]
	where g is called a link function. The link function g is assumed to be a known, one-to-one, third-order continuously differentiable function. If $g=\mu^{-1}$ then $\eta_i=X^t_i\beta$, and $g$ is called the \textbf{canonical or natural link function}. If $g$ is not canonical, then

	it is assumed that $d(g\circ\mu)(\eta)/d\eta \ne 0$ for all $\eta$.	
\end{itemize}

In a GLM, the parameter of interest is $\beta$. Parameters $\psi_i$ are considered to be nuisance parameters. It is often assumed that $\psi_i = \psi/t_i$, $i = 1,\ldots,n$ with an unknown
$\psi$ and known $t_i$'s or, alternatively $\psi_i =a( )$ for some known function $a$. Note that $\psi_i$
enter $var(Y_i|X_i) = \psi_i\kappa{''}(\eta_i)$, making it more flexible, that is allowing for over- or underdispersion.\\

\textbf{Exemple:} Let Let $Y_i|X_i \backsim Poi(\lambda_i)$. We can write the density
\[ f_\eta(y_i)= \mathtt{exp}\{ y_ilog(\lambda_i) - \lambda_i \}\frac{1}{y_i!}\mathbb{I}_{\{ 1,2,\ldots \}}(y_i) \]
that is, the canonical parameter $\eta_i = \mathtt{log}(\lambda_i), \kappa(\eta_i) = \lambda_i = \mathtt{exp}(\eta_i)$, $\psi_i = 1$ and $h(y_i) = (y_i)^{-1}\mathbb{I}_{\{ 1,2,\ldots \}}(y_i)g(y_i)$. Since $E(Y_i|X_i) = \kappa_0(\eta_i) = \mathtt{exp}(\eta_i) =: \mu(\eta_i)$, the canonical link is

$g(x) = \mu^{-1}(x) = \mathtt{log}(x)$, which is called the \textbf{log-link} $(g(\mu(\eta_i)) = \eta_i)$. Hence, 
\[ \mathtt{log}\{\mathtt{E}(Y_i|X_i=x_i)\} = x^t_i\beta,\]
where $x_i \in \mathbb{R}^k,\hspace{0.25cm} i=1,\ldots,n$.\\

\begin{center}
	\textbf{\large Estimation}
\end{center}
Let $\theta = (\beta, \psi)$ and $(g\circ\mu)^{-1} = \zeta$ (for a canonical link $\zeta(x) \equiv x$). Then

\[ \ell(\theta)= \sum_{i=1}^{n} \bigg[ \frac{\zeta(X^t_i\beta)Y_i-\kappa\{\zeta(X^t_i\beta)\}} {a(\psi)} + \mathtt{log}h(Y_i,\psi)  \bigg] .\]
Further, consider the canonical link. Taking derivatives w.r.t. $\beta$ and we get the following
score equations
\[  \frac{\partial \ell(\theta)}{\partial\beta}= \frac{1}{a(\psi)} \sum_{i=1}^{n} \{ Y_i - \mu(X^t_i) \}X_i=0 \]

\[ \frac{\partial \ell(\theta)}{\partial\psi}= \sum_{i=1}^{n}\bigg[ \frac{\partial\mathtt{log}h(y_i,\psi)}{\partial\psi}+ \{a^{-1}(\psi)\}^{'}\{ X^t_i\beta Y_i+ \kappa(X^t_i\beta) \} \bigg]= 0 \]

Where $ \kappa(X^t_i\beta)= \mu(X^t_i\beta)$ was used. If MLE of $\beta$ exists, then it can be found from the
first equation without estimating. Estimation of $\psi$ from the second equation in many cases is a difficult task and depends on a particular distribution. To estimate $\beta$ and study its properties we also need
\[-\frac{\partial^2\ell(\theta)}{\partial\beta\partial\beta^t}= \frac{1}{a(\psi)} \sum_{i=1}^{n}\bigg[ \kappa(X^t_i\beta)^{''}X_i X^t_i \bigg]=: -\frac{F_n(\beta)}{a(\psi)} \]
With this, we can set up the Newton-Raphson algorithm as
\[ \hat\beta^{(j+1)}= \hat\beta^{(j)} +\big\{ F_n(\hat\beta^{(j)}) \big\}^{-1} S_n(\hat\beta^{(j)}), \hspace{0.25cm} j=0,1,2,\ldots, \]
where $S_n(\hat\beta^{(j)})= a(\psi)\partial\ell(\theta)/\partial\beta.$\\

\begin{center}
	\textbf{\large Goodness-of-fit and models' comparison}
\end{center}
Now, we want to to assess how good the model fits the data,
i.e., to measure the discrepancy between the data $Y_i|X_i$ and estimated $\mathtt{E}(Y_i|X_i) = \mu_i$. First, some definitions. The \textbf{null model} is simplest model, and has only one parameter, representing a common mean $\mu$, say, for all $Y_i|X_i$. At the other extreme is the \textbf{full model}, which has $n$ parameters, one for each observation. The full model gives a baseline for measuring the discrepancy for an intermediate model with k parameters. Assume for the moment that $\psi$  is known
and denote $\ell(\hat{\mu},\psi)$ the log-likelihood with $\hat{\mu} = g^{?1}(X\hat{\beta})$. The maximum likelihood in the
full model is then $\ell(Y, \psi)$ ($=\mu_i$ are replaced by $Y_i$). Then the \textbf{deviance of the fitted	model} is defined as
\[ D(Y,\hat{\mu})= a(\psi)2\{ \ell(Y,\psi)- \ell(\hat{\mu}, \psi) \}  \]
Note that $D(Y,\hat{\mu})/a(\psi)$ is called the \textbf{scaled deviance}(or the deviance for $2\{ \ell(Y,\psi)- \ell(\hat{\mu}, \psi) \}$).
The \textbf{generalised Pearson statistic} is defined via
\[ \chi^2=  \frac{\sum_{i=1}^{n}(Y_i-\hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]. The following methods are used to measure the goodness-of-fit, and compare models:

\begin{itemize}
	\item  \textbf{Analysis of deviance:} Scaled deviance can be used to compare two nested models, i.e. the parameter space
under one model is a subspace of that under the second model. let $M_k$ and $M_q$, with $q<k$ (k and q are the number of parameters in $M_k$ and $M_q$ respectively) two nested models. Let us denote $D_{M_k}$ and $D_{M_q}$ respectively as the scaled deviance of $M_k$ and $M_q$. Since we have assume that $\psi$ is known, we have the following formula:
	\[\frac{D_{M_q}- D_{M_k}}{\psi} \overset{approx}{\sim} \chi^2_{k-q}\] 
	A widely used
rule of thumb(to measure goodness-of-fit) is that a good fit has the scaled deviance about $n ? k$, which is the expectation of a $\chi^2_{n-k}$ distributed random variable. Large values of the scaled deviance are
considered to indicate a bad fit. However, this has to be treated with care. For Poisson
data with large $\lambda_i$ and Binomial data with large $m_i$, the approximation to $\chi^2_{n-k}$ works
reasonable, but not in many other cases. Therefore we can use other methods.
	
	\item \textbf{Residual analysis:} Here, the residuals used are expected to behave approximately as zero-mean normally distributed variables. \textbf{Pearson residuals} defined via
	\[ r^p_i= \frac{Y_i-\hat{\mu}_i}{d\sqrt{V(\hat{\mu}_i)}}, \hspace{0.25cm} i=1,\dots,n \]. Pearson residuals have the disadvantage of being skewed for non-normal responses. As a remedy, we have the \textbf{ Anscombe residulas}, which in the special case of the Poisson distribution is given by \[ r^a_i= \frac{3(Y^{2/3}_i-\hat{\mu}^{2/3}_i)}{2\hat{\mu}^{1/6}_i}, \hspace{0.25cm} i=1,\dots,n \].
	\item \textbf{Deviance residuals:} based on the deviance, they are defined by \[ r^d_i= \mathtt{sign}(Y_i-\hat{\mu}_i)\sqrt{2\{  \ell_i(Y_i,\psi)- \ell_i(\hat{\mu}_i,\psi)\}}, \hspace{0.25cm} i=1,\dots,n \]
	where $\ell_i$ is the log-likelihood corresponding to the $i$-th observation, so that $\sum_{i=1}^{n}(r^d_i)^2= D(Y, \hat{\mu})$). A standardised version of the deviance (as well as Pearson)
residuals are used: \[ \frac{r^d_i}{\sqrt{a(\hat{\psi})(1-h_i)}}, \hspace{0.25cm}, i=1,\dots,n\] 
	where $h_i = H_{i,i}$ with the hat matrix $H$ taking now the form $H = W^{1/2}X(X^t W X)^{-1}X^t W^{1/2}$, where W is the weight matrix from the Fisher scoring. In an adequate model the plot
of standardised residuals against $\hat{\eta}= X\hat{\beta}$ should show \textit{no patterns}. The \textbf{null pattern} is a distribution of residuals with mean zero and constant variance.
	
	\item \textbf{Akaikeinformation criterion (AIC) and Bayes information criterion (BIC):} These criterions can be used to compare models with different subset of parameters or even to compare two different
models (e.g., with different link functions or a non-linear and with a linear model). These
two criteria are most popular examples of penalised goodness-of-fit criteria
	\[AIC(M)= -2\ell(M)+ 2|M|\]
	\[BIC(M)= -2\ell(M)+ \mathtt{log}(n)|M|,\] where $\ell(M)$ denotes the log-likelihood corresponding to a model $M$ and $|M|$ is the number of parameters in that model $M$. The models, selected with these criteria are then
	\[ \hat{M}_{AIC}= \mathtt{arg}\min_{M\in \mathcal{M}}AIC(M)  \]
	\[ \hat{M}_{BIC}= \mathtt{arg}\min_{M\in \mathcal{M}}BIC(M)  \]	
\end{itemize}
\newpage

{\color{royalblue}\section{Results}}
For the exercise, the dataset \textit{student-mat.csv} can be found on \href{https://www.kaggle.com/uciml/student-alcohol-consumption}{Kaggle}. Variables G1, G2, G3 are first, second and final
grades in mathematics. The remaining variables are explanatory variables. We would like
to identify variables that explain grades in mathematics.

\begin{enumerate}[label=(\alph*)]
	\item First of all, we need to identify the distribution of G1, G2, and G3. From the Q-Q plots with normal theoretical distribution of figure \ref{fig 4.2}, we notice too many zero points and points away on the tails. Moreover, the emperical densities plots are skewed on the left, that is way different from the bell-shape of a normal distribution. Therefore G1, G2, and G3 are not normally distributed. On the other hand, the figure \ref{fig 4.3} of the Q-Q plot with Poisson as theoretical distribution, displays points along the identity line suggesting that we might have a Poisson distribution. However, there are some zero points (especially in G1 and G2), which is a sign of under-dispersion, and over-dispersion for G1. Actually since the variables are Poisson distributed, hence their means should be equal their variances. But different results (see table \ref{tableau41}) confirm the latter assumption.
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1cm}|P{1cm}|P{1cm}|}
			\hline
			 & mean & var \\
			\hline
			G1 & 10.909 & 11.017 \\
			\hline
			G2 & 10.714 & 14.149 \\
			\hline
			G3& 10.415 & 20.989 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau41}Variances and means of G1, G2, and G3}
	\end{table} 
	
	\begin{figure}
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot1.tex}}
			\caption{Emperical densities}
			\label{fig 4.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot2.tex}}
			\caption{Q-Q plot with normal theoretical distribution}
			\label{fig 4.2}
		\end{subfigure}
		\caption{Checking normality assumption}
		\label{fig:roc_curve}
	\end{figure}
	
	\begin{figure}
		\centering
		\scalebox{0.75}{\input{Ex4plot3.tex}}
		\caption{Q-Q plot with Poisson as theoretical distribution}
		\label{fig 4.3}
	\end{figure}
	
\newpage
	
	\item A generalised linear model(Model 1) is fitted to explain G1 including all explanatory variables. With a significance level of 0.05, we notice that all covariates are not significant. Moreover, we there too many covariates that arenot significant which is a sign of a bad fitted model.
	
	\begin{figure}
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot4.tex}}
			\caption{Pearson residuals}
			\label{fig 4.4}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex4plot5.tex}}
			\caption{Anscombe residuals}
			\label{fig 4.5}
		\end{subfigure}
		\caption{Q-Q plots of residuals with normal theoretical distribution: Model 1}
		\label{fig}
	\end{figure}
	
	
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex4plot6.tex}}
		\caption{Residuals analysis: Model 1}
		\label{fig 4.6}
	\end{figure}
	
	
	
	\begin{figure}[ht]
		\centering
		\scalebox{1.2}{\input{Ex4plot7.tex}}
		\caption{Q-Q plot with Poisson as theoretical distribution}
		\label{fig 4.7}
	\end{figure}
	
	
\end{enumerate}





%55555555555555555555555555555555555555555555%

{\color{royalblue}\chapter{Survival analysis}}
{\color{royalblue}\section{Problem description}}
We want to analyze data where the outcome variable is the time until the occurrence of an event of interest. The event can be death, occurrence of a disease, marriage, divorce, etc. The time to event or survival time can be measured in days, weeks, years, etc. For example, if the event of interest is heart attack, then the survival time can be the time in years until a person develops a heart attack. subjects are usually followed over a specified time period and the focus is on the time at which the event of interest occurs. Why not use linear regression to model the survival time as a function of a set of predictor variables? First, survival times are typically positive numbers; ordinary linear regression may not be the best choice unless these times are first transformed in a way that
removes this restriction. Second, and more importantly, ordinary linear regression cannot effectively handle the censoring of observations. Why not compare proportion of events in your groups using risk/odds ratios or logistic regression? Simply because it ignores time. \\
To tackle these issues, we'll use some survival analysis.

{\color{royalblue}\section{Methods}}
Let $T$ be a non-negative random variable that represents the time to event. We assume its CDF as $F$ that has pdf $f$. Central concepts of the survival analysis are the \textit{survivor function} (the probability that a subject
will survive past time t) $S(t)=P(T>t)=1-F(t)$, the \textit{hazard function} $h(t)=\frac{f(t)}{1-F(t)}$ (loosely speaking, it is the probability density of failure at time
$t$, given survival to then), and the \textit{cumulative hazard function} (accumulated risk up to time t) $H(t)= \int_{0}^{t}h(s)ds= -log\{S(t)\}$. Thus we have $S(t)=\mathtt{exp}\{-H(t)\}$ and $f(t)= h(t)\mathtt{exp}\{-H(t)\}$.\\
\\
\textbf{Exemples:}Some common parametric distributions
\begin{enumerate}
	\item Exponential distribution: $h(t) =\lambda $ and $S(t) = \mathtt{exp}(-\lambda t)$
	\item Weibull distribution: $h(t) = \alpha\lambda^\alpha t^{\alpha-1}$ and $S(t) = \mathtt{exp}\{(-\lambda t)^\alpha\}$
\end{enumerate}

Ideally, we would have independent realisations of $T$: $t_1,\dots,t_n$. However, in practice
the failure time cannot always be observed due to various reasons. This phenomenon
is called \textit{censoring}. We have \textit{Type I censoring}, where $T$ is observed until some pre-determined time $c$. If $T<c$, we observe the value $t_i$ of $T$ , if $T>c$, we only know that $T$ survived beyond $c$. \textit{Type II censoring} (rarely used)
arises when $n$ independent variables are observed until there have been $r$ failures, so only
$0<T_{(1)}<\dots<T{(r)}$ are observed. These are all examples of \textit{right-censoring}. \textit{Left-censoring} (the time
of origin is not known) is less common.\\
\\
Under censoring one rather deals with $Y_i = \min\{T_i, C_i\}$ (the observed response), where $C_i$ denotes the censoring time for the $i$th subject. That is, a pair $(y_i; \delta_j)$ is observed, where Let $\delta$ denotes the event indicator.
\[
\delta_i = 
\begin{cases} 
0 & \text{if the event was observed } T_i \leq C_i \\
1 & \text{if the response was censored  } T_i > C_i
\end{cases}
\]
Note that $T$ and $C$ are independent.\\
\\
Let's assume that $T$ has a continuous distribution $F$ and there are $n$ data points available
$(y_1, \delta_1),\dots,(y_n, \delta_n)$, where $y_i = \min\{t_i, \delta_i\}$. Assume that $F(x) = F(x, \theta)$ is a some
parametric distribution and that censoring variables $C_i$ have CDF $G$
and pdf $g$, which are independent on $?$. The log-likelihood contribution from $y_i$ can be represent as
\[ \ell(\theta)= \sum_{i=1}^{n}[ \delta_i\mathtt{log}\{h(y_i;\theta)\}- H(y_i;\theta) ] \]

For exponential distribution, have 
\[ \ell(\theta)= \sum_{i=1}^{n} (\delta_ilog(\lambda)-\lambda y_i)= \log(\lambda)\sum_{i=1}^{n}\delta_i- \lambda \sum_{i=1}^{n}y_i \]
implying \[\hat{\lambda}_{ML}= \frac{\sum_{i=1}^{n}\delta_i}{\sum_{i=1}^{n}y_i}. \]
An
approximate confidence interval for $\lambda$ (using asymptotic normality of maximum likelihood
estimators) as
\[ [\hat{\lambda}(1- z_{\alpha/2}/\sqrt{r}) ,\hat{\lambda}(1+ z_{\alpha/2}/\sqrt{r} ], \hspace{0.25cm} r= \sum_{i=1}^{n}\delta_i .\]

A commonly used parametric distribution for modelling lifetimes
with monotone hazard is the Weibull distribution. Values for $\lambda$ and $\alpha$ can be estimated
by the maximum likelihood similarly to the exponential distribution, however, this has to
be done numerically.
\newpage

\textbf{Nonparametric estimators}\\
Often it is unclear which parametric model would be appropriate for the data (if any). A
standard tool for initial data inspection, for suggesting plausible models and for checking
their fit is a nonparametric estimator of the survivor function. For no censored
observations, we could estimate $\hat{S}(t)= n^{-1}\sum_{i=1}^{n}\mathbb{I}(T_i >t)$. For censored observation, let $0\leq \tau_1< \tau_n< \dots$ be the ordered uncensored failure times. Let $r_i$ denote the number
of units that are still in risk at $\tau_i$ (=not failed yet or censored) and $d_i$ the number of
units that fail at $\tau_i$. The \textbf{Kaplan-Meier estimator} for the survivor function $S$ is given by
\[ \hat{S}_{KM}(t)= \underset{ \{j:\tau_j<t\} }{\prod} \Big(1- \frac{d_j}{r_j} \Big) \]
A further estimator for $S$ is the \textbf{Fleming-Harrington estimator} $\hat{S}_{FH}(t)$. It is a plug in
estimator defined by
\[ \hat{S}(t)= \mathtt{exp}\{- \hat{H}(t) \}, \]
where $\hat{H}(t)= \underset{ \{j:\tau_j<t\} }{\sum} \frac{d_j}{r_j}$, is the \textit{Nelson-Aalen} estimator for $H$.\\
\\
\textbf{Confidence bands}\\
Assume that $\hat{S}$ is an estimator for $S$ (e.g. the Kaplan-Meier or the Fleming-Harrington estimator) and let $\hat{\mathtt{var}}(\mathtt{log}(\hat{S}))$ be some estimate for the variance of log(S). An approximate confidence band
(contained in $[0, 1]$) is given by 
\[ [\mathtt{exp}(-\mathtt{exp}(B^-), \mathtt{exp}(-\mathtt{exp}(B^+)]  ,\]
where
$B^\pm = \mathtt{log}(-\mathtt{log}\hat{S}(t))\pm z_{\alpha/2}\mathtt{log}^{-1} \hat{S}(t) \sqrt{\hat{\mathtt{var}}(\mathtt{log}(\hat{S}))}$\\
\\
\textbf{Log-rank test}\\
We wish to decide whether or not two (or more) samples stem from the same survivor
function or not. Assume that the failure times $\tau_1<\dots< \tau_k$ are realizations of two random variables $T_1$ and
$T_2$ corresponding to two groups of items (patients). For each observed failure time $\tau_j$ we
consider the contingency table
\begin{table}[H]
	\centering
	\begin{tabular}{c c c}
		\hline
		Groups & failure at time $\tau_j$ & items at risk at time $\tau_j$ \\
		\hline
		1 & $d_{1j}$ & $r_{1j}$ \\
		2 & $d_{2j}$ & $r_{2j}$ \\
		\hline
		$1 + 2$ & $d_j$ & $r_j$ \\ 
	\end{tabular}
\end{table}

Under the null-hypothesis that $T_1 = T_2$ the expected number of failures at time $\tau_j$ in
group 1 and 2 are hypergeometrically distributed with parameters $r_j, r_{1j}, d_j$ and $r_j, r_{2j}, d_{j}$
respectively. Thus, mean and variance of the number of failures in group 1 and 2 can be
computed as
\[ e_{1j}=\frac{d_j}{r_j}r_{1j} \hspace{0.25cm} \mathtt{and} \hspace{0.25cm} e_{2j}=\frac{d_j}{r_j}r_{2j} \] and
\[ v_{1j}= v_{2j}= \frac{d_j r_{1j} r_{2j}(r_j-d_j)}{r^2_j(r_j- 1)} \]
Under the null-hypothesis, the statistic
\[ \chi^2 = \frac{  \Big[ \sum_{j=1}^{k} (d_{1j}-e_{1j})\Big]^2 }{\sum_{j=1}^{k}v_{1j}} \]
is $\chi^2$-distributed with 1 degree of freedom.\\

\textbf{Graphical tool to check if the Weibull model is adequate}\\
Under the assumption that $T$ is Weibull distributed, one has
\[ \mathtt{log}\{-\mathtt{log}(S(t))\}= \alpha\mathtt{log}(t)+ \mathtt{log}(\lambda), \hspace{0.25cm}, t>0. \]
Now let $\hat{S}(t)$ be a nonparametric estimate for $S$ (e.g. the Kaplan-Meier estimator $\hat{S}_{KM}$).Then the plot $\mathtt{log}\{-\mathtt{log}(\hat{S}(t))\}$ against $\mathtt{log}(t)$ should approximately be a straight line with
slope $\alpha$ and intercept $-\mathtt{log}(\lambda)$.

{\color{royalblue}\section{Results}}







%66666666666666666666666666666666666666666666%

{\color{royalblue}\chapter{Kernel density estimation}}
{\color{royalblue}\section{Problem description}}

Consider observations which are realizations of univariate random variables,
$X_1, \ldots, X_n \sim F$ where $F$ denotes an unknown cumulative distribution function. The goal is to estimate the distribution $F$. In particular, we are interested in estimating the density $f = F^{'}$, assuming that it exists.\\
Instead of assuming a parametric model for the distribution (e.g. Normal distribution with unknown expectation and variance), we rather want to be "as general as possible": that is, we only assume that the density exists and is suitably smooth (e.g. differentiable). It is then possible to estimate the unknown density function $f(\cdot)$.

{\color{royalblue}\section{Methods}}

\textbf{Definition}\\
\\
Let $X_1,\dots,X_n \overset{iid}{\sim} F$ with a given density $F^{'} = f$. A \textbf{kernel density estimator} for $f$
is defined via
\[ \hat{f}(x;h)= \sum_{i=1}^{n}K\Big( \frac{x-X_i}{h} \Big), \hspace{0.25cm} x\in \mathbb{R}, \hspace{0.25cm} h>0.\]
Thereby $K :\mathbb{R} \rightarrow \mathbb{R}$, such that $\int_{-\infty}^{\infty} K(x)dx=1$ is known as \textbf{kernel} and $h > 0$ is called
 \textbf{bandwidth}.
\\
Some classical kernels:
\begin{enumerate}
	\item 0.5$\mathbb{I}(|x|\leq 1)$ (the rectangular or uniform kernel)
	\item $(1-|x|)\mathbb{I}(|x|\leq 1)$ (the triangular kernel)
	\item $0.75(1-x^2)\mathbb{I}(|x|\leq 1)$ (the Epanechnikov kernel)
	\item $2^{-1/2}\mathtt{exp}(-x^2 /2)$ (the Gaussian kernel)
\end{enumerate}
Now we want to find a practical way of choosing $K$ and $h$. The optimal bandwidth is given by $h_{CV}= \mathtt{arg}\min_{h>0} CV(h)$, where $CV(.)$ is the \textbf{(leave-one-out) cross-validation criterion}.  
\[ CV(h)= \int\{ \hat{f}(x;h) \}^2 dx -2\frac{1}{n(n-1)h}\sum_{i=1}^{n}\sum_{j\ne i}K(\frac{X_j -X_i}{h}).\]

Then, the cross-validation kernel density
estimator is define via
\[ \hat{f}(x;h_{CV})= \frac{1}{nh_{CV}}\sum_{i=1}^{n}K(\frac{X_i -x}{h_{CV}}) \]

\newpage
{\color{royalblue}\section{Results}}
The dataset used for the exercise is \textit{StudentsPerformace.csv} and can be found on \href{https://www.kaggle.com/spscientist/students-performance-in-exams}{Kaggle datasets}. In our analysis we will only consider the
following variables:\\
$\mathtt{test.preparation.course}$: If a student took part at the preparation course\\
$\mathtt{math.score}$: Score on the math exam (0-100)\\
$\mathtt{reading.score}$: Score on the reading exam (0-100)\\
$\mathtt{ writing.score}$: Score on the writing exam (0-100)\\

\begin{enumerate}[label=(\alph*)]
	\item After the implement of the kernel density estimation in an \textbf{R} function, we have the following plot \ref{fig 6.1}. Since we want to avoid under- or oversmoothing, the ideal bandwidth would be 8. This Bandwidth is then used to plot \ref{fig 6.2} with four different kernels. We can notice that the Epanechnikov kernel function would be ideal for the kernel density estimation because its curve is not too smooth or too rough. However, the kernels curves are pretty close, which is not the case for the bandwidths. Hence, the choice of the kernel does not matter very much as the choice of the bandwidth.
	
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot1.tex}}
			\caption{ with the Epanechnikov kernel and 4 different bandwidths}
			\label{fig 6.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot2.tex}}
			\caption{ with four kernel functions: bandwidth = 8 }
			\label{fig 6.2}
		\end{subfigure}
	    \caption{Plots of kernel density estimators of math.score}
		\label{figg}
	\end{figure}
	
	\item After the implemention of the cross-validation (CV) criterion to find the optimal bandwidth, we use it to find the optimal bandwidth in density for all three scores math.score, reading.score and
writing.score. The same is done with \textbf{R} built-in functions $\mathtt{bw.ucv}$, and $\mathtt{bw.bcv}$, then we have the table \ref{tableau61}. The cross-validation criterion returns the highest optimal bandwidth for all three scores.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.9cm}|P{1cm}|P{1cm}|P{1cm}|}
			\hline
			& CV & $\mathtt{bw.ucv}$ & $\mathtt{bw.bcv}$ \\
			\hline
			math.score & 5.506 & 4.644 & 4.257 \\
			\hline
			reading.score & 4.465 & 3.756 & 4.393 \\
			\hline
			writing.score & 5.489 & 4.265 & 4.175 \\
			\hline  
		\end{tabular}
		\caption{\label{tableau61}Optimal bandwidth for each samples with different methods}
	\end{table} 
	
	%the following plots are from the next question, but I want them on this page
	\item 
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot3.tex}}
			\caption{}
			\label{fig 6.3}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex6plot4.tex}}
			\caption{}
			\label{fig 6.4}
		\end{subfigure}
	
	    \begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
	    	\centering
	    	\scalebox{0.75}{\input{Ex6plot5.tex}}
	    	\caption{}
	    	\label{fig 6.5}
	    \end{subfigure}
		\caption{Densities plots of all three scores of the students that did not take part in the preparation course with the students who attended the preparation course}
		\label{figgg}
	\end{figure}
	
	From figure \ref{figgg}, we notice that the densities are skewed to the left. We also notice that they have the same shape, with different modes. This means that the two groups may have same distribution with different parameters. 	
	
	
	
	
	
\end{enumerate}







%77777777777777777777777777777777777777777777%

{\color{royalblue}\chapter{Nonparametric regression: local polynomials}}
{\color{royalblue}\section{Problem description}}

To study the relation between a dependent variable $Y$ and an independent variable $X$, the common method used is linear regression. When appropriate, this method is very useful as its suppose a simple model of the form 
\begin{equation}
Y = \beta_{0} + \beta_{i}x_{i} + \epsilon_{i}
\end{equation}
This is advantageous since it is easy to interpret and to calculate. Moreover, when the assumptions on the residues $\epsilon_{i}$  are verified, we can run some tests on the parameters. \\
However, the restricted assumption of linearity is frequently not fulfilled, eventually when the data set is very large. In that case, we would like to find a complex model that will better highlight the relation between $Y$ and $X$. A first approach for this aim would be to specify another parametric form for this relation, for example a transformation of the observations or a polynomial regression. Nonetheless it remains difficult to find the suitable relation since the form of the data does not really change after these transformations. That is why in this section, we opt for a non-parametric regression technique (local polynomials) in which data choose their own form of relation (the predictor does not take a predetermined form but is constructed according to information derived from the data) making things more flexible.

{\color{royalblue}\section{Methods}}
Let $(Y_1, X_1),\dots,(Y_n, X_n)$ be iid as $(Y, X)$ random variables, $Y \in \mathbb{R}$ and $X \in \mathbb{R}^d$.
Consider a random design
nonparametric regression model
\[ Y_i= f(X_i)+ \epsilon_{i}, \hspace{0.25cm} i=1,\dots,n \]
\[ \mathtt{E}(\epsilon_{i}|X_i)= 0, \hspace{0.25cm}  \mathtt{E}(\epsilon_{i}^2|X_i)= \sigma^2 .\]
Let$K:\mathbb{R}^d \rightarrow \mathbb{R}_{+}$ be a kernel function, and denote $e_k = (0,\dots,0,1,0,\dots,0) \in \mathbb{R}^(\ell+1) $ a unit vector with $1$ at $k$-th position, $k =
1,\dots,\ell+1$. Moreover, we define
\[ P(X_i-x)= \{1,(X_i-x),\dots,(X_i-x)^\ell  \}^t \]

\[X = 
\begin{pmatrix}
1 & (X_1-x) & \cdots & (X_1-x)^\ell \\
\vdots  & \vdots  & \ddots & \vdots  \\
1 & (X_n-x) & \cdots & (X_1-x)^\ell 
\end{pmatrix}, \hspace{0.25cm} 
Y=
\begin{pmatrix}
Y_1 \\
\vdots  \\
Y_n 
\end{pmatrix} \]

\[V=\mathtt{diag}\bigg\{K\bigg(\frac{X_1-x}{h}\bigg),\dots,K\bigg(\frac{X_n-x}{h}\bigg) \bigg\} \]
Then a \textbf{local polynomial estimator} of $f^{(k-1)}(x)$ is a linear estimator
\[ \hat{f}^(k-1)(x)=(k-1)!e^t_k \bigg( \frac{1}{nh}X^tVX \bigg)^{-1}P(X_i-x)
K\bigg(\frac{X_i-x}{h}\bigg)=\sum_{i=1}^{n}W_{k,i}(x)Y_i \]
with the weight function
\[ W_{k,i}(x)= \frac{(k-1)!}{nh}e^t_k\bigg( \frac{1}{nh}X^tVX \bigg)^{-1} 
P(X_i -x)K\bigg( \frac{X_i -x}{h} \bigg) .\]
The bandwiddth $h$ can be chosen efficiently with the following GCV(generalized cross-validation)
\[GCV(h)= \frac{\sum_{i=1}^{n} \Big\{ Y_i- \hat{f}(X_i;h) \Big\}^2}{\{ 1-n^{-1}\sum_{i=1}^{n}W_{k,i}(h) \}^2} \]

\newpage
{\color{royalblue}\section{Results}}
We use the dataset from Exercise 1 on Kenyan children. We are interested in the

following two variables\\
\textbf{hypage}: Age of a child
\\
\textbf{zwast}: Z-score for wasting\\
\\
Z-score for wasting is defined as the weight of a child standardised with the median and
standard deviation of children with the same height from the healthy population. We would
like to investigate how the Z-score for wasting changes with age, that is we consider the
model 
\[\mathtt{ zwast}_i= f(\mathtt{hypage}_i)+ \epsilon_{i}, \hspace{0.25cm}  \mathtt{for} \hspace{0.25cm}
\epsilon_{i} \sim \mathcal{N}(0,\sigma^{2}), \hspace{0.25cm}  i=1,\dots,n. \]

\begin{enumerate}[label=(\alph*)]
	\item After implementing a local polynomial fit and by fixing the polynomial degree to 1, we have the plots in figure \ref{figggg}. Since we want to avoid under- or oversmoothing, the ideal bandwidth from the plot \ref{fig 7.1} would be 8. This bandwidth is then used to plot the estimate of $f$ with 4 different bandwidths (figure \ref{fig 7.2}). It is clear that the curves of the estimator present the same shape, and almost the same level of smoothness. In this case, all the kernels might be used for further analysis.
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex7plot1.tex}}
			\caption{ with the Epanechnikov kernel and 4 different bandwidths}
			\label{fig 7.1}
		\end{subfigure}
		\begin{subfigure}[b]{0.48\linewidth}        %% or \columnwidth
			\centering
			\scalebox{0.75}{\input{Ex7plot2.tex}}
			\caption{ with four kernel functions: bandwidth = 8 }
			\label{fig 7.2}
		\end{subfigure}
		\caption{Plots of local polynomial estimator of $f$ }
		\label{figggg}
	\end{figure}
	
	 \newpage
	 \item Now we want to find the optimal bandwidth with Generalised Cross Validation (GCV). For this aim, we implement a function that calculates the GCV, then with used Epanechnikov kernel and obtained GCV-bandwidth to estimate f using
polynomial degrees from 1 to 4. The results are in table \ref{tableau71}. We note that the GCV-bandwidths are less than 8, and are different. This means that we will have different estimators for different polynomial degrees.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|P{1.9cm}|P{1.25cm}|P{1.25cm}|P{1.25cm}|P{1.25cm}|}
			\hline
			Polynomial degree & 1 & 2 & 3 & 4 \\
			\hline
			GCV-bandwidth & 4.999956 & 10.99995 & 10.99994 & 8.403991 \\
			\hline 
		\end{tabular}
		\caption{\label{tableau71}Optimal bandwidth for each polynomial degree}
	\end{table} 
	
	The plot of all four fits are in figure \ref{fig 7.3}. We notice that the curves follow the same pattern in general, with less smoothness meaning that the obtained GCV-bandwidths are completely reasonable. However, the fitting curve for the polynomial degree 1 is very smooth compared to the others, especially with the fitting curve of the polynomial degree which is less smooth than the others. Moreover, the boundaries highlight slight signs of over-fitting(for polynomial degree 4), or under-fitting (for polynomial degree 2). Overall, using the polynomial degree 3 with the corresponding optimal bandwidth would be ideal. 
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex7plot3.tex}}
		\caption{Plot of all four fits obtained using the GCV-bandwidths from table \ref{tableau71}}
		\label{fig 7.3}
	\end{figure}
	
	\newpage
	\item In this question, we use the function \textbf{localpoly.reg} of library \textit{NonpModelCheck} to calculate the first derivative of the function of $\mathtt{ zwast}$ with the GCV-bandwidth and polynomial degrees from 1 to 4. Figure \ref{fig 7.4} shows the plot of all four derivative fits. We can see noticeable differences on boundaries, since the curves  start at very different $\mathtt{ zwast}$ values. This suggest that some derivative fits(especailly the one from the polynomial degree 4) takes into account a lot of outliers, while others might use them less. Nevertheless, we do have signs of better $\mathtt{ zwast}$ after 2 years. Actually, after 2 years, the derivative fitting curves head towards 0 (and we also have some fluctuations around 0). Finally, the derivative fits curves we got here are too smooth, therefore GCV-bandwidths from \ref{tableau71} are not reasonable or not optimal. We may solve this issue just by using the corresponding derivatives for each polynomial degree to find the optimal bandwidths.
	
	
	\begin{figure}[H]
		\centering
		\scalebox{1}{\input{Ex7plot4.tex}}
		\caption{Plot of all four derivative fits obtained using the GCV-bandwidths from table \ref{tableau71}}
		\label{fig 7.4}
	\end{figure}
	
	
	
\end{enumerate}


%88888888888888888888888888888888888888888888%

{\color{royalblue}\chapter{Nonparametric regression: splines}}
{\color{royalblue}\section{Problem description}}

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%99999999999999999999999999999999999999999999%

{\color{royalblue}\chapter{Mixed models}}
{\color{royalblue}\section{Problem description}}
To illustrate the targeted problem ib this section, we use the following example. Let us consider the following linear model,\\
\begin{equation}
Y_{i,t} = \beta_{0} + \beta_{i}t + \epsilon_{i,t}
\end{equation}
Here, $\beta_{0}$ and $\beta_{i}$
 

{\color{royalblue}\section{Methods}}
	
{\color{royalblue}\section{Results}}

%10101010101010101010101010101010101010101010%

{\color{royalblue}\chapter{Partial least squares}}
{\color{royalblue}\section{Problem description}}
In some problems of linear regression or prediction phenomenon, explicable variables can be correlated, sometimes causing  multicollinearity. This phenomenon that occurs most of the time in big data analysis, is a consequence of bad results related to regressions' coefficients estimated with least squares. To solve this problem, we have principal component analysis (PCA), and the partial least squares (PLS). We are using the latter.

{\color{royalblue}\section{Methods}}

{\color{royalblue}\section{Results}}


\end{document}