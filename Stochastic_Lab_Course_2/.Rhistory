#Simulation of a binomial random variable with rbinom
bin_rbin <- rbinom(N, n, p)
#Plot the empirical pdf of all three samples on one panel
bin_inv <- data.frame(bin_inv)
bin_inv$method <- rep("inverse CDF", 1000)
colnames(bin_inv) <- c("rand_num", "method")
bin_bern <- data.frame(bin_bern)
bin_bern$method <- rep("From Bernoulli", 1000)
colnames(bin_bern) <- c("rand_num", "method")
bin_rbin <- data.frame(bin_rbin)
bin_rbin$method <- rep("rbinom", 1000)
colnames(bin_rbin) <- c("rand_num", "method")
df <- rbind(bin_inv, bin_bern, bin_rbin)
ggplot(df) +
geom_density(aes(x = rand_num, fill = method), alpha = 0.5) +
labs(fill = "Simulation \n method", title = "empirical probability density functions of all three samples", x = "random numbers")
install.packages("survival")
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("proxy", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
detach("package:proxy", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("survival", lib.loc="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
remove.packages("survival", lib="/usr/lib/R/library")
remove.packages("survival", lib="/usr/lib/R/library")
.libPaths(proxy)
.libPaths("proxy")
.libPaths("ggplot2")
?.libPaths()
library("tidyverse")
library("ggplot2", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
library("survival", lib.loc="/usr/lib/R/library")
library("dplyr", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
library("tidyverse", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
detach("package:tidyverse", unload=TRUE)
library("tidyverse", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
remove.packages("tidyverse", lib="~/R/x86_64-pc-linux-gnu-library/3.5")
remove.packages("survival", lib="/usr/lib/R/library")
remove.packages("survival", lib="/usr/lib/R/library")
remove.packages("survival", lib="/usr/lib/R/library")
remove.packages("survival", lib="/usr/lib/R/library")
install.packages("survival")
remove.packages("lattice", lib="/usr/lib/R/library")
remove.packages("lattice", lib="/usr/lib/R/library")
remove.packages("lattice", lib="/usr/lib/R/library")
install.packages("lattice")
library("lattice", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
remove.packages("lattice", lib="/usr/lib/R/library")
install.packages("survival")
library("survival", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
remove.packages("survival", lib="/usr/lib/R/library")
detach("package:survival", unload=TRUE)
library("tidyverse")
library("survival")
install.packages("tydiverse")
install.packages("tidyverse")
library("nlme", lib.loc="/usr/lib/R/library")
remove.packages("nlme", lib="/usr/lib/R/library")
remove.packages("nlme", lib="/usr/lib/R/library")
setwd("~/Stochastic_Lab/Stochastic_Lab_Course_2")
library("tidyverse")
library("haven")
library(maptools)
library(raster)
library(rgdal)
##Question(a)
children <- read_dta("childrenfinal.dta")
#remove all variables that start with “s“, “v” and “m”, followed by a number
children1<- children %>%
dplyr::select(-matches("^[svm][0-9]"))
#convert labelled double into double variables
children2<- children1 %>%
mutate_if(is.double, as.double)
##Question(b)
#(b) Make a smaller tibble that contains variables hypage, ruralfacto, female, zstunt, zweight, zwast, adm2.
children3<- children2 %>%
dplyr::select(c(hypage, ruralfacto, female, zstunt, zweight, zwast, adm2))
#Make a scatter plot of zstunt against hypage
ggplot(children3, aes(x = zstunt, y = hypage)) +
geom_point() +
ggtitle("scatter plot of zstunt against hypage")
#Add a smooth line to the plot
ggplot(children3, aes(x = zstunt, y = hypage)) +
geom_point() +
geom_smooth(se = F) +
ggtitle("scatter plot of zstunt against hypage \n with smooth line")
#smooth plots of zstunt against age for females and males on one plot
ggplot(children3, aes(x = zstunt, y = hypage,  colour = factor(female))) +
geom_point() +
geom_smooth(se = F) +
scale_colour_manual(labels = c("male", "female"), values = c("blue2", "green3")) +
guides(colour = guide_legend(title="Gender")) +
ggtitle("scatter plot of zstunt against hypage \n for females and males")
#plot zstunt against age for urban and rural children
ggplot(children3, aes(x = zstunt, y = hypage,  colour = factor(ruralfacto))) +
geom_point() +
geom_smooth(se = F) +
scale_colour_manual(labels = c("urban", "rural"), values = c("blue2", "green3")) +
guides(colour = guide_legend(title="Area")) +
ggtitle("zstunt against age for urban and rural children")
#Experiment with different aesthetics, themes and font sizes for the plots, report your favourite
##Question(c)
#help link https://rpubs.com/spoonerf/countrymapggplot2
Kenya1<-getData("GADM", country="KE", level=1) #Download Kenya shapefile data
#WARNING for linux users: Do not capitalize epsg in the following code to avoid errors.
Kenya1_UTM<-spTransform(Kenya1, CRS("+init=epsg:32537")) # setting an appropriate projection
colnames(children3)[7] <- "NAME_1" #rename adm2 into NAME_1
#sort in alphabetic order with respect to NAME_1
children3<- children3[order(children3$NAME_1),]
Kenya1_UTM@data<- Kenya1_UTM@data[order(Kenya1_UTM@data$NAME_1),]
#summarising children3 data by the mean of zstunt in the corresponding county
children4 <- children3 %>%
group_by(NAME_1) %>%
summarise(mean = mean(zstunt), n = n())
#Adding the missing county Isiolo
children4[nrow(children4) + 1,] <- NA
children4$NAME_1[47] <- "Isiolo"
#Prepare the dataframe for ggplot
Kenya1_UTM@data$id <- rownames(Kenya1_UTM@data)
Kenya1_UTM@data <- mutate(Kenya1_UTM@data, zstunt.mean= children4$mean)
Kenya1_df <- fortify(Kenya1_UTM)
Kenya1_df <- full_join(Kenya1_df,Kenya1_UTM@data, by="id")
#In order to add names to map, we need another dataframe with all the conunties' centroids
# "coordinates" extracts centroids of the polygons, in the order listed at Kenya1_UTM@data
centroids_df <- as.data.frame(coordinates(Kenya1_UTM))
names(centroids_df) <- c("long", "lat")
children4<- children4[order(children4$NAME_1),]
centroids_df$NAME_1 <- Kenya1_UTM@data$NAME_1
centroids_df$zstunt.mean <- children4$mean
#Generating the map
ggplot(data = Kenya1_df, aes(x = long, y = lat, group = group, fill = zstunt.mean)) +
geom_polygon(color = "black", size = 0.25) +
geom_text(data = centroids_df, aes(x = long, y = lat, label = NAME_1, group = NULL), size = 3) +
scale_fill_distiller(name="Zstunt mean for \n each county", palette = "Spectral") +
theme(aspect.ratio=1)
##(d)write the tibble from (b) into a text file
write.table(children3,"children3.txt")
set.seed(250)
library("tidyverse")
##Question (a)
#Switch the default random number generator in R to Wichmann-Hill
RNGkind(kind = "Wichmann-Hill", normal.kind = NULL)
#Inversion method to simulate a binomial random variable
N = 1000
n = 10
p = 0.4
u <- runif(N)
bins <- .bincode(u, breaks = c(0, pbinom(0:10, 10, 0.4)), right = F, include.lowest = T)
bin_inv <- numeric()
for(i in 1:N){
bin_inv[i] <- bins[i]-1
}
#Simulation of a binomial random variable by simulating corresponding Bernoulli random variables by inversion method
bin_bern <- numeric()
for (i in 1:N){
v <- runif(n)
bin_bern[i] <- sum(v < p)
}
#Simulation of a binomial random variable with rbinom
bin_rbin <- rbinom(N, n, p)
#Plot the histograms of all three samples on one panel
bin_inv <- data.frame(bin_inv)
bin_inv$method <- rep("inverse CDF", 1000)
colnames(bin_inv) <- c("rand_num", "method")
bin_bern <- data.frame(bin_bern)
bin_bern$method <- rep("From Bernoulli", 1000)
colnames(bin_bern) <- c("rand_num", "method")
bin_rbin <- data.frame(bin_rbin)
bin_rbin$method <- rep("rbinom", 1000)
colnames(bin_rbin) <- c("rand_num", "method")
df <- rbind(bin_inv, bin_bern, bin_rbin)
ggplot(df, aes(x = rand_num, fill = method)) +
geom_histogram( binwidth=.5, position="dodge") +
labs(fill = "Simulation \n method", title = "Histograms of all three samples", x = "random numbers")
#Switch the random number generator back to its default
RNGkind(kind = "default", normal.kind = NULL)
##Question(b)
f <- function(x){
((2*pi)^(-1/2))*exp(-(x^2)/2)
}
g <- function(x){
(pi*(1 + x^2))^(-1)
}
#First determine the best value of the constant c, such that f(x) <= g(x)
x <- -1500:1500
c <- max(f(x)/g(x))
#10000 standard normal random variables using
#the accept-reject method, generating Cauchy distributed random variables using inversion method
N <- 10000
j <- 0
rand_num <- numeric()
while(length(rand_num) != N){
w <- runif(1) #step 1
cauchy <- tan((w-(1/2))*pi) #still step 1
U <- runif(1) #step 2
if(U*c*g(cauchy) <= f(cauchy)){
rand_num[j] <- cauchy #step 3
j <- j + 1
}
}
#Histogram of the obtained sample with the standard normal density
k <- rnorm(N)
df <- data.frame(rand_num, k)
ggplot(df) +
geom_histogram(aes( x = rand_num, y = ..density.., colour = rand_num), colour ="white") +
geom_density(aes(x = k), colour = "blue") +
ggtitle("Histogram of the obtained sample and plot the standard normal density")
#QQ-plot
ggplot(data = df, mapping = aes(sample = rand_num)) +
stat_qq()
#it is not possible to simulate from the standard Cauchy density using
#the accept-reject method, the standard normal candidate density
#because max(g(x)/f(x)) == Inf
setwd("~/Stochastic_Lab/Stochastic_Lab_Course_2")
set.seed(1)
library("tidyverse")
library("bootstrap")
n <- 100 #the sample size
R <- 1000 #the number of bootstrap replications
M <- 1000 #the number of Monte Carlo samples
#Question(a)
#Simulate a sample with size n from the Weibull distribution with the scale parameter lamda = 13 and shape parameter k = 1
xmed <- 9.010913 # calculated with the formula of xmed
sigma <- 13 # calculated with the formula of sigma
l <- 13
k <- 1
#Building a two-sided bootstrap percentile confidence intervals for s = sigma and x_med at the significance level a = 0:95
#We use M Monte Carlo samples to estimate the coverage probability of both confidence intervals.
is_median <- 0
is_sd <- 0
alpha <- 0.95
CI_MED_left <- 0
CI_MED_right <- 0
CI_SD_left <- 0
CI_SD_right <- 0
for (j in 1:M) {
sample_weibull <- rweibull(n, k, l)
MED <- 0
SD <- 0
for (i in 1:R) {
myBootstrap <- sample(sample_weibull, n, replace = T)
MED[i] <- median(myBootstrap)
SD[i] <- sd(myBootstrap)
}
MED <- sort(MED)
SD <- sort(SD)
CI_MED_left[j] <- MED[floor(R*(1-alpha))/2]
CI_MED_right[j] <- MED[floor(R*(1-(1-alpha)/2))]
CI_SD_left[j] <- SD[floor(R*(1-alpha))/2]
CI_SD_right[j] <- SD[floor(R*(1-(1-alpha)/2))]
df <- data.frame(CI_MED_left, CI_MED_right, CI_SD_left, CI_SD_right)
}
df <- df %>%
mutate(is_median = xmed >= CI_MED_left & xmed <= CI_MED_right)
df <- df %>%
mutate(is_sd = sigma >= CI_SD_left & sigma <= CI_SD_right)
#estimation of the coverage probability of both confidence intervals
First_coverage_prob <- c(sum(df$is_median)/M, sum(df$is_sd)/M)
#Estimation of the average interval length
First_average_interval_length <- c(sum(df$CI_MED_right - df$CI_MED_left)/nrow(df), sum(df$CI_SD_right - df$CI_SD_left)/nrow(df))
#####if n = R = 1000
n <- 1000
R <- 1000
is_median <- 0
is_sd <- 0
alpha <- 0.95
CI_MED_left <- 0
CI_MED_right <- 0
CI_SD_left <- 0
CI_SD_right <- 0
for (j in 1:M) {
sample_weibull <- rweibull(n, k, l)
MED <- 0
SD <- 0
for (i in 1:R) {
myBootstrap <- sample(sample_weibull, n, replace = T)
MED[i] <- median(myBootstrap)
SD[i] <- sd(myBootstrap)
}
MED <- sort(MED)
SD <- sort(SD)
CI_MED_left[j] <- MED[floor(R*(1-alpha))/2]
CI_MED_right[j] <- MED[floor(R*(1-(1-alpha)/2))]
CI_SD_left[j] <- SD[floor(R*(1-alpha))/2]
CI_SD_right[j] <- SD[floor(R*(1-(1-alpha)/2))]
df <- data.frame(CI_MED_left, CI_MED_right, CI_SD_left, CI_SD_right)
}
df <- df %>%
mutate(is_median = xmed >= CI_MED_left & xmed <= CI_MED_right)
df <- df %>%
mutate(is_sd = sigma >= CI_SD_left & sigma <= CI_SD_right)
#estimation of the coverage probability of both confidence intervals
Second_coverage_prob <- c(sum(df$is_median)/M, sum(df$is_sd)/M)
#Estimation of the average interval length
Second_average_interval_length <- c(sum(df$CI_MED_right - df$CI_MED_left)/nrow(df), sum(df$CI_SD_right - df$CI_SD_left)/nrow(df))
#####if n = 100, R = 5000
n <- 100
R <- 5000
is_median <- 0
is_sd <- 0
alpha <- 0.95
CI_MED_left <- 0
CI_MED_right <- 0
CI_SD_left <- 0
CI_SD_right <- 0
for (j in 1:M) {
sample_weibull <- rweibull(n, k, l)
MED <- 0
SD <- 0
for (i in 1:R) {
myBootstrap <- sample(sample_weibull, n, replace = T)
MED[i] <- median(myBootstrap)
SD[i] <- sd(myBootstrap)
}
MED <- sort(MED)
SD <- sort(SD)
CI_MED_left[j] <- MED[floor(R*(1-alpha))/2]
CI_MED_right[j] <- MED[floor(R*(1-(1-alpha)/2))]
CI_SD_left[j] <- SD[floor(R*(1-alpha))/2]
CI_SD_right[j] <- SD[floor(R*(1-(1-alpha)/2))]
df <- data.frame(CI_MED_left, CI_MED_right, CI_SD_left, CI_SD_right)
}
df <- df %>%
mutate(is_median = xmed >= CI_MED_left & xmed <= CI_MED_right)
df <- df %>%
mutate(is_sd = sigma >= CI_SD_left & sigma <= CI_SD_right)
#estimation of the coverage probability of both confidence intervals
Third_coverage_prob <- c(sum(df$is_median)/M, sum(df$is_sd)/M)
#Estimation of the average interval length
Third_average_interval_length <- c(sum(df$CI_MED_right - df$CI_MED_left)/nrow(df), sum(df$CI_SD_right - df$CI_SD_left)/nrow(df))
####bootstrap accelerated
####with M Monte Carlo samples to assess the coverage probability and the average length
#First whith the median
CI_MED_left <- rep(0, M)
CI_MED_right <- rep(0, M)
z_MED <- 0
a.0_MED <- 0
for (j in 1:M) {
sample_weibull <- rweibull(100, k, l)
A <- bcanon(sample_weibull, R, theta = median, alpha = c(0.025, 0.975))
z_MED[j] <- A$z0
a.0_MED[j] <- A$acc
CI_MED_left[j] <- A$confpoints[1,2]
CI_MED_right[j] <- A$confpoints[2,2]
}
df_bca_point_MED <- data.frame(CI_MED_left, CI_MED_right)
df_bca_point_MED <- df_bca_point_MED %>%
mutate(is_median = xmed >= CI_MED_left & xmed <= CI_MED_right)
#Then with the sd
CI_SD_left <- 0
CI_SD_right <- 0
z_SD <- 0
a.0_SD <- 0
for (j in 1:M) {
sample_weibull <- rweibull(100, k, l)
A <- bcanon(sample_weibull, R, theta = sd, alpha = c(0.025, 0.975))
z_SD[j] <- A$z0
a.0_SD[j] <- A$acc
CI_SD_left[j] <- A$confpoints[1,2]
CI_SD_right[j] <- A$confpoints[2,2]
}
df_bca_point_SD <- data.frame(CI_SD_left, CI_SD_right)
df_bca_point_SD <- df_bca_point_SD %>%
mutate(is_sd = sigma >= CI_SD_left & sigma <= CI_SD_right)
#Before we continue, we group relevant results into dataframes
estim <-  data.frame(z_SD, a.0_SD, z_MED, a.0_MED) #estimated bias correction and estimated acceleration constant values
df <- data.frame(df_bca_point_MED, df_bca_point_SD)
# Now, estimation of the coverage probability of both confidence intervals
Fourth_coverage_prob <- c(sum(df$is_median)/M, sum(df$is_sd)/M)
#Estimation of the average interval length
Fourth_average_interval_length <- c(sum(df$CI_MED_right - df$CI_MED_left)/nrow(df), sum(df$CI_SD_right - df$CI_SD_left)/nrow(df))
####IMPORTANT: take the average of  each z_SD, a.0_SD, z_MED and a.0_MED, to comment on z0 and a^
#Question(b)
shhs1 <- read.delim("shhs1.txt")
#histogram and empirical distribution of the variable rdi4p
ggplot(shhs1, aes(x = rdi4p, y = ..density..)) +
geom_histogram(color = 'white') +
geom_density(aes(x = rdi4p), colour = "blue")
#bootstrap percentile (two-sided) confidence intervals for the standard deviation and median
rdi4p.med <- median(shhs1$rdi4p)
rdi4p.sigma <- sd(shhs1$rdi4p)
alpha <- 0.95
n <- length(shhs1$rdi4p) #sample size
R <- 1000 #number of bootstraps replicantions
MED <- 0
SD <- 0
for (i in 1:R) {
myBootstrap <- sample(shhs1$rdi4p, n, replace = T)
MED[i] <- median(myBootstrap)
SD[i] <- sd(myBootstrap)
}
MED <- sort(MED)
SD <- sort(SD)
CI_MED_left <- MED[floor(R*(1-alpha))/2]
CI_MED_right <- MED[floor(R*(1-(1-alpha)/2))]
CI_SD_left <- SD[floor(R*(1-alpha))/2]
CI_SD_right <- SD[floor(R*(1-(1-alpha)/2))]
is_median <- rdi4p.med >= CI_MED_left & rdi4p.med <= CI_MED_right
is_sd <- rdi4p.sigma >= CI_SD_left & rdi4p.sigma <= CI_SD_right
#Now, we build the bootstrap accelerated bias-corrected confidence intervals
#First for the median
rdi4p.median.rem <- shhs1$rdi4p [! shhs1$rdi4p %in% median(shhs1$rdi4p)] #we remove all values equal to the median, to avoid errors
A <- bcanon(rdi4p.median.rem, R, theta = median, alpha = c(0.025, 0.975))
z_MED <- A$z0
a.0_MED <- A$acc
CI_MED_left <- A$confpoints[1,2]
CI_MED_right <- A$confpoints[2,2]
is_median_A = rdi4p.med >= CI_MED_left & rdi4p.med <= CI_MED_right
#Then for the sd
A <- bcanon(shhs1$rdi4p, R, theta = sd, alpha = c(0.025, 0.975))
z_SD <- A$z0
a.0_SD <- A$acc
CI_SD_left <- A$confpoints[1,2]
CI_SD_right <- A$confpoints[2,2]
is_sd_A = rdi4p.sigma >= CI_SD_left & rdi4p.sigma <= CI_SD_right
setwd("~/Stochastic_Lab/Stochastic_Lab_Course_2")
library("tidyverse")
student <- read.csv("student-mat.csv")
#Question(a)
#First we need to identify the distribution of each of G1, G2, G3
#We use Q-Q plots to do so. In order to use facet_wrap to plot into one panel, the dataset is refined as follow:
df1 <- data.frame(student$G1, rep('G1', nrow(student)))
colnames(df1) <- c("grades", "types")
df2 <- data.frame(student$G2, rep('G2', nrow(student)))
colnames(df2) <- c("grades", "types")
df3 <- data.frame(student$G3, rep('G3', nrow(student)))
colnames(df3) <- c("grades", "types")
df0 <- rbind(df1, df2, df3)
#Normal distributed?
ggplot(data = df0, mapping = aes(sample = grades)) +
geom_density(aes(x = grades), fill = "chartreuse") +
geom_abline(alpha = 0.25) +
ggtitle("Emperical densities") +
theme(plot.title = element_text(hjust = 0.5)) + #to center the title on the plot
facet_wrap(. ~types)
ggplot(data = df0, mapping = aes(sample = grades)) +
stat_qq(distribution = stats::qnorm, dparams = list(mean = mean(df0$grades), sd = sd(df0$grades))) +
geom_abline(alpha = 0.25) +
ggtitle("Q-Q plot with normal theoretical distribution") +
theme(plot.title = element_text(hjust = 0.5)) + #to center the title on the plot
facet_wrap(. ~types)
#Poisson distributed?
ggplot(data = df0, mapping = aes(sample = grades)) +
stat_qq(distribution = stats::qpois, dparams = list(lambda = mean(df0$grades))) +
geom_abline(alpha = 0.25) +
ggtitle("Q-Q plot with Poisson as theoretical distribution") +
theme(plot.title = element_text(hjust = 0.5)) + #to center the title on the plot
facet_wrap(. ~types)
# Are there signs for over-dispersion or any other anomalies in the
#distributions of any of G1, G2, G3?
a1 <- glm(formula = G1 ~. -G2 -G3, family=poisson, data = student)
summary(a1)
a2 <- glm(formula = G2 ~. -G1 -G3, family=poisson, data = student)
summary(a2)
a3 <- glm(formula = G3 ~. -G1 -G2, family=poisson, data = student)
summary(a3)
plot(a1)
model.1 <- glm(formula = G1 ~. -G2 -G2, family = quasipoisson, data = student) #quasipoisson to solve over-dispersion?????
summary(model.1)
#Pearson residuals
pearson.resid <- residuals(model.1, "pearson")
df <- data.frame(pearson.resid)
ggplot(data = df, mapping = aes(sample = pearson.resid)) +
stat_qq(distribution = stats::qnorm, dparams = list(mean = mean(df$pearson.resid), sd = sd(df$pearson.resid))) +
geom_abline(alpha = 0.25) +
ggtitle("Pearson residuals Q-Q plot with normal theoretical distribution") +
theme(plot.title = element_text(hjust = 0.5)) #to center the title on the plot
ans.resid <- anscombe.residuals(model.1, 1)
library("survival")
library("tidyverse")
library("survival")
?survfit.object
?with
?Surv
setwd("~/Stochastic_Lab/Stochastic_Lab_Course_2")
library("tidyverse")
library("survival")
thor <- read.delim("Thoracic.txt", sep = " ")
thor <- thor %>%
select(c(T.2, X60, F.7))
?select
thor <- thor %>%
dplyr::select(c(T.2, X60, F.7))
setwd("~/Stochastic_Lab/Stochastic_Lab_Course_2")
library("tidyverse")
library("survival")
thor <- read.delim("Thoracic.txt", sep = " ")
thor <- thor %>%
dplyr::select(c(T.2, X60, F.7))
colnames(thor) <- c("PRE30", "AGE", "Risk1Y")
km <- with(thor, Surv(time, status))
head(km)
head(thor)
#Question(a)
#Computing nonparametric estimators of the survivor function: Kaplan-Meier
km <- with(thor, Surv(AGE, Risk1Y))
head(km)
head(km, 20)
head(thor)
